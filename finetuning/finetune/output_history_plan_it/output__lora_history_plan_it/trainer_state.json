{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 854.0,
  "global_step": 1708,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00117096018735363,
      "grad_norm": 5.214605331420898,
      "learning_rate": 0.0,
      "loss": 1.3434,
      "step": 1
    },
    {
      "epoch": 0.00234192037470726,
      "grad_norm": 6.77822732925415,
      "learning_rate": 1.1990623328406574e-05,
      "loss": 1.2583,
      "step": 2
    },
    {
      "epoch": 0.00351288056206089,
      "grad_norm": 4.616593837738037,
      "learning_rate": 1.900468833579672e-05,
      "loss": 1.122,
      "step": 3
    },
    {
      "epoch": 0.00468384074941452,
      "grad_norm": 5.014520645141602,
      "learning_rate": 2.3981246656813148e-05,
      "loss": 1.2829,
      "step": 4
    },
    {
      "epoch": 0.00585480093676815,
      "grad_norm": 5.014520645141602,
      "learning_rate": 2.3981246656813148e-05,
      "loss": 1.1451,
      "step": 5
    },
    {
      "epoch": 0.00702576112412178,
      "grad_norm": 5.142696380615234,
      "learning_rate": 2.784136518143904e-05,
      "loss": 1.4419,
      "step": 6
    },
    {
      "epoch": 0.00819672131147541,
      "grad_norm": 5.879173755645752,
      "learning_rate": 3.099531166420329e-05,
      "loss": 0.992,
      "step": 7
    },
    {
      "epoch": 0.00936768149882904,
      "grad_norm": 5.4695024490356445,
      "learning_rate": 3.366193541954093e-05,
      "loss": 0.7754,
      "step": 8
    },
    {
      "epoch": 0.01053864168618267,
      "grad_norm": 2.3822431564331055,
      "learning_rate": 3.597186998521972e-05,
      "loss": 1.0098,
      "step": 9
    },
    {
      "epoch": 0.0117096018735363,
      "grad_norm": 2.3294856548309326,
      "learning_rate": 3.800937667159344e-05,
      "loss": 0.9461,
      "step": 10
    },
    {
      "epoch": 0.01288056206088993,
      "grad_norm": 3.503978967666626,
      "learning_rate": 3.983198850984562e-05,
      "loss": 0.6447,
      "step": 11
    },
    {
      "epoch": 0.01405152224824356,
      "grad_norm": 2.101301670074463,
      "learning_rate": 4.148074146945969e-05,
      "loss": 0.5545,
      "step": 12
    },
    {
      "epoch": 0.01522248243559719,
      "grad_norm": 2.101301670074463,
      "learning_rate": 4.148074146945969e-05,
      "loss": 0.9783,
      "step": 13
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 3.1525309085845947,
      "learning_rate": 4.298593499260987e-05,
      "loss": 0.8647,
      "step": 14
    },
    {
      "epoch": 0.01756440281030445,
      "grad_norm": 2.8595774173736572,
      "learning_rate": 4.437057880970483e-05,
      "loss": 0.811,
      "step": 15
    },
    {
      "epoch": 0.01873536299765808,
      "grad_norm": 3.8010201454162598,
      "learning_rate": 4.5652558747947496e-05,
      "loss": 0.7308,
      "step": 16
    },
    {
      "epoch": 0.01990632318501171,
      "grad_norm": 2.559422016143799,
      "learning_rate": 4.684605351723575e-05,
      "loss": 0.7284,
      "step": 17
    },
    {
      "epoch": 0.02107728337236534,
      "grad_norm": 2.6821396350860596,
      "learning_rate": 4.7962493313626296e-05,
      "loss": 0.8849,
      "step": 18
    },
    {
      "epoch": 0.02224824355971897,
      "grad_norm": 2.4622199535369873,
      "learning_rate": 4.901122729829134e-05,
      "loss": 0.532,
      "step": 19
    },
    {
      "epoch": 0.0234192037470726,
      "grad_norm": 3.214818239212036,
      "learning_rate": 5e-05,
      "loss": 0.6399,
      "step": 20
    },
    {
      "epoch": 0.02459016393442623,
      "grad_norm": 2.276649236679077,
      "learning_rate": 5e-05,
      "loss": 0.8302,
      "step": 21
    },
    {
      "epoch": 0.02576112412177986,
      "grad_norm": 3.597808837890625,
      "learning_rate": 5e-05,
      "loss": 0.4759,
      "step": 22
    },
    {
      "epoch": 0.026932084309133488,
      "grad_norm": 2.9934821128845215,
      "learning_rate": 5e-05,
      "loss": 0.5789,
      "step": 23
    },
    {
      "epoch": 0.02810304449648712,
      "grad_norm": 4.248179912567139,
      "learning_rate": 5e-05,
      "loss": 0.7657,
      "step": 24
    },
    {
      "epoch": 0.02927400468384075,
      "grad_norm": 2.367793321609497,
      "learning_rate": 5e-05,
      "loss": 0.3522,
      "step": 25
    },
    {
      "epoch": 0.03044496487119438,
      "grad_norm": 2.1936135292053223,
      "learning_rate": 5e-05,
      "loss": 0.6923,
      "step": 26
    },
    {
      "epoch": 0.03161592505854801,
      "grad_norm": 4.765908718109131,
      "learning_rate": 5e-05,
      "loss": 0.5151,
      "step": 27
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 2.3348352909088135,
      "learning_rate": 5e-05,
      "loss": 0.4884,
      "step": 28
    },
    {
      "epoch": 0.03395784543325527,
      "grad_norm": 2.820142984390259,
      "learning_rate": 5e-05,
      "loss": 0.4094,
      "step": 29
    },
    {
      "epoch": 0.0351288056206089,
      "grad_norm": 4.263189315795898,
      "learning_rate": 5e-05,
      "loss": 0.9359,
      "step": 30
    },
    {
      "epoch": 0.03629976580796253,
      "grad_norm": 8.413872718811035,
      "learning_rate": 5e-05,
      "loss": 0.7142,
      "step": 31
    },
    {
      "epoch": 0.03747072599531616,
      "grad_norm": 3.497511863708496,
      "learning_rate": 5e-05,
      "loss": 0.4691,
      "step": 32
    },
    {
      "epoch": 0.03864168618266979,
      "grad_norm": 5.87220573425293,
      "learning_rate": 5e-05,
      "loss": 0.531,
      "step": 33
    },
    {
      "epoch": 0.03981264637002342,
      "grad_norm": 1.5416933298110962,
      "learning_rate": 5e-05,
      "loss": 0.4523,
      "step": 34
    },
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 9.348201751708984,
      "learning_rate": 5e-05,
      "loss": 0.8937,
      "step": 35
    },
    {
      "epoch": 0.04215456674473068,
      "grad_norm": 2.8289756774902344,
      "learning_rate": 5e-05,
      "loss": 0.3975,
      "step": 36
    },
    {
      "epoch": 0.04332552693208431,
      "grad_norm": 2.073476552963257,
      "learning_rate": 5e-05,
      "loss": 0.5521,
      "step": 37
    },
    {
      "epoch": 0.04449648711943794,
      "grad_norm": 2.4932281970977783,
      "learning_rate": 5e-05,
      "loss": 0.6526,
      "step": 38
    },
    {
      "epoch": 0.04566744730679157,
      "grad_norm": 2.060286283493042,
      "learning_rate": 5e-05,
      "loss": 0.3981,
      "step": 39
    },
    {
      "epoch": 0.0468384074941452,
      "grad_norm": 4.530902862548828,
      "learning_rate": 5e-05,
      "loss": 0.6042,
      "step": 40
    },
    {
      "epoch": 0.04800936768149883,
      "grad_norm": 5.406681537628174,
      "learning_rate": 5e-05,
      "loss": 0.4168,
      "step": 41
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 2.959347724914551,
      "learning_rate": 5e-05,
      "loss": 0.4028,
      "step": 42
    },
    {
      "epoch": 0.05035128805620609,
      "grad_norm": 5.609697341918945,
      "learning_rate": 5e-05,
      "loss": 0.3195,
      "step": 43
    },
    {
      "epoch": 0.05152224824355972,
      "grad_norm": 2.1583571434020996,
      "learning_rate": 5e-05,
      "loss": 0.4553,
      "step": 44
    },
    {
      "epoch": 0.05269320843091335,
      "grad_norm": 2.3426921367645264,
      "learning_rate": 5e-05,
      "loss": 0.4473,
      "step": 45
    },
    {
      "epoch": 0.053864168618266976,
      "grad_norm": 2.199881076812744,
      "learning_rate": 5e-05,
      "loss": 0.4092,
      "step": 46
    },
    {
      "epoch": 0.05503512880562061,
      "grad_norm": 3.743037700653076,
      "learning_rate": 5e-05,
      "loss": 0.4678,
      "step": 47
    },
    {
      "epoch": 0.05620608899297424,
      "grad_norm": 4.592536449432373,
      "learning_rate": 5e-05,
      "loss": 0.3687,
      "step": 48
    },
    {
      "epoch": 0.05737704918032787,
      "grad_norm": 3.981288433074951,
      "learning_rate": 5e-05,
      "loss": 0.4927,
      "step": 49
    },
    {
      "epoch": 0.0585480093676815,
      "grad_norm": 10.320956230163574,
      "learning_rate": 5e-05,
      "loss": 0.4237,
      "step": 50
    },
    {
      "epoch": 0.059718969555035126,
      "grad_norm": 1.9038225412368774,
      "learning_rate": 5e-05,
      "loss": 0.207,
      "step": 51
    },
    {
      "epoch": 0.06088992974238876,
      "grad_norm": 3.5985848903656006,
      "learning_rate": 5e-05,
      "loss": 0.4316,
      "step": 52
    },
    {
      "epoch": 0.06206088992974239,
      "grad_norm": 3.8429110050201416,
      "learning_rate": 5e-05,
      "loss": 0.5167,
      "step": 53
    },
    {
      "epoch": 0.06323185011709602,
      "grad_norm": 1.653245210647583,
      "learning_rate": 5e-05,
      "loss": 0.3531,
      "step": 54
    },
    {
      "epoch": 0.06440281030444965,
      "grad_norm": 7.291226863861084,
      "learning_rate": 5e-05,
      "loss": 0.5372,
      "step": 55
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 1.5314332246780396,
      "learning_rate": 5e-05,
      "loss": 0.3516,
      "step": 56
    },
    {
      "epoch": 0.06674473067915691,
      "grad_norm": 1.956543207168579,
      "learning_rate": 5e-05,
      "loss": 0.5215,
      "step": 57
    },
    {
      "epoch": 0.06791569086651054,
      "grad_norm": 2.93119215965271,
      "learning_rate": 5e-05,
      "loss": 0.3274,
      "step": 58
    },
    {
      "epoch": 0.06908665105386416,
      "grad_norm": 2.247525930404663,
      "learning_rate": 5e-05,
      "loss": 0.3615,
      "step": 59
    },
    {
      "epoch": 0.0702576112412178,
      "grad_norm": 2.0311052799224854,
      "learning_rate": 5e-05,
      "loss": 0.3015,
      "step": 60
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 4.7880988121032715,
      "learning_rate": 5e-05,
      "loss": 0.6415,
      "step": 61
    },
    {
      "epoch": 0.07259953161592506,
      "grad_norm": 2.9636270999908447,
      "learning_rate": 5e-05,
      "loss": 0.3096,
      "step": 62
    },
    {
      "epoch": 0.07377049180327869,
      "grad_norm": 3.4498348236083984,
      "learning_rate": 5e-05,
      "loss": 0.6375,
      "step": 63
    },
    {
      "epoch": 0.07494145199063232,
      "grad_norm": 1.9527820348739624,
      "learning_rate": 5e-05,
      "loss": 0.4779,
      "step": 64
    },
    {
      "epoch": 0.07611241217798595,
      "grad_norm": 2.521918535232544,
      "learning_rate": 5e-05,
      "loss": 0.3386,
      "step": 65
    },
    {
      "epoch": 0.07728337236533958,
      "grad_norm": 2.6687910556793213,
      "learning_rate": 5e-05,
      "loss": 0.7139,
      "step": 66
    },
    {
      "epoch": 0.07845433255269321,
      "grad_norm": 1.8507169485092163,
      "learning_rate": 5e-05,
      "loss": 0.4421,
      "step": 67
    },
    {
      "epoch": 0.07962529274004684,
      "grad_norm": 3.81817889213562,
      "learning_rate": 5e-05,
      "loss": 0.4715,
      "step": 68
    },
    {
      "epoch": 0.08079625292740047,
      "grad_norm": 3.81817889213562,
      "learning_rate": 5e-05,
      "loss": 0.5453,
      "step": 69
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 2.6464788913726807,
      "learning_rate": 5e-05,
      "loss": 0.4257,
      "step": 70
    },
    {
      "epoch": 0.08313817330210772,
      "grad_norm": 1.5626648664474487,
      "learning_rate": 5e-05,
      "loss": 0.2533,
      "step": 71
    },
    {
      "epoch": 0.08430913348946135,
      "grad_norm": 1.5074260234832764,
      "learning_rate": 5e-05,
      "loss": 0.4424,
      "step": 72
    },
    {
      "epoch": 0.08548009367681499,
      "grad_norm": 1.4211064577102661,
      "learning_rate": 5e-05,
      "loss": 0.4073,
      "step": 73
    },
    {
      "epoch": 0.08665105386416862,
      "grad_norm": 1.9145188331604004,
      "learning_rate": 5e-05,
      "loss": 0.2682,
      "step": 74
    },
    {
      "epoch": 0.08782201405152225,
      "grad_norm": 2.0245282649993896,
      "learning_rate": 5e-05,
      "loss": 0.4261,
      "step": 75
    },
    {
      "epoch": 0.08899297423887588,
      "grad_norm": 2.2013657093048096,
      "learning_rate": 5e-05,
      "loss": 0.5074,
      "step": 76
    },
    {
      "epoch": 0.09016393442622951,
      "grad_norm": 3.24601149559021,
      "learning_rate": 5e-05,
      "loss": 0.4576,
      "step": 77
    },
    {
      "epoch": 0.09133489461358314,
      "grad_norm": 2.5229015350341797,
      "learning_rate": 5e-05,
      "loss": 0.3106,
      "step": 78
    },
    {
      "epoch": 0.09250585480093677,
      "grad_norm": 1.7370082139968872,
      "learning_rate": 5e-05,
      "loss": 0.2356,
      "step": 79
    },
    {
      "epoch": 0.0936768149882904,
      "grad_norm": 1.8520010709762573,
      "learning_rate": 5e-05,
      "loss": 0.3261,
      "step": 80
    },
    {
      "epoch": 0.09484777517564402,
      "grad_norm": 1.9886549711227417,
      "learning_rate": 5e-05,
      "loss": 0.3386,
      "step": 81
    },
    {
      "epoch": 0.09601873536299765,
      "grad_norm": 1.4475369453430176,
      "learning_rate": 5e-05,
      "loss": 0.2346,
      "step": 82
    },
    {
      "epoch": 0.09718969555035128,
      "grad_norm": 1.122023105621338,
      "learning_rate": 5e-05,
      "loss": 0.217,
      "step": 83
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 3.1216042041778564,
      "learning_rate": 5e-05,
      "loss": 0.2596,
      "step": 84
    },
    {
      "epoch": 0.09953161592505855,
      "grad_norm": 1.6984285116195679,
      "learning_rate": 5e-05,
      "loss": 0.2955,
      "step": 85
    },
    {
      "epoch": 0.10070257611241218,
      "grad_norm": 1.4986275434494019,
      "learning_rate": 5e-05,
      "loss": 0.341,
      "step": 86
    },
    {
      "epoch": 0.10187353629976581,
      "grad_norm": 2.2065653800964355,
      "learning_rate": 5e-05,
      "loss": 0.236,
      "step": 87
    },
    {
      "epoch": 0.10304449648711944,
      "grad_norm": 1.6123908758163452,
      "learning_rate": 5e-05,
      "loss": 0.3324,
      "step": 88
    },
    {
      "epoch": 0.10421545667447307,
      "grad_norm": 3.5940940380096436,
      "learning_rate": 5e-05,
      "loss": 0.2824,
      "step": 89
    },
    {
      "epoch": 0.1053864168618267,
      "grad_norm": 1.5122936964035034,
      "learning_rate": 5e-05,
      "loss": 0.2692,
      "step": 90
    },
    {
      "epoch": 0.10655737704918032,
      "grad_norm": 6.897449016571045,
      "learning_rate": 5e-05,
      "loss": 0.2937,
      "step": 91
    },
    {
      "epoch": 0.10772833723653395,
      "grad_norm": 5.757046699523926,
      "learning_rate": 5e-05,
      "loss": 1.0009,
      "step": 92
    },
    {
      "epoch": 0.10889929742388758,
      "grad_norm": 3.928140878677368,
      "learning_rate": 5e-05,
      "loss": 0.3411,
      "step": 93
    },
    {
      "epoch": 0.11007025761124122,
      "grad_norm": 1.8379827737808228,
      "learning_rate": 5e-05,
      "loss": 0.1767,
      "step": 94
    },
    {
      "epoch": 0.11124121779859485,
      "grad_norm": 1.7278289794921875,
      "learning_rate": 5e-05,
      "loss": 0.3482,
      "step": 95
    },
    {
      "epoch": 0.11241217798594848,
      "grad_norm": 2.730746269226074,
      "learning_rate": 5e-05,
      "loss": 0.4185,
      "step": 96
    },
    {
      "epoch": 0.11358313817330211,
      "grad_norm": 1.8887033462524414,
      "learning_rate": 5e-05,
      "loss": 0.2006,
      "step": 97
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 1.9083702564239502,
      "learning_rate": 5e-05,
      "loss": 0.3984,
      "step": 98
    },
    {
      "epoch": 0.11592505854800937,
      "grad_norm": 1.390443205833435,
      "learning_rate": 5e-05,
      "loss": 0.1753,
      "step": 99
    },
    {
      "epoch": 0.117096018735363,
      "grad_norm": 1.1400411128997803,
      "learning_rate": 5e-05,
      "loss": 0.2683,
      "step": 100
    },
    {
      "epoch": 0.11826697892271663,
      "grad_norm": 1.8049110174179077,
      "learning_rate": 5e-05,
      "loss": 0.4358,
      "step": 101
    },
    {
      "epoch": 0.11943793911007025,
      "grad_norm": 2.0938689708709717,
      "learning_rate": 5e-05,
      "loss": 0.3114,
      "step": 102
    },
    {
      "epoch": 0.12060889929742388,
      "grad_norm": 1.804787278175354,
      "learning_rate": 5e-05,
      "loss": 0.4637,
      "step": 103
    },
    {
      "epoch": 0.12177985948477751,
      "grad_norm": 1.2658668756484985,
      "learning_rate": 5e-05,
      "loss": 0.2873,
      "step": 104
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 1.873358964920044,
      "learning_rate": 5e-05,
      "loss": 0.6103,
      "step": 105
    },
    {
      "epoch": 0.12412177985948478,
      "grad_norm": 5.316211223602295,
      "learning_rate": 5e-05,
      "loss": 0.2347,
      "step": 106
    },
    {
      "epoch": 0.1252927400468384,
      "grad_norm": 2.471151113510132,
      "learning_rate": 5e-05,
      "loss": 0.6116,
      "step": 107
    },
    {
      "epoch": 0.12646370023419204,
      "grad_norm": 1.0174005031585693,
      "learning_rate": 5e-05,
      "loss": 0.4091,
      "step": 108
    },
    {
      "epoch": 0.12763466042154567,
      "grad_norm": 1.6437973976135254,
      "learning_rate": 5e-05,
      "loss": 0.3155,
      "step": 109
    },
    {
      "epoch": 0.1288056206088993,
      "grad_norm": 0.9809241890907288,
      "learning_rate": 5e-05,
      "loss": 0.2067,
      "step": 110
    },
    {
      "epoch": 0.12997658079625293,
      "grad_norm": 1.3609901666641235,
      "learning_rate": 5e-05,
      "loss": 0.3043,
      "step": 111
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.9465838670730591,
      "learning_rate": 5e-05,
      "loss": 0.1632,
      "step": 112
    },
    {
      "epoch": 0.1323185011709602,
      "grad_norm": 1.3333791494369507,
      "learning_rate": 5e-05,
      "loss": 0.2313,
      "step": 113
    },
    {
      "epoch": 0.13348946135831383,
      "grad_norm": 1.0413110256195068,
      "learning_rate": 5e-05,
      "loss": 0.3347,
      "step": 114
    },
    {
      "epoch": 0.13466042154566746,
      "grad_norm": 0.72878497838974,
      "learning_rate": 5e-05,
      "loss": 0.2732,
      "step": 115
    },
    {
      "epoch": 0.1358313817330211,
      "grad_norm": 0.7987414002418518,
      "learning_rate": 5e-05,
      "loss": 0.2341,
      "step": 116
    },
    {
      "epoch": 0.13700234192037472,
      "grad_norm": 0.9893429279327393,
      "learning_rate": 5e-05,
      "loss": 0.3109,
      "step": 117
    },
    {
      "epoch": 0.13817330210772832,
      "grad_norm": 0.8896848559379578,
      "learning_rate": 5e-05,
      "loss": 0.1942,
      "step": 118
    },
    {
      "epoch": 0.13934426229508196,
      "grad_norm": 0.9982425570487976,
      "learning_rate": 5e-05,
      "loss": 0.3673,
      "step": 119
    },
    {
      "epoch": 0.1405152224824356,
      "grad_norm": 1.9315125942230225,
      "learning_rate": 5e-05,
      "loss": 0.5356,
      "step": 120
    },
    {
      "epoch": 0.14168618266978922,
      "grad_norm": 0.9469835758209229,
      "learning_rate": 5e-05,
      "loss": 0.2613,
      "step": 121
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 1.309743046760559,
      "learning_rate": 5e-05,
      "loss": 0.3492,
      "step": 122
    },
    {
      "epoch": 0.14402810304449648,
      "grad_norm": 1.5659046173095703,
      "learning_rate": 5e-05,
      "loss": 0.3538,
      "step": 123
    },
    {
      "epoch": 0.1451990632318501,
      "grad_norm": 0.7462148666381836,
      "learning_rate": 5e-05,
      "loss": 0.1597,
      "step": 124
    },
    {
      "epoch": 0.14637002341920374,
      "grad_norm": 0.7605217695236206,
      "learning_rate": 5e-05,
      "loss": 0.1819,
      "step": 125
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 1.2131644487380981,
      "learning_rate": 5e-05,
      "loss": 0.2398,
      "step": 126
    },
    {
      "epoch": 0.148711943793911,
      "grad_norm": 1.3360111713409424,
      "learning_rate": 5e-05,
      "loss": 0.2744,
      "step": 127
    },
    {
      "epoch": 0.14988290398126464,
      "grad_norm": 0.7718749046325684,
      "learning_rate": 5e-05,
      "loss": 0.1672,
      "step": 128
    },
    {
      "epoch": 0.15105386416861827,
      "grad_norm": 1.591950535774231,
      "learning_rate": 5e-05,
      "loss": 0.3992,
      "step": 129
    },
    {
      "epoch": 0.1522248243559719,
      "grad_norm": 0.9559087753295898,
      "learning_rate": 5e-05,
      "loss": 0.264,
      "step": 130
    },
    {
      "epoch": 0.15339578454332553,
      "grad_norm": 0.7844812870025635,
      "learning_rate": 5e-05,
      "loss": 0.1844,
      "step": 131
    },
    {
      "epoch": 0.15456674473067916,
      "grad_norm": 1.100276231765747,
      "learning_rate": 5e-05,
      "loss": 0.2685,
      "step": 132
    },
    {
      "epoch": 0.1557377049180328,
      "grad_norm": 1.2670758962631226,
      "learning_rate": 5e-05,
      "loss": 0.2708,
      "step": 133
    },
    {
      "epoch": 0.15690866510538642,
      "grad_norm": 1.5236181020736694,
      "learning_rate": 5e-05,
      "loss": 0.2731,
      "step": 134
    },
    {
      "epoch": 0.15807962529274006,
      "grad_norm": 0.9375239014625549,
      "learning_rate": 5e-05,
      "loss": 0.2358,
      "step": 135
    },
    {
      "epoch": 0.1592505854800937,
      "grad_norm": 1.1388038396835327,
      "learning_rate": 5e-05,
      "loss": 0.2873,
      "step": 136
    },
    {
      "epoch": 0.16042154566744732,
      "grad_norm": 0.8393994569778442,
      "learning_rate": 5e-05,
      "loss": 0.3382,
      "step": 137
    },
    {
      "epoch": 0.16159250585480095,
      "grad_norm": 0.7134610414505005,
      "learning_rate": 5e-05,
      "loss": 0.1547,
      "step": 138
    },
    {
      "epoch": 0.16276346604215455,
      "grad_norm": 1.7614420652389526,
      "learning_rate": 5e-05,
      "loss": 0.2742,
      "step": 139
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 1.0215318202972412,
      "learning_rate": 5e-05,
      "loss": 0.2707,
      "step": 140
    },
    {
      "epoch": 0.16510538641686182,
      "grad_norm": 1.0719683170318604,
      "learning_rate": 5e-05,
      "loss": 0.3209,
      "step": 141
    },
    {
      "epoch": 0.16627634660421545,
      "grad_norm": 0.7245623469352722,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 142
    },
    {
      "epoch": 0.16744730679156908,
      "grad_norm": 0.8317916393280029,
      "learning_rate": 5e-05,
      "loss": 0.1733,
      "step": 143
    },
    {
      "epoch": 0.1686182669789227,
      "grad_norm": 1.056338906288147,
      "learning_rate": 5e-05,
      "loss": 0.2294,
      "step": 144
    },
    {
      "epoch": 0.16978922716627634,
      "grad_norm": 0.8608675003051758,
      "learning_rate": 5e-05,
      "loss": 0.2305,
      "step": 145
    },
    {
      "epoch": 0.17096018735362997,
      "grad_norm": 1.2183167934417725,
      "learning_rate": 5e-05,
      "loss": 0.3239,
      "step": 146
    },
    {
      "epoch": 0.1721311475409836,
      "grad_norm": 1.070072889328003,
      "learning_rate": 5e-05,
      "loss": 0.1736,
      "step": 147
    },
    {
      "epoch": 0.17330210772833723,
      "grad_norm": 1.5810309648513794,
      "learning_rate": 5e-05,
      "loss": 0.3944,
      "step": 148
    },
    {
      "epoch": 0.17447306791569087,
      "grad_norm": 0.9401994347572327,
      "learning_rate": 5e-05,
      "loss": 0.3796,
      "step": 149
    },
    {
      "epoch": 0.1756440281030445,
      "grad_norm": 0.8002363443374634,
      "learning_rate": 5e-05,
      "loss": 0.1715,
      "step": 150
    },
    {
      "epoch": 0.17681498829039813,
      "grad_norm": 0.8493637442588806,
      "learning_rate": 5e-05,
      "loss": 0.1992,
      "step": 151
    },
    {
      "epoch": 0.17798594847775176,
      "grad_norm": 0.835015058517456,
      "learning_rate": 5e-05,
      "loss": 0.2282,
      "step": 152
    },
    {
      "epoch": 0.1791569086651054,
      "grad_norm": 0.9111880660057068,
      "learning_rate": 5e-05,
      "loss": 0.2142,
      "step": 153
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 1.0698832273483276,
      "learning_rate": 5e-05,
      "loss": 0.3114,
      "step": 154
    },
    {
      "epoch": 0.18149882903981265,
      "grad_norm": 0.6556869745254517,
      "learning_rate": 5e-05,
      "loss": 0.1431,
      "step": 155
    },
    {
      "epoch": 0.18266978922716628,
      "grad_norm": 1.0058135986328125,
      "learning_rate": 5e-05,
      "loss": 0.3043,
      "step": 156
    },
    {
      "epoch": 0.18384074941451992,
      "grad_norm": 0.9608814120292664,
      "learning_rate": 5e-05,
      "loss": 0.2986,
      "step": 157
    },
    {
      "epoch": 0.18501170960187355,
      "grad_norm": 0.6208246946334839,
      "learning_rate": 5e-05,
      "loss": 0.1646,
      "step": 158
    },
    {
      "epoch": 0.18618266978922718,
      "grad_norm": 0.8164880275726318,
      "learning_rate": 5e-05,
      "loss": 0.3549,
      "step": 159
    },
    {
      "epoch": 0.1873536299765808,
      "grad_norm": 1.0270733833312988,
      "learning_rate": 5e-05,
      "loss": 0.244,
      "step": 160
    },
    {
      "epoch": 0.1885245901639344,
      "grad_norm": 0.9286057949066162,
      "learning_rate": 5e-05,
      "loss": 0.1475,
      "step": 161
    },
    {
      "epoch": 0.18969555035128804,
      "grad_norm": 1.2761151790618896,
      "learning_rate": 5e-05,
      "loss": 0.3258,
      "step": 162
    },
    {
      "epoch": 0.19086651053864168,
      "grad_norm": 1.3765424489974976,
      "learning_rate": 5e-05,
      "loss": 0.2411,
      "step": 163
    },
    {
      "epoch": 0.1920374707259953,
      "grad_norm": 1.326846957206726,
      "learning_rate": 5e-05,
      "loss": 0.3459,
      "step": 164
    },
    {
      "epoch": 0.19320843091334894,
      "grad_norm": 1.0993804931640625,
      "learning_rate": 5e-05,
      "loss": 0.4845,
      "step": 165
    },
    {
      "epoch": 0.19437939110070257,
      "grad_norm": 0.9288591742515564,
      "learning_rate": 5e-05,
      "loss": 0.2247,
      "step": 166
    },
    {
      "epoch": 0.1955503512880562,
      "grad_norm": 1.1554075479507446,
      "learning_rate": 5e-05,
      "loss": 0.3754,
      "step": 167
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 1.1598328351974487,
      "learning_rate": 5e-05,
      "loss": 0.4138,
      "step": 168
    },
    {
      "epoch": 0.19789227166276346,
      "grad_norm": 1.0032092332839966,
      "learning_rate": 5e-05,
      "loss": 0.1771,
      "step": 169
    },
    {
      "epoch": 0.1990632318501171,
      "grad_norm": 1.1944389343261719,
      "learning_rate": 5e-05,
      "loss": 0.297,
      "step": 170
    },
    {
      "epoch": 0.20023419203747073,
      "grad_norm": 1.075093150138855,
      "learning_rate": 5e-05,
      "loss": 0.1711,
      "step": 171
    },
    {
      "epoch": 0.20140515222482436,
      "grad_norm": 0.9078665971755981,
      "learning_rate": 5e-05,
      "loss": 0.2624,
      "step": 172
    },
    {
      "epoch": 0.202576112412178,
      "grad_norm": 1.1971244812011719,
      "learning_rate": 5e-05,
      "loss": 0.2904,
      "step": 173
    },
    {
      "epoch": 0.20374707259953162,
      "grad_norm": 0.8339627385139465,
      "learning_rate": 5e-05,
      "loss": 0.2356,
      "step": 174
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 0.8550008535385132,
      "learning_rate": 5e-05,
      "loss": 0.16,
      "step": 175
    },
    {
      "epoch": 0.20608899297423888,
      "grad_norm": 1.4188977479934692,
      "learning_rate": 5e-05,
      "loss": 0.2788,
      "step": 176
    },
    {
      "epoch": 0.20725995316159251,
      "grad_norm": 1.6935633420944214,
      "learning_rate": 5e-05,
      "loss": 0.2221,
      "step": 177
    },
    {
      "epoch": 0.20843091334894615,
      "grad_norm": 1.189397931098938,
      "learning_rate": 5e-05,
      "loss": 0.2218,
      "step": 178
    },
    {
      "epoch": 0.20960187353629978,
      "grad_norm": 0.7078304886817932,
      "learning_rate": 5e-05,
      "loss": 0.1439,
      "step": 179
    },
    {
      "epoch": 0.2107728337236534,
      "grad_norm": 0.7527623772621155,
      "learning_rate": 5e-05,
      "loss": 0.2644,
      "step": 180
    },
    {
      "epoch": 0.21194379391100704,
      "grad_norm": 0.9803325533866882,
      "learning_rate": 5e-05,
      "loss": 0.1571,
      "step": 181
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 1.2317390441894531,
      "learning_rate": 5e-05,
      "loss": 0.2449,
      "step": 182
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 1.3089085817337036,
      "learning_rate": 5e-05,
      "loss": 0.4212,
      "step": 183
    },
    {
      "epoch": 0.2154566744730679,
      "grad_norm": 0.8835243582725525,
      "learning_rate": 5e-05,
      "loss": 0.119,
      "step": 184
    },
    {
      "epoch": 0.21662763466042154,
      "grad_norm": 1.3073348999023438,
      "learning_rate": 5e-05,
      "loss": 0.281,
      "step": 185
    },
    {
      "epoch": 0.21779859484777517,
      "grad_norm": 1.1504024267196655,
      "learning_rate": 5e-05,
      "loss": 0.2689,
      "step": 186
    },
    {
      "epoch": 0.2189695550351288,
      "grad_norm": 0.726905882358551,
      "learning_rate": 5e-05,
      "loss": 0.1361,
      "step": 187
    },
    {
      "epoch": 0.22014051522248243,
      "grad_norm": 1.0983704328536987,
      "learning_rate": 5e-05,
      "loss": 0.334,
      "step": 188
    },
    {
      "epoch": 0.22131147540983606,
      "grad_norm": 1.3364756107330322,
      "learning_rate": 5e-05,
      "loss": 0.3498,
      "step": 189
    },
    {
      "epoch": 0.2224824355971897,
      "grad_norm": 1.0859384536743164,
      "learning_rate": 5e-05,
      "loss": 0.2363,
      "step": 190
    },
    {
      "epoch": 0.22365339578454332,
      "grad_norm": 0.9849263429641724,
      "learning_rate": 5e-05,
      "loss": 0.2928,
      "step": 191
    },
    {
      "epoch": 0.22482435597189696,
      "grad_norm": 0.9199542999267578,
      "learning_rate": 5e-05,
      "loss": 0.1596,
      "step": 192
    },
    {
      "epoch": 0.2259953161592506,
      "grad_norm": 0.977390468120575,
      "learning_rate": 5e-05,
      "loss": 0.3196,
      "step": 193
    },
    {
      "epoch": 0.22716627634660422,
      "grad_norm": 0.8276990652084351,
      "learning_rate": 5e-05,
      "loss": 0.347,
      "step": 194
    },
    {
      "epoch": 0.22833723653395785,
      "grad_norm": 0.8313707709312439,
      "learning_rate": 5e-05,
      "loss": 0.1899,
      "step": 195
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 0.7502405047416687,
      "learning_rate": 5e-05,
      "loss": 0.1834,
      "step": 196
    },
    {
      "epoch": 0.2306791569086651,
      "grad_norm": 1.4024752378463745,
      "learning_rate": 5e-05,
      "loss": 0.1666,
      "step": 197
    },
    {
      "epoch": 0.23185011709601874,
      "grad_norm": 0.9320037364959717,
      "learning_rate": 5e-05,
      "loss": 0.1097,
      "step": 198
    },
    {
      "epoch": 0.23302107728337237,
      "grad_norm": 0.9487895369529724,
      "learning_rate": 5e-05,
      "loss": 0.2006,
      "step": 199
    },
    {
      "epoch": 0.234192037470726,
      "grad_norm": 1.1319248676300049,
      "learning_rate": 5e-05,
      "loss": 0.3253,
      "step": 200
    },
    {
      "epoch": 0.23536299765807964,
      "grad_norm": 0.7792619466781616,
      "learning_rate": 5e-05,
      "loss": 0.1707,
      "step": 201
    },
    {
      "epoch": 0.23653395784543327,
      "grad_norm": 0.9384832382202148,
      "learning_rate": 5e-05,
      "loss": 0.3187,
      "step": 202
    },
    {
      "epoch": 0.23770491803278687,
      "grad_norm": 0.6792188286781311,
      "learning_rate": 5e-05,
      "loss": 0.1993,
      "step": 203
    },
    {
      "epoch": 0.2388758782201405,
      "grad_norm": 0.8928541541099548,
      "learning_rate": 5e-05,
      "loss": 0.2055,
      "step": 204
    },
    {
      "epoch": 0.24004683840749413,
      "grad_norm": 0.859683096408844,
      "learning_rate": 5e-05,
      "loss": 0.173,
      "step": 205
    },
    {
      "epoch": 0.24121779859484777,
      "grad_norm": 1.567777156829834,
      "learning_rate": 5e-05,
      "loss": 0.1051,
      "step": 206
    },
    {
      "epoch": 0.2423887587822014,
      "grad_norm": 0.9574695229530334,
      "learning_rate": 5e-05,
      "loss": 0.2787,
      "step": 207
    },
    {
      "epoch": 0.24355971896955503,
      "grad_norm": 0.850570797920227,
      "learning_rate": 5e-05,
      "loss": 0.2329,
      "step": 208
    },
    {
      "epoch": 0.24473067915690866,
      "grad_norm": 1.1697769165039062,
      "learning_rate": 5e-05,
      "loss": 0.3362,
      "step": 209
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 0.9994024038314819,
      "learning_rate": 5e-05,
      "loss": 0.1552,
      "step": 210
    },
    {
      "epoch": 0.24707259953161592,
      "grad_norm": 0.9246962070465088,
      "learning_rate": 5e-05,
      "loss": 0.1987,
      "step": 211
    },
    {
      "epoch": 0.24824355971896955,
      "grad_norm": 0.941358745098114,
      "learning_rate": 5e-05,
      "loss": 0.181,
      "step": 212
    },
    {
      "epoch": 0.24941451990632318,
      "grad_norm": 0.8752478957176208,
      "learning_rate": 5e-05,
      "loss": 0.2503,
      "step": 213
    },
    {
      "epoch": 0.2505854800936768,
      "grad_norm": 0.8088522553443909,
      "learning_rate": 5e-05,
      "loss": 0.2047,
      "step": 214
    },
    {
      "epoch": 0.25175644028103045,
      "grad_norm": 1.2897788286209106,
      "learning_rate": 5e-05,
      "loss": 0.2777,
      "step": 215
    },
    {
      "epoch": 0.2529274004683841,
      "grad_norm": 1.2910860776901245,
      "learning_rate": 5e-05,
      "loss": 0.2827,
      "step": 216
    },
    {
      "epoch": 0.2540983606557377,
      "grad_norm": 1.1043345928192139,
      "learning_rate": 5e-05,
      "loss": 0.2238,
      "step": 217
    },
    {
      "epoch": 0.25526932084309134,
      "grad_norm": 1.0658113956451416,
      "learning_rate": 5e-05,
      "loss": 0.2572,
      "step": 218
    },
    {
      "epoch": 0.25644028103044497,
      "grad_norm": 1.6460152864456177,
      "learning_rate": 5e-05,
      "loss": 0.4793,
      "step": 219
    },
    {
      "epoch": 0.2576112412177986,
      "grad_norm": 0.7906126379966736,
      "learning_rate": 5e-05,
      "loss": 0.2579,
      "step": 220
    },
    {
      "epoch": 0.25878220140515223,
      "grad_norm": 0.9286418557167053,
      "learning_rate": 5e-05,
      "loss": 0.3114,
      "step": 221
    },
    {
      "epoch": 0.25995316159250587,
      "grad_norm": 0.6560574769973755,
      "learning_rate": 5e-05,
      "loss": 0.1549,
      "step": 222
    },
    {
      "epoch": 0.2611241217798595,
      "grad_norm": 1.0743409395217896,
      "learning_rate": 5e-05,
      "loss": 0.2846,
      "step": 223
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 0.6324546933174133,
      "learning_rate": 5e-05,
      "loss": 0.1253,
      "step": 224
    },
    {
      "epoch": 0.26346604215456676,
      "grad_norm": 1.0980011224746704,
      "learning_rate": 5e-05,
      "loss": 0.1913,
      "step": 225
    },
    {
      "epoch": 0.2646370023419204,
      "grad_norm": 1.4868396520614624,
      "learning_rate": 5e-05,
      "loss": 0.241,
      "step": 226
    },
    {
      "epoch": 0.265807962529274,
      "grad_norm": 0.642329216003418,
      "learning_rate": 5e-05,
      "loss": 0.1581,
      "step": 227
    },
    {
      "epoch": 0.26697892271662765,
      "grad_norm": 0.8131322860717773,
      "learning_rate": 5e-05,
      "loss": 0.294,
      "step": 228
    },
    {
      "epoch": 0.2681498829039813,
      "grad_norm": 0.8366737365722656,
      "learning_rate": 5e-05,
      "loss": 0.2398,
      "step": 229
    },
    {
      "epoch": 0.2693208430913349,
      "grad_norm": 0.7943037748336792,
      "learning_rate": 5e-05,
      "loss": 0.2001,
      "step": 230
    },
    {
      "epoch": 0.27049180327868855,
      "grad_norm": 0.725827157497406,
      "learning_rate": 5e-05,
      "loss": 0.1552,
      "step": 231
    },
    {
      "epoch": 0.2716627634660422,
      "grad_norm": 1.1141122579574585,
      "learning_rate": 5e-05,
      "loss": 0.2975,
      "step": 232
    },
    {
      "epoch": 0.2728337236533958,
      "grad_norm": 0.9035328030586243,
      "learning_rate": 5e-05,
      "loss": 0.3274,
      "step": 233
    },
    {
      "epoch": 0.27400468384074944,
      "grad_norm": 1.0822924375534058,
      "learning_rate": 5e-05,
      "loss": 0.3517,
      "step": 234
    },
    {
      "epoch": 0.275175644028103,
      "grad_norm": 1.15290105342865,
      "learning_rate": 5e-05,
      "loss": 0.2512,
      "step": 235
    },
    {
      "epoch": 0.27634660421545665,
      "grad_norm": 0.8853816986083984,
      "learning_rate": 5e-05,
      "loss": 0.2276,
      "step": 236
    },
    {
      "epoch": 0.2775175644028103,
      "grad_norm": 0.9572566747665405,
      "learning_rate": 5e-05,
      "loss": 0.1657,
      "step": 237
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 0.8031160831451416,
      "learning_rate": 5e-05,
      "loss": 0.1159,
      "step": 238
    },
    {
      "epoch": 0.27985948477751754,
      "grad_norm": 1.279334306716919,
      "learning_rate": 5e-05,
      "loss": 0.2673,
      "step": 239
    },
    {
      "epoch": 0.2810304449648712,
      "grad_norm": 0.5596112012863159,
      "learning_rate": 5e-05,
      "loss": 0.0971,
      "step": 240
    },
    {
      "epoch": 0.2822014051522248,
      "grad_norm": 1.0023081302642822,
      "learning_rate": 5e-05,
      "loss": 0.2505,
      "step": 241
    },
    {
      "epoch": 0.28337236533957844,
      "grad_norm": 1.0409903526306152,
      "learning_rate": 5e-05,
      "loss": 0.2625,
      "step": 242
    },
    {
      "epoch": 0.28454332552693207,
      "grad_norm": 0.8791027665138245,
      "learning_rate": 5e-05,
      "loss": 0.1999,
      "step": 243
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 1.4206795692443848,
      "learning_rate": 5e-05,
      "loss": 0.3105,
      "step": 244
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 1.10529625415802,
      "learning_rate": 5e-05,
      "loss": 0.285,
      "step": 245
    },
    {
      "epoch": 0.28805620608899296,
      "grad_norm": 0.8031046390533447,
      "learning_rate": 5e-05,
      "loss": 0.1757,
      "step": 246
    },
    {
      "epoch": 0.2892271662763466,
      "grad_norm": 1.1345341205596924,
      "learning_rate": 5e-05,
      "loss": 0.221,
      "step": 247
    },
    {
      "epoch": 0.2903981264637002,
      "grad_norm": 0.7396357655525208,
      "learning_rate": 5e-05,
      "loss": 0.1289,
      "step": 248
    },
    {
      "epoch": 0.29156908665105385,
      "grad_norm": 1.1183576583862305,
      "learning_rate": 5e-05,
      "loss": 0.2422,
      "step": 249
    },
    {
      "epoch": 0.2927400468384075,
      "grad_norm": 1.3137481212615967,
      "learning_rate": 5e-05,
      "loss": 0.3003,
      "step": 250
    },
    {
      "epoch": 0.2939110070257611,
      "grad_norm": 1.1680582761764526,
      "learning_rate": 5e-05,
      "loss": 0.2109,
      "step": 251
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 0.815115213394165,
      "learning_rate": 5e-05,
      "loss": 0.1367,
      "step": 252
    },
    {
      "epoch": 0.2962529274004684,
      "grad_norm": 0.8709319233894348,
      "learning_rate": 5e-05,
      "loss": 0.1839,
      "step": 253
    },
    {
      "epoch": 0.297423887587822,
      "grad_norm": 0.6196947693824768,
      "learning_rate": 5e-05,
      "loss": 0.1766,
      "step": 254
    },
    {
      "epoch": 0.29859484777517564,
      "grad_norm": 0.8150281310081482,
      "learning_rate": 5e-05,
      "loss": 0.2202,
      "step": 255
    },
    {
      "epoch": 0.2997658079625293,
      "grad_norm": 0.8564501404762268,
      "learning_rate": 5e-05,
      "loss": 0.2748,
      "step": 256
    },
    {
      "epoch": 0.3009367681498829,
      "grad_norm": 1.5288991928100586,
      "learning_rate": 5e-05,
      "loss": 0.3191,
      "step": 257
    },
    {
      "epoch": 0.30210772833723654,
      "grad_norm": 0.6199830770492554,
      "learning_rate": 5e-05,
      "loss": 0.1448,
      "step": 258
    },
    {
      "epoch": 0.30327868852459017,
      "grad_norm": 1.1288964748382568,
      "learning_rate": 5e-05,
      "loss": 0.163,
      "step": 259
    },
    {
      "epoch": 0.3044496487119438,
      "grad_norm": 0.8351098895072937,
      "learning_rate": 5e-05,
      "loss": 0.1221,
      "step": 260
    },
    {
      "epoch": 0.30562060889929743,
      "grad_norm": 0.7324774265289307,
      "learning_rate": 5e-05,
      "loss": 0.1295,
      "step": 261
    },
    {
      "epoch": 0.30679156908665106,
      "grad_norm": 0.7884380221366882,
      "learning_rate": 5e-05,
      "loss": 0.1358,
      "step": 262
    },
    {
      "epoch": 0.3079625292740047,
      "grad_norm": 1.162404179573059,
      "learning_rate": 5e-05,
      "loss": 0.2422,
      "step": 263
    },
    {
      "epoch": 0.3091334894613583,
      "grad_norm": 0.9573385715484619,
      "learning_rate": 5e-05,
      "loss": 0.1847,
      "step": 264
    },
    {
      "epoch": 0.31030444964871196,
      "grad_norm": 0.8194458484649658,
      "learning_rate": 5e-05,
      "loss": 0.1799,
      "step": 265
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 1.116303563117981,
      "learning_rate": 5e-05,
      "loss": 0.1821,
      "step": 266
    },
    {
      "epoch": 0.3126463700234192,
      "grad_norm": 0.9703623652458191,
      "learning_rate": 5e-05,
      "loss": 0.1539,
      "step": 267
    },
    {
      "epoch": 0.31381733021077285,
      "grad_norm": 0.9841969609260559,
      "learning_rate": 5e-05,
      "loss": 0.222,
      "step": 268
    },
    {
      "epoch": 0.3149882903981265,
      "grad_norm": 0.9663064479827881,
      "learning_rate": 5e-05,
      "loss": 0.2702,
      "step": 269
    },
    {
      "epoch": 0.3161592505854801,
      "grad_norm": 0.5674263834953308,
      "learning_rate": 5e-05,
      "loss": 0.1168,
      "step": 270
    },
    {
      "epoch": 0.31733021077283374,
      "grad_norm": 1.0058563947677612,
      "learning_rate": 5e-05,
      "loss": 0.1981,
      "step": 271
    },
    {
      "epoch": 0.3185011709601874,
      "grad_norm": 0.7716712355613708,
      "learning_rate": 5e-05,
      "loss": 0.252,
      "step": 272
    },
    {
      "epoch": 0.319672131147541,
      "grad_norm": 0.8082581162452698,
      "learning_rate": 5e-05,
      "loss": 0.1479,
      "step": 273
    },
    {
      "epoch": 0.32084309133489464,
      "grad_norm": 0.9204736948013306,
      "learning_rate": 5e-05,
      "loss": 0.1598,
      "step": 274
    },
    {
      "epoch": 0.32201405152224827,
      "grad_norm": 1.3254423141479492,
      "learning_rate": 5e-05,
      "loss": 0.1717,
      "step": 275
    },
    {
      "epoch": 0.3231850117096019,
      "grad_norm": 1.2265480756759644,
      "learning_rate": 5e-05,
      "loss": 0.1925,
      "step": 276
    },
    {
      "epoch": 0.32435597189695553,
      "grad_norm": 0.8192685842514038,
      "learning_rate": 5e-05,
      "loss": 0.2522,
      "step": 277
    },
    {
      "epoch": 0.3255269320843091,
      "grad_norm": 0.6600170731544495,
      "learning_rate": 5e-05,
      "loss": 0.1088,
      "step": 278
    },
    {
      "epoch": 0.32669789227166274,
      "grad_norm": 0.7531118988990784,
      "learning_rate": 5e-05,
      "loss": 0.1763,
      "step": 279
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 0.9952727556228638,
      "learning_rate": 5e-05,
      "loss": 0.1893,
      "step": 280
    },
    {
      "epoch": 0.32903981264637,
      "grad_norm": 1.3409243822097778,
      "learning_rate": 5e-05,
      "loss": 0.1536,
      "step": 281
    },
    {
      "epoch": 0.33021077283372363,
      "grad_norm": 0.8877723813056946,
      "learning_rate": 5e-05,
      "loss": 0.176,
      "step": 282
    },
    {
      "epoch": 0.33138173302107726,
      "grad_norm": 1.1299899816513062,
      "learning_rate": 5e-05,
      "loss": 0.1677,
      "step": 283
    },
    {
      "epoch": 0.3325526932084309,
      "grad_norm": 1.2417161464691162,
      "learning_rate": 5e-05,
      "loss": 0.1527,
      "step": 284
    },
    {
      "epoch": 0.3337236533957845,
      "grad_norm": 0.8568666577339172,
      "learning_rate": 5e-05,
      "loss": 0.1025,
      "step": 285
    },
    {
      "epoch": 0.33489461358313816,
      "grad_norm": 1.1761068105697632,
      "learning_rate": 5e-05,
      "loss": 0.2194,
      "step": 286
    },
    {
      "epoch": 0.3360655737704918,
      "grad_norm": 0.6158666610717773,
      "learning_rate": 5e-05,
      "loss": 0.0921,
      "step": 287
    },
    {
      "epoch": 0.3372365339578454,
      "grad_norm": 0.7970369458198547,
      "learning_rate": 5e-05,
      "loss": 0.1884,
      "step": 288
    },
    {
      "epoch": 0.33840749414519905,
      "grad_norm": 1.2562289237976074,
      "learning_rate": 5e-05,
      "loss": 0.3271,
      "step": 289
    },
    {
      "epoch": 0.3395784543325527,
      "grad_norm": 1.7467982769012451,
      "learning_rate": 5e-05,
      "loss": 0.2012,
      "step": 290
    },
    {
      "epoch": 0.3407494145199063,
      "grad_norm": 0.9643467664718628,
      "learning_rate": 5e-05,
      "loss": 0.1978,
      "step": 291
    },
    {
      "epoch": 0.34192037470725994,
      "grad_norm": 1.8555964231491089,
      "learning_rate": 5e-05,
      "loss": 0.4528,
      "step": 292
    },
    {
      "epoch": 0.3430913348946136,
      "grad_norm": 1.2701634168624878,
      "learning_rate": 5e-05,
      "loss": 0.2933,
      "step": 293
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 1.1599663496017456,
      "learning_rate": 5e-05,
      "loss": 0.2809,
      "step": 294
    },
    {
      "epoch": 0.34543325526932084,
      "grad_norm": 1.358594536781311,
      "learning_rate": 5e-05,
      "loss": 0.3884,
      "step": 295
    },
    {
      "epoch": 0.34660421545667447,
      "grad_norm": 0.9427188634872437,
      "learning_rate": 5e-05,
      "loss": 0.2145,
      "step": 296
    },
    {
      "epoch": 0.3477751756440281,
      "grad_norm": 1.2458173036575317,
      "learning_rate": 5e-05,
      "loss": 0.1613,
      "step": 297
    },
    {
      "epoch": 0.34894613583138173,
      "grad_norm": 1.3062440156936646,
      "learning_rate": 5e-05,
      "loss": 0.2354,
      "step": 298
    },
    {
      "epoch": 0.35011709601873536,
      "grad_norm": 0.7090517282485962,
      "learning_rate": 5e-05,
      "loss": 0.1874,
      "step": 299
    },
    {
      "epoch": 0.351288056206089,
      "grad_norm": 1.1247384548187256,
      "learning_rate": 5e-05,
      "loss": 0.3011,
      "step": 300
    },
    {
      "epoch": 0.3524590163934426,
      "grad_norm": 0.6808926463127136,
      "learning_rate": 5e-05,
      "loss": 0.161,
      "step": 301
    },
    {
      "epoch": 0.35362997658079626,
      "grad_norm": 0.7662374377250671,
      "learning_rate": 5e-05,
      "loss": 0.145,
      "step": 302
    },
    {
      "epoch": 0.3548009367681499,
      "grad_norm": 0.872136116027832,
      "learning_rate": 5e-05,
      "loss": 0.1511,
      "step": 303
    },
    {
      "epoch": 0.3559718969555035,
      "grad_norm": 1.0382317304611206,
      "learning_rate": 5e-05,
      "loss": 0.2874,
      "step": 304
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 0.7521259784698486,
      "learning_rate": 5e-05,
      "loss": 0.1784,
      "step": 305
    },
    {
      "epoch": 0.3583138173302108,
      "grad_norm": 0.9366226196289062,
      "learning_rate": 5e-05,
      "loss": 0.2101,
      "step": 306
    },
    {
      "epoch": 0.3594847775175644,
      "grad_norm": 0.9546685814857483,
      "learning_rate": 5e-05,
      "loss": 0.2212,
      "step": 307
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 0.9587090015411377,
      "learning_rate": 5e-05,
      "loss": 0.3071,
      "step": 308
    },
    {
      "epoch": 0.3618266978922717,
      "grad_norm": 1.1695137023925781,
      "learning_rate": 5e-05,
      "loss": 0.2704,
      "step": 309
    },
    {
      "epoch": 0.3629976580796253,
      "grad_norm": 0.6276284456253052,
      "learning_rate": 5e-05,
      "loss": 0.1082,
      "step": 310
    },
    {
      "epoch": 0.36416861826697894,
      "grad_norm": 0.8297263979911804,
      "learning_rate": 5e-05,
      "loss": 0.147,
      "step": 311
    },
    {
      "epoch": 0.36533957845433257,
      "grad_norm": 0.821936845779419,
      "learning_rate": 5e-05,
      "loss": 0.1717,
      "step": 312
    },
    {
      "epoch": 0.3665105386416862,
      "grad_norm": 1.220076560974121,
      "learning_rate": 5e-05,
      "loss": 0.1795,
      "step": 313
    },
    {
      "epoch": 0.36768149882903983,
      "grad_norm": 1.2442275285720825,
      "learning_rate": 5e-05,
      "loss": 0.229,
      "step": 314
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 1.1790831089019775,
      "learning_rate": 5e-05,
      "loss": 0.301,
      "step": 315
    },
    {
      "epoch": 0.3700234192037471,
      "grad_norm": 1.223130226135254,
      "learning_rate": 5e-05,
      "loss": 0.1072,
      "step": 316
    },
    {
      "epoch": 0.3711943793911007,
      "grad_norm": 0.6094563603401184,
      "learning_rate": 5e-05,
      "loss": 0.1413,
      "step": 317
    },
    {
      "epoch": 0.37236533957845436,
      "grad_norm": 1.6681251525878906,
      "learning_rate": 5e-05,
      "loss": 0.1701,
      "step": 318
    },
    {
      "epoch": 0.373536299765808,
      "grad_norm": 1.2188060283660889,
      "learning_rate": 5e-05,
      "loss": 0.149,
      "step": 319
    },
    {
      "epoch": 0.3747072599531616,
      "grad_norm": 1.4479701519012451,
      "learning_rate": 5e-05,
      "loss": 0.3974,
      "step": 320
    },
    {
      "epoch": 0.3758782201405152,
      "grad_norm": 1.1586542129516602,
      "learning_rate": 5e-05,
      "loss": 0.2794,
      "step": 321
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 0.9190848469734192,
      "learning_rate": 5e-05,
      "loss": 0.2495,
      "step": 322
    },
    {
      "epoch": 0.37822014051522246,
      "grad_norm": 0.8213471174240112,
      "learning_rate": 5e-05,
      "loss": 0.1872,
      "step": 323
    },
    {
      "epoch": 0.3793911007025761,
      "grad_norm": 1.0497727394104004,
      "learning_rate": 5e-05,
      "loss": 0.2362,
      "step": 324
    },
    {
      "epoch": 0.3805620608899297,
      "grad_norm": 1.9755668640136719,
      "learning_rate": 5e-05,
      "loss": 0.256,
      "step": 325
    },
    {
      "epoch": 0.38173302107728335,
      "grad_norm": 0.7299156785011292,
      "learning_rate": 5e-05,
      "loss": 0.1361,
      "step": 326
    },
    {
      "epoch": 0.382903981264637,
      "grad_norm": 1.1700559854507446,
      "learning_rate": 5e-05,
      "loss": 0.2266,
      "step": 327
    },
    {
      "epoch": 0.3840749414519906,
      "grad_norm": 1.4109866619110107,
      "learning_rate": 5e-05,
      "loss": 0.3779,
      "step": 328
    },
    {
      "epoch": 0.38524590163934425,
      "grad_norm": 0.8116752505302429,
      "learning_rate": 5e-05,
      "loss": 0.2012,
      "step": 329
    },
    {
      "epoch": 0.3864168618266979,
      "grad_norm": 1.19831383228302,
      "learning_rate": 5e-05,
      "loss": 0.1872,
      "step": 330
    },
    {
      "epoch": 0.3875878220140515,
      "grad_norm": 0.9433501362800598,
      "learning_rate": 5e-05,
      "loss": 0.2027,
      "step": 331
    },
    {
      "epoch": 0.38875878220140514,
      "grad_norm": 0.6648943424224854,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 332
    },
    {
      "epoch": 0.38992974238875877,
      "grad_norm": 1.2863585948944092,
      "learning_rate": 5e-05,
      "loss": 0.2449,
      "step": 333
    },
    {
      "epoch": 0.3911007025761124,
      "grad_norm": 0.8190505504608154,
      "learning_rate": 5e-05,
      "loss": 0.2966,
      "step": 334
    },
    {
      "epoch": 0.39227166276346603,
      "grad_norm": 0.7696841359138489,
      "learning_rate": 5e-05,
      "loss": 0.1699,
      "step": 335
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.5312880873680115,
      "learning_rate": 5e-05,
      "loss": 0.0759,
      "step": 336
    },
    {
      "epoch": 0.3946135831381733,
      "grad_norm": 1.0083527565002441,
      "learning_rate": 5e-05,
      "loss": 0.2419,
      "step": 337
    },
    {
      "epoch": 0.3957845433255269,
      "grad_norm": 1.9726855754852295,
      "learning_rate": 5e-05,
      "loss": 0.2738,
      "step": 338
    },
    {
      "epoch": 0.39695550351288056,
      "grad_norm": 1.3978989124298096,
      "learning_rate": 5e-05,
      "loss": 0.2237,
      "step": 339
    },
    {
      "epoch": 0.3981264637002342,
      "grad_norm": 0.7686148881912231,
      "learning_rate": 5e-05,
      "loss": 0.158,
      "step": 340
    },
    {
      "epoch": 0.3992974238875878,
      "grad_norm": 0.9584537744522095,
      "learning_rate": 5e-05,
      "loss": 0.238,
      "step": 341
    },
    {
      "epoch": 0.40046838407494145,
      "grad_norm": 1.082664132118225,
      "learning_rate": 5e-05,
      "loss": 0.2533,
      "step": 342
    },
    {
      "epoch": 0.4016393442622951,
      "grad_norm": 0.7694451808929443,
      "learning_rate": 5e-05,
      "loss": 0.1954,
      "step": 343
    },
    {
      "epoch": 0.4028103044496487,
      "grad_norm": 0.8378486633300781,
      "learning_rate": 5e-05,
      "loss": 0.2498,
      "step": 344
    },
    {
      "epoch": 0.40398126463700235,
      "grad_norm": 0.689791738986969,
      "learning_rate": 5e-05,
      "loss": 0.1495,
      "step": 345
    },
    {
      "epoch": 0.405152224824356,
      "grad_norm": 1.0347574949264526,
      "learning_rate": 5e-05,
      "loss": 0.2174,
      "step": 346
    },
    {
      "epoch": 0.4063231850117096,
      "grad_norm": 1.29342782497406,
      "learning_rate": 5e-05,
      "loss": 0.278,
      "step": 347
    },
    {
      "epoch": 0.40749414519906324,
      "grad_norm": 1.4108498096466064,
      "learning_rate": 5e-05,
      "loss": 0.2907,
      "step": 348
    },
    {
      "epoch": 0.40866510538641687,
      "grad_norm": 0.9927060604095459,
      "learning_rate": 5e-05,
      "loss": 0.1251,
      "step": 349
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 0.9453164339065552,
      "learning_rate": 5e-05,
      "loss": 0.164,
      "step": 350
    },
    {
      "epoch": 0.41100702576112413,
      "grad_norm": 1.014307975769043,
      "learning_rate": 5e-05,
      "loss": 0.226,
      "step": 351
    },
    {
      "epoch": 0.41217798594847777,
      "grad_norm": 1.0853267908096313,
      "learning_rate": 5e-05,
      "loss": 0.2033,
      "step": 352
    },
    {
      "epoch": 0.4133489461358314,
      "grad_norm": 0.9873884916305542,
      "learning_rate": 5e-05,
      "loss": 0.166,
      "step": 353
    },
    {
      "epoch": 0.41451990632318503,
      "grad_norm": 0.5221686959266663,
      "learning_rate": 5e-05,
      "loss": 0.0839,
      "step": 354
    },
    {
      "epoch": 0.41569086651053866,
      "grad_norm": 1.016061782836914,
      "learning_rate": 5e-05,
      "loss": 0.1729,
      "step": 355
    },
    {
      "epoch": 0.4168618266978923,
      "grad_norm": 0.8550485968589783,
      "learning_rate": 5e-05,
      "loss": 0.1035,
      "step": 356
    },
    {
      "epoch": 0.4180327868852459,
      "grad_norm": 0.9558325409889221,
      "learning_rate": 5e-05,
      "loss": 0.2594,
      "step": 357
    },
    {
      "epoch": 0.41920374707259955,
      "grad_norm": 0.9039031267166138,
      "learning_rate": 5e-05,
      "loss": 0.1673,
      "step": 358
    },
    {
      "epoch": 0.4203747072599532,
      "grad_norm": 1.4360183477401733,
      "learning_rate": 5e-05,
      "loss": 0.254,
      "step": 359
    },
    {
      "epoch": 0.4215456674473068,
      "grad_norm": 0.7659235596656799,
      "learning_rate": 5e-05,
      "loss": 0.1305,
      "step": 360
    },
    {
      "epoch": 0.42271662763466045,
      "grad_norm": 1.2922145128250122,
      "learning_rate": 5e-05,
      "loss": 0.1961,
      "step": 361
    },
    {
      "epoch": 0.4238875878220141,
      "grad_norm": 1.1796131134033203,
      "learning_rate": 5e-05,
      "loss": 0.1801,
      "step": 362
    },
    {
      "epoch": 0.42505854800936765,
      "grad_norm": 1.171575665473938,
      "learning_rate": 5e-05,
      "loss": 0.1243,
      "step": 363
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 1.0696163177490234,
      "learning_rate": 5e-05,
      "loss": 0.1611,
      "step": 364
    },
    {
      "epoch": 0.4274004683840749,
      "grad_norm": 1.2386114597320557,
      "learning_rate": 5e-05,
      "loss": 0.2863,
      "step": 365
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.1149060726165771,
      "learning_rate": 5e-05,
      "loss": 0.2159,
      "step": 366
    },
    {
      "epoch": 0.4297423887587822,
      "grad_norm": 1.0797175168991089,
      "learning_rate": 5e-05,
      "loss": 0.3134,
      "step": 367
    },
    {
      "epoch": 0.4309133489461358,
      "grad_norm": 1.1823807954788208,
      "learning_rate": 5e-05,
      "loss": 0.2178,
      "step": 368
    },
    {
      "epoch": 0.43208430913348944,
      "grad_norm": 1.127069354057312,
      "learning_rate": 5e-05,
      "loss": 0.3176,
      "step": 369
    },
    {
      "epoch": 0.4332552693208431,
      "grad_norm": 0.8034687638282776,
      "learning_rate": 5e-05,
      "loss": 0.1222,
      "step": 370
    },
    {
      "epoch": 0.4344262295081967,
      "grad_norm": 1.1844327449798584,
      "learning_rate": 5e-05,
      "loss": 0.2623,
      "step": 371
    },
    {
      "epoch": 0.43559718969555034,
      "grad_norm": 1.2834389209747314,
      "learning_rate": 5e-05,
      "loss": 0.2123,
      "step": 372
    },
    {
      "epoch": 0.43676814988290397,
      "grad_norm": 0.9468677043914795,
      "learning_rate": 5e-05,
      "loss": 0.219,
      "step": 373
    },
    {
      "epoch": 0.4379391100702576,
      "grad_norm": 0.9225346446037292,
      "learning_rate": 5e-05,
      "loss": 0.1226,
      "step": 374
    },
    {
      "epoch": 0.43911007025761123,
      "grad_norm": 0.8512652516365051,
      "learning_rate": 5e-05,
      "loss": 0.1531,
      "step": 375
    },
    {
      "epoch": 0.44028103044496486,
      "grad_norm": 1.3465176820755005,
      "learning_rate": 5e-05,
      "loss": 0.2168,
      "step": 376
    },
    {
      "epoch": 0.4414519906323185,
      "grad_norm": 1.0369926691055298,
      "learning_rate": 5e-05,
      "loss": 0.1585,
      "step": 377
    },
    {
      "epoch": 0.4426229508196721,
      "grad_norm": 0.7083991169929504,
      "learning_rate": 5e-05,
      "loss": 0.1546,
      "step": 378
    },
    {
      "epoch": 0.44379391100702575,
      "grad_norm": 0.8070288300514221,
      "learning_rate": 5e-05,
      "loss": 0.1487,
      "step": 379
    },
    {
      "epoch": 0.4449648711943794,
      "grad_norm": 1.0516401529312134,
      "learning_rate": 5e-05,
      "loss": 0.1661,
      "step": 380
    },
    {
      "epoch": 0.446135831381733,
      "grad_norm": 0.8086187243461609,
      "learning_rate": 5e-05,
      "loss": 0.1154,
      "step": 381
    },
    {
      "epoch": 0.44730679156908665,
      "grad_norm": 0.7257534861564636,
      "learning_rate": 5e-05,
      "loss": 0.139,
      "step": 382
    },
    {
      "epoch": 0.4484777517564403,
      "grad_norm": 1.0179098844528198,
      "learning_rate": 5e-05,
      "loss": 0.1217,
      "step": 383
    },
    {
      "epoch": 0.4496487119437939,
      "grad_norm": 0.9215434193611145,
      "learning_rate": 5e-05,
      "loss": 0.1365,
      "step": 384
    },
    {
      "epoch": 0.45081967213114754,
      "grad_norm": 1.4979742765426636,
      "learning_rate": 5e-05,
      "loss": 0.2659,
      "step": 385
    },
    {
      "epoch": 0.4519906323185012,
      "grad_norm": 0.6532194018363953,
      "learning_rate": 5e-05,
      "loss": 0.1481,
      "step": 386
    },
    {
      "epoch": 0.4531615925058548,
      "grad_norm": 0.9872451424598694,
      "learning_rate": 5e-05,
      "loss": 0.1522,
      "step": 387
    },
    {
      "epoch": 0.45433255269320844,
      "grad_norm": 1.3049442768096924,
      "learning_rate": 5e-05,
      "loss": 0.1594,
      "step": 388
    },
    {
      "epoch": 0.45550351288056207,
      "grad_norm": 0.6751830577850342,
      "learning_rate": 5e-05,
      "loss": 0.1175,
      "step": 389
    },
    {
      "epoch": 0.4566744730679157,
      "grad_norm": 1.9522929191589355,
      "learning_rate": 5e-05,
      "loss": 0.4196,
      "step": 390
    },
    {
      "epoch": 0.45784543325526933,
      "grad_norm": 1.073785424232483,
      "learning_rate": 5e-05,
      "loss": 0.1371,
      "step": 391
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 1.2862513065338135,
      "learning_rate": 5e-05,
      "loss": 0.4012,
      "step": 392
    },
    {
      "epoch": 0.4601873536299766,
      "grad_norm": 0.7993120551109314,
      "learning_rate": 5e-05,
      "loss": 0.1195,
      "step": 393
    },
    {
      "epoch": 0.4613583138173302,
      "grad_norm": 0.7543487548828125,
      "learning_rate": 5e-05,
      "loss": 0.0721,
      "step": 394
    },
    {
      "epoch": 0.46252927400468385,
      "grad_norm": 1.2492098808288574,
      "learning_rate": 5e-05,
      "loss": 0.1716,
      "step": 395
    },
    {
      "epoch": 0.4637002341920375,
      "grad_norm": 0.8402919769287109,
      "learning_rate": 5e-05,
      "loss": 0.179,
      "step": 396
    },
    {
      "epoch": 0.4648711943793911,
      "grad_norm": 0.7483439445495605,
      "learning_rate": 5e-05,
      "loss": 0.2723,
      "step": 397
    },
    {
      "epoch": 0.46604215456674475,
      "grad_norm": 0.9640482068061829,
      "learning_rate": 5e-05,
      "loss": 0.0966,
      "step": 398
    },
    {
      "epoch": 0.4672131147540984,
      "grad_norm": 0.8265079855918884,
      "learning_rate": 5e-05,
      "loss": 0.144,
      "step": 399
    },
    {
      "epoch": 0.468384074941452,
      "grad_norm": 0.7526500225067139,
      "learning_rate": 5e-05,
      "loss": 0.0968,
      "step": 400
    },
    {
      "epoch": 0.46955503512880564,
      "grad_norm": 1.6386750936508179,
      "learning_rate": 5e-05,
      "loss": 0.3129,
      "step": 401
    },
    {
      "epoch": 0.4707259953161593,
      "grad_norm": 0.9946482181549072,
      "learning_rate": 5e-05,
      "loss": 0.1551,
      "step": 402
    },
    {
      "epoch": 0.4718969555035129,
      "grad_norm": 0.5665644407272339,
      "learning_rate": 5e-05,
      "loss": 0.0758,
      "step": 403
    },
    {
      "epoch": 0.47306791569086654,
      "grad_norm": 0.9031678438186646,
      "learning_rate": 5e-05,
      "loss": 0.1532,
      "step": 404
    },
    {
      "epoch": 0.47423887587822017,
      "grad_norm": 0.9003222584724426,
      "learning_rate": 5e-05,
      "loss": 0.2122,
      "step": 405
    },
    {
      "epoch": 0.47540983606557374,
      "grad_norm": 0.9529834389686584,
      "learning_rate": 5e-05,
      "loss": 0.1639,
      "step": 406
    },
    {
      "epoch": 0.4765807962529274,
      "grad_norm": 0.9651116728782654,
      "learning_rate": 5e-05,
      "loss": 0.2302,
      "step": 407
    },
    {
      "epoch": 0.477751756440281,
      "grad_norm": 1.0952123403549194,
      "learning_rate": 5e-05,
      "loss": 0.1657,
      "step": 408
    },
    {
      "epoch": 0.47892271662763464,
      "grad_norm": 0.8789905309677124,
      "learning_rate": 5e-05,
      "loss": 0.2191,
      "step": 409
    },
    {
      "epoch": 0.48009367681498827,
      "grad_norm": 1.7677239179611206,
      "learning_rate": 5e-05,
      "loss": 0.1972,
      "step": 410
    },
    {
      "epoch": 0.4812646370023419,
      "grad_norm": 0.8753963708877563,
      "learning_rate": 5e-05,
      "loss": 0.0908,
      "step": 411
    },
    {
      "epoch": 0.48243559718969553,
      "grad_norm": 0.695610523223877,
      "learning_rate": 5e-05,
      "loss": 0.1888,
      "step": 412
    },
    {
      "epoch": 0.48360655737704916,
      "grad_norm": 1.1923125982284546,
      "learning_rate": 5e-05,
      "loss": 0.2178,
      "step": 413
    },
    {
      "epoch": 0.4847775175644028,
      "grad_norm": 0.5678662657737732,
      "learning_rate": 5e-05,
      "loss": 0.0906,
      "step": 414
    },
    {
      "epoch": 0.4859484777517564,
      "grad_norm": 1.046420693397522,
      "learning_rate": 5e-05,
      "loss": 0.1815,
      "step": 415
    },
    {
      "epoch": 0.48711943793911006,
      "grad_norm": 1.1379776000976562,
      "learning_rate": 5e-05,
      "loss": 0.3071,
      "step": 416
    },
    {
      "epoch": 0.4882903981264637,
      "grad_norm": 0.9790414571762085,
      "learning_rate": 5e-05,
      "loss": 0.2178,
      "step": 417
    },
    {
      "epoch": 0.4894613583138173,
      "grad_norm": 1.0611685514450073,
      "learning_rate": 5e-05,
      "loss": 0.1769,
      "step": 418
    },
    {
      "epoch": 0.49063231850117095,
      "grad_norm": 1.84306001663208,
      "learning_rate": 5e-05,
      "loss": 0.8391,
      "step": 419
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 1.1006109714508057,
      "learning_rate": 5e-05,
      "loss": 0.2292,
      "step": 420
    },
    {
      "epoch": 0.4929742388758782,
      "grad_norm": 1.1441748142242432,
      "learning_rate": 5e-05,
      "loss": 0.2123,
      "step": 421
    },
    {
      "epoch": 0.49414519906323184,
      "grad_norm": 1.1577527523040771,
      "learning_rate": 5e-05,
      "loss": 0.24,
      "step": 422
    },
    {
      "epoch": 0.4953161592505855,
      "grad_norm": 0.49567511677742004,
      "learning_rate": 5e-05,
      "loss": 0.0934,
      "step": 423
    },
    {
      "epoch": 0.4964871194379391,
      "grad_norm": 0.629272997379303,
      "learning_rate": 5e-05,
      "loss": 0.1845,
      "step": 424
    },
    {
      "epoch": 0.49765807962529274,
      "grad_norm": 0.5377404689788818,
      "learning_rate": 5e-05,
      "loss": 0.165,
      "step": 425
    },
    {
      "epoch": 0.49882903981264637,
      "grad_norm": 0.9602814316749573,
      "learning_rate": 5e-05,
      "loss": 0.2033,
      "step": 426
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8518045544624329,
      "learning_rate": 5e-05,
      "loss": 0.1359,
      "step": 427
    },
    {
      "epoch": 0.5011709601873536,
      "grad_norm": 0.8103641867637634,
      "learning_rate": 5e-05,
      "loss": 0.1632,
      "step": 428
    },
    {
      "epoch": 0.5023419203747073,
      "grad_norm": 1.261052131652832,
      "learning_rate": 5e-05,
      "loss": 0.3159,
      "step": 429
    },
    {
      "epoch": 0.5035128805620609,
      "grad_norm": 1.0644497871398926,
      "learning_rate": 5e-05,
      "loss": 0.3552,
      "step": 430
    },
    {
      "epoch": 0.5046838407494145,
      "grad_norm": 0.9725793600082397,
      "learning_rate": 5e-05,
      "loss": 0.1781,
      "step": 431
    },
    {
      "epoch": 0.5058548009367682,
      "grad_norm": 0.5316311717033386,
      "learning_rate": 5e-05,
      "loss": 0.0994,
      "step": 432
    },
    {
      "epoch": 0.5070257611241218,
      "grad_norm": 0.9271491169929504,
      "learning_rate": 5e-05,
      "loss": 0.1793,
      "step": 433
    },
    {
      "epoch": 0.5081967213114754,
      "grad_norm": 1.004918098449707,
      "learning_rate": 5e-05,
      "loss": 0.1734,
      "step": 434
    },
    {
      "epoch": 0.509367681498829,
      "grad_norm": 0.6243261098861694,
      "learning_rate": 5e-05,
      "loss": 0.0953,
      "step": 435
    },
    {
      "epoch": 0.5105386416861827,
      "grad_norm": 0.9663435220718384,
      "learning_rate": 5e-05,
      "loss": 0.2677,
      "step": 436
    },
    {
      "epoch": 0.5117096018735363,
      "grad_norm": 1.0352445840835571,
      "learning_rate": 5e-05,
      "loss": 0.2642,
      "step": 437
    },
    {
      "epoch": 0.5128805620608899,
      "grad_norm": 0.9381771683692932,
      "learning_rate": 5e-05,
      "loss": 0.2021,
      "step": 438
    },
    {
      "epoch": 0.5140515222482436,
      "grad_norm": 0.876467764377594,
      "learning_rate": 5e-05,
      "loss": 0.2314,
      "step": 439
    },
    {
      "epoch": 0.5152224824355972,
      "grad_norm": 0.6904439330101013,
      "learning_rate": 5e-05,
      "loss": 0.0972,
      "step": 440
    },
    {
      "epoch": 0.5163934426229508,
      "grad_norm": 0.7310240864753723,
      "learning_rate": 5e-05,
      "loss": 0.2127,
      "step": 441
    },
    {
      "epoch": 0.5175644028103045,
      "grad_norm": 1.2319086790084839,
      "learning_rate": 5e-05,
      "loss": 0.4819,
      "step": 442
    },
    {
      "epoch": 0.5187353629976581,
      "grad_norm": 0.8333838582038879,
      "learning_rate": 5e-05,
      "loss": 0.2644,
      "step": 443
    },
    {
      "epoch": 0.5199063231850117,
      "grad_norm": 0.9104984998703003,
      "learning_rate": 5e-05,
      "loss": 0.13,
      "step": 444
    },
    {
      "epoch": 0.5210772833723654,
      "grad_norm": 0.7133839130401611,
      "learning_rate": 5e-05,
      "loss": 0.1261,
      "step": 445
    },
    {
      "epoch": 0.522248243559719,
      "grad_norm": 0.8257941007614136,
      "learning_rate": 5e-05,
      "loss": 0.1392,
      "step": 446
    },
    {
      "epoch": 0.5234192037470726,
      "grad_norm": 0.7511181235313416,
      "learning_rate": 5e-05,
      "loss": 0.1611,
      "step": 447
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 0.6908153891563416,
      "learning_rate": 5e-05,
      "loss": 0.0659,
      "step": 448
    },
    {
      "epoch": 0.5257611241217799,
      "grad_norm": 1.21065092086792,
      "learning_rate": 5e-05,
      "loss": 0.2304,
      "step": 449
    },
    {
      "epoch": 0.5269320843091335,
      "grad_norm": 0.653860867023468,
      "learning_rate": 5e-05,
      "loss": 0.1457,
      "step": 450
    },
    {
      "epoch": 0.5281030444964872,
      "grad_norm": 1.2576342821121216,
      "learning_rate": 5e-05,
      "loss": 0.1937,
      "step": 451
    },
    {
      "epoch": 0.5292740046838408,
      "grad_norm": 0.622241199016571,
      "learning_rate": 5e-05,
      "loss": 0.0883,
      "step": 452
    },
    {
      "epoch": 0.5304449648711944,
      "grad_norm": 0.7924981713294983,
      "learning_rate": 5e-05,
      "loss": 0.1139,
      "step": 453
    },
    {
      "epoch": 0.531615925058548,
      "grad_norm": 0.9035850763320923,
      "learning_rate": 5e-05,
      "loss": 0.2618,
      "step": 454
    },
    {
      "epoch": 0.5327868852459017,
      "grad_norm": 1.2416880130767822,
      "learning_rate": 5e-05,
      "loss": 0.1359,
      "step": 455
    },
    {
      "epoch": 0.5339578454332553,
      "grad_norm": 0.7408509254455566,
      "learning_rate": 5e-05,
      "loss": 0.1257,
      "step": 456
    },
    {
      "epoch": 0.5351288056206089,
      "grad_norm": 1.0189204216003418,
      "learning_rate": 5e-05,
      "loss": 0.079,
      "step": 457
    },
    {
      "epoch": 0.5362997658079626,
      "grad_norm": 0.7394603490829468,
      "learning_rate": 5e-05,
      "loss": 0.1165,
      "step": 458
    },
    {
      "epoch": 0.5374707259953162,
      "grad_norm": 1.160362720489502,
      "learning_rate": 5e-05,
      "loss": 0.246,
      "step": 459
    },
    {
      "epoch": 0.5386416861826698,
      "grad_norm": 0.5929608941078186,
      "learning_rate": 5e-05,
      "loss": 0.0771,
      "step": 460
    },
    {
      "epoch": 0.5398126463700235,
      "grad_norm": 0.8025851249694824,
      "learning_rate": 5e-05,
      "loss": 0.113,
      "step": 461
    },
    {
      "epoch": 0.5409836065573771,
      "grad_norm": 0.7403103709220886,
      "learning_rate": 5e-05,
      "loss": 0.1039,
      "step": 462
    },
    {
      "epoch": 0.5421545667447307,
      "grad_norm": 0.7334148287773132,
      "learning_rate": 5e-05,
      "loss": 0.1096,
      "step": 463
    },
    {
      "epoch": 0.5433255269320844,
      "grad_norm": 0.9092703461647034,
      "learning_rate": 5e-05,
      "loss": 0.1412,
      "step": 464
    },
    {
      "epoch": 0.544496487119438,
      "grad_norm": 0.905539870262146,
      "learning_rate": 5e-05,
      "loss": 0.0763,
      "step": 465
    },
    {
      "epoch": 0.5456674473067916,
      "grad_norm": 0.8486769795417786,
      "learning_rate": 5e-05,
      "loss": 0.2182,
      "step": 466
    },
    {
      "epoch": 0.5468384074941453,
      "grad_norm": 0.8408458828926086,
      "learning_rate": 5e-05,
      "loss": 0.1256,
      "step": 467
    },
    {
      "epoch": 0.5480093676814989,
      "grad_norm": 1.32437002658844,
      "learning_rate": 5e-05,
      "loss": 0.1794,
      "step": 468
    },
    {
      "epoch": 0.5491803278688525,
      "grad_norm": 0.7309964299201965,
      "learning_rate": 5e-05,
      "loss": 0.0797,
      "step": 469
    },
    {
      "epoch": 0.550351288056206,
      "grad_norm": 1.1398450136184692,
      "learning_rate": 5e-05,
      "loss": 0.2465,
      "step": 470
    },
    {
      "epoch": 0.5515222482435597,
      "grad_norm": 1.339651346206665,
      "learning_rate": 5e-05,
      "loss": 0.2775,
      "step": 471
    },
    {
      "epoch": 0.5526932084309133,
      "grad_norm": 1.0791654586791992,
      "learning_rate": 5e-05,
      "loss": 0.1889,
      "step": 472
    },
    {
      "epoch": 0.5538641686182669,
      "grad_norm": 1.3196585178375244,
      "learning_rate": 5e-05,
      "loss": 0.0944,
      "step": 473
    },
    {
      "epoch": 0.5550351288056206,
      "grad_norm": 0.8331947326660156,
      "learning_rate": 5e-05,
      "loss": 0.2698,
      "step": 474
    },
    {
      "epoch": 0.5562060889929742,
      "grad_norm": 1.0622767210006714,
      "learning_rate": 5e-05,
      "loss": 0.154,
      "step": 475
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 1.0703495740890503,
      "learning_rate": 5e-05,
      "loss": 0.1943,
      "step": 476
    },
    {
      "epoch": 0.5585480093676815,
      "grad_norm": 0.548612654209137,
      "learning_rate": 5e-05,
      "loss": 0.1177,
      "step": 477
    },
    {
      "epoch": 0.5597189695550351,
      "grad_norm": 1.2618077993392944,
      "learning_rate": 5e-05,
      "loss": 0.3037,
      "step": 478
    },
    {
      "epoch": 0.5608899297423887,
      "grad_norm": 0.7599453926086426,
      "learning_rate": 5e-05,
      "loss": 0.1158,
      "step": 479
    },
    {
      "epoch": 0.5620608899297423,
      "grad_norm": 0.8712602853775024,
      "learning_rate": 5e-05,
      "loss": 0.225,
      "step": 480
    },
    {
      "epoch": 0.563231850117096,
      "grad_norm": 0.4985368847846985,
      "learning_rate": 5e-05,
      "loss": 0.1914,
      "step": 481
    },
    {
      "epoch": 0.5644028103044496,
      "grad_norm": 0.8336830735206604,
      "learning_rate": 5e-05,
      "loss": 0.1702,
      "step": 482
    },
    {
      "epoch": 0.5655737704918032,
      "grad_norm": 0.8304510712623596,
      "learning_rate": 5e-05,
      "loss": 0.2102,
      "step": 483
    },
    {
      "epoch": 0.5667447306791569,
      "grad_norm": 0.8106937408447266,
      "learning_rate": 5e-05,
      "loss": 0.1973,
      "step": 484
    },
    {
      "epoch": 0.5679156908665105,
      "grad_norm": 0.7826782464981079,
      "learning_rate": 5e-05,
      "loss": 0.1481,
      "step": 485
    },
    {
      "epoch": 0.5690866510538641,
      "grad_norm": 1.095781683921814,
      "learning_rate": 5e-05,
      "loss": 0.1422,
      "step": 486
    },
    {
      "epoch": 0.5702576112412178,
      "grad_norm": 1.0501593351364136,
      "learning_rate": 5e-05,
      "loss": 0.1119,
      "step": 487
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.0661882162094116,
      "learning_rate": 5e-05,
      "loss": 0.2392,
      "step": 488
    },
    {
      "epoch": 0.572599531615925,
      "grad_norm": 1.163630485534668,
      "learning_rate": 5e-05,
      "loss": 0.1564,
      "step": 489
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 1.0264490842819214,
      "learning_rate": 5e-05,
      "loss": 0.1669,
      "step": 490
    },
    {
      "epoch": 0.5749414519906323,
      "grad_norm": 1.076097011566162,
      "learning_rate": 5e-05,
      "loss": 0.2201,
      "step": 491
    },
    {
      "epoch": 0.5761124121779859,
      "grad_norm": 1.100126028060913,
      "learning_rate": 5e-05,
      "loss": 0.2089,
      "step": 492
    },
    {
      "epoch": 0.5772833723653396,
      "grad_norm": 1.2737339735031128,
      "learning_rate": 5e-05,
      "loss": 0.3568,
      "step": 493
    },
    {
      "epoch": 0.5784543325526932,
      "grad_norm": 0.9025272727012634,
      "learning_rate": 5e-05,
      "loss": 0.1352,
      "step": 494
    },
    {
      "epoch": 0.5796252927400468,
      "grad_norm": 0.8779374361038208,
      "learning_rate": 5e-05,
      "loss": 0.1529,
      "step": 495
    },
    {
      "epoch": 0.5807962529274004,
      "grad_norm": 1.0999667644500732,
      "learning_rate": 5e-05,
      "loss": 0.1415,
      "step": 496
    },
    {
      "epoch": 0.5819672131147541,
      "grad_norm": 1.0017880201339722,
      "learning_rate": 5e-05,
      "loss": 0.1254,
      "step": 497
    },
    {
      "epoch": 0.5831381733021077,
      "grad_norm": 1.3621774911880493,
      "learning_rate": 5e-05,
      "loss": 0.0854,
      "step": 498
    },
    {
      "epoch": 0.5843091334894613,
      "grad_norm": 1.0528817176818848,
      "learning_rate": 5e-05,
      "loss": 0.2735,
      "step": 499
    },
    {
      "epoch": 0.585480093676815,
      "grad_norm": 1.2192304134368896,
      "learning_rate": 5e-05,
      "loss": 0.2059,
      "step": 500
    },
    {
      "epoch": 0.5866510538641686,
      "grad_norm": 1.055724024772644,
      "learning_rate": 5e-05,
      "loss": 0.1432,
      "step": 501
    },
    {
      "epoch": 0.5878220140515222,
      "grad_norm": 0.8838439583778381,
      "learning_rate": 5e-05,
      "loss": 0.173,
      "step": 502
    },
    {
      "epoch": 0.5889929742388759,
      "grad_norm": 0.8295734524726868,
      "learning_rate": 5e-05,
      "loss": 0.1445,
      "step": 503
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 1.5232207775115967,
      "learning_rate": 5e-05,
      "loss": 0.1451,
      "step": 504
    },
    {
      "epoch": 0.5913348946135831,
      "grad_norm": 1.0858200788497925,
      "learning_rate": 5e-05,
      "loss": 0.1854,
      "step": 505
    },
    {
      "epoch": 0.5925058548009368,
      "grad_norm": 0.93076092004776,
      "learning_rate": 5e-05,
      "loss": 0.1649,
      "step": 506
    },
    {
      "epoch": 0.5936768149882904,
      "grad_norm": 0.6938176155090332,
      "learning_rate": 5e-05,
      "loss": 0.1153,
      "step": 507
    },
    {
      "epoch": 0.594847775175644,
      "grad_norm": 0.9901054501533508,
      "learning_rate": 5e-05,
      "loss": 0.1539,
      "step": 508
    },
    {
      "epoch": 0.5960187353629977,
      "grad_norm": 0.8014410138130188,
      "learning_rate": 5e-05,
      "loss": 0.1719,
      "step": 509
    },
    {
      "epoch": 0.5971896955503513,
      "grad_norm": 0.8002728223800659,
      "learning_rate": 5e-05,
      "loss": 0.1465,
      "step": 510
    },
    {
      "epoch": 0.5983606557377049,
      "grad_norm": 0.8149044513702393,
      "learning_rate": 5e-05,
      "loss": 0.1931,
      "step": 511
    },
    {
      "epoch": 0.5995316159250585,
      "grad_norm": 0.9445458650588989,
      "learning_rate": 5e-05,
      "loss": 0.2626,
      "step": 512
    },
    {
      "epoch": 0.6007025761124122,
      "grad_norm": 0.7923038601875305,
      "learning_rate": 5e-05,
      "loss": 0.0889,
      "step": 513
    },
    {
      "epoch": 0.6018735362997658,
      "grad_norm": 0.9929264783859253,
      "learning_rate": 5e-05,
      "loss": 0.1395,
      "step": 514
    },
    {
      "epoch": 0.6030444964871194,
      "grad_norm": 1.182382583618164,
      "learning_rate": 5e-05,
      "loss": 0.3015,
      "step": 515
    },
    {
      "epoch": 0.6042154566744731,
      "grad_norm": 0.8427441716194153,
      "learning_rate": 5e-05,
      "loss": 0.1947,
      "step": 516
    },
    {
      "epoch": 0.6053864168618267,
      "grad_norm": 0.9242683053016663,
      "learning_rate": 5e-05,
      "loss": 0.1767,
      "step": 517
    },
    {
      "epoch": 0.6065573770491803,
      "grad_norm": 1.228156566619873,
      "learning_rate": 5e-05,
      "loss": 0.1498,
      "step": 518
    },
    {
      "epoch": 0.607728337236534,
      "grad_norm": 0.6807793974876404,
      "learning_rate": 5e-05,
      "loss": 0.1229,
      "step": 519
    },
    {
      "epoch": 0.6088992974238876,
      "grad_norm": 1.0770968198776245,
      "learning_rate": 5e-05,
      "loss": 0.1404,
      "step": 520
    },
    {
      "epoch": 0.6100702576112412,
      "grad_norm": 0.9881559014320374,
      "learning_rate": 5e-05,
      "loss": 0.1556,
      "step": 521
    },
    {
      "epoch": 0.6112412177985949,
      "grad_norm": 1.3535584211349487,
      "learning_rate": 5e-05,
      "loss": 0.2113,
      "step": 522
    },
    {
      "epoch": 0.6124121779859485,
      "grad_norm": 0.8340993523597717,
      "learning_rate": 5e-05,
      "loss": 0.1899,
      "step": 523
    },
    {
      "epoch": 0.6135831381733021,
      "grad_norm": 1.0335068702697754,
      "learning_rate": 5e-05,
      "loss": 0.2511,
      "step": 524
    },
    {
      "epoch": 0.6147540983606558,
      "grad_norm": 1.1316537857055664,
      "learning_rate": 5e-05,
      "loss": 0.1867,
      "step": 525
    },
    {
      "epoch": 0.6159250585480094,
      "grad_norm": 0.8552352786064148,
      "learning_rate": 5e-05,
      "loss": 0.1275,
      "step": 526
    },
    {
      "epoch": 0.617096018735363,
      "grad_norm": 1.0018441677093506,
      "learning_rate": 5e-05,
      "loss": 0.1996,
      "step": 527
    },
    {
      "epoch": 0.6182669789227166,
      "grad_norm": 0.9891950488090515,
      "learning_rate": 5e-05,
      "loss": 0.2389,
      "step": 528
    },
    {
      "epoch": 0.6194379391100703,
      "grad_norm": 0.671401858329773,
      "learning_rate": 5e-05,
      "loss": 0.1116,
      "step": 529
    },
    {
      "epoch": 0.6206088992974239,
      "grad_norm": 0.789700448513031,
      "learning_rate": 5e-05,
      "loss": 0.1037,
      "step": 530
    },
    {
      "epoch": 0.6217798594847775,
      "grad_norm": 0.8998275399208069,
      "learning_rate": 5e-05,
      "loss": 0.201,
      "step": 531
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 0.6795015931129456,
      "learning_rate": 5e-05,
      "loss": 0.1995,
      "step": 532
    },
    {
      "epoch": 0.6241217798594848,
      "grad_norm": 1.2574182748794556,
      "learning_rate": 5e-05,
      "loss": 0.191,
      "step": 533
    },
    {
      "epoch": 0.6252927400468384,
      "grad_norm": 0.6443217396736145,
      "learning_rate": 5e-05,
      "loss": 0.1125,
      "step": 534
    },
    {
      "epoch": 0.6264637002341921,
      "grad_norm": 1.210105061531067,
      "learning_rate": 5e-05,
      "loss": 0.3849,
      "step": 535
    },
    {
      "epoch": 0.6276346604215457,
      "grad_norm": 1.2152644395828247,
      "learning_rate": 5e-05,
      "loss": 0.2097,
      "step": 536
    },
    {
      "epoch": 0.6288056206088993,
      "grad_norm": 0.5208961367607117,
      "learning_rate": 5e-05,
      "loss": 0.1057,
      "step": 537
    },
    {
      "epoch": 0.629976580796253,
      "grad_norm": 0.704797625541687,
      "learning_rate": 5e-05,
      "loss": 0.0853,
      "step": 538
    },
    {
      "epoch": 0.6311475409836066,
      "grad_norm": 0.6334642767906189,
      "learning_rate": 5e-05,
      "loss": 0.16,
      "step": 539
    },
    {
      "epoch": 0.6323185011709602,
      "grad_norm": 1.1453953981399536,
      "learning_rate": 5e-05,
      "loss": 0.1863,
      "step": 540
    },
    {
      "epoch": 0.6334894613583139,
      "grad_norm": 0.9565908312797546,
      "learning_rate": 5e-05,
      "loss": 0.0968,
      "step": 541
    },
    {
      "epoch": 0.6346604215456675,
      "grad_norm": 0.6604236960411072,
      "learning_rate": 5e-05,
      "loss": 0.0918,
      "step": 542
    },
    {
      "epoch": 0.6358313817330211,
      "grad_norm": 1.082199215888977,
      "learning_rate": 5e-05,
      "loss": 0.281,
      "step": 543
    },
    {
      "epoch": 0.6370023419203747,
      "grad_norm": 0.7068102955818176,
      "learning_rate": 5e-05,
      "loss": 0.0798,
      "step": 544
    },
    {
      "epoch": 0.6381733021077284,
      "grad_norm": 0.7484638690948486,
      "learning_rate": 5e-05,
      "loss": 0.0969,
      "step": 545
    },
    {
      "epoch": 0.639344262295082,
      "grad_norm": 1.030338168144226,
      "learning_rate": 5e-05,
      "loss": 0.1998,
      "step": 546
    },
    {
      "epoch": 0.6405152224824356,
      "grad_norm": 0.5279579758644104,
      "learning_rate": 5e-05,
      "loss": 0.0906,
      "step": 547
    },
    {
      "epoch": 0.6416861826697893,
      "grad_norm": 1.2763831615447998,
      "learning_rate": 5e-05,
      "loss": 0.21,
      "step": 548
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 1.5399115085601807,
      "learning_rate": 5e-05,
      "loss": 0.2831,
      "step": 549
    },
    {
      "epoch": 0.6440281030444965,
      "grad_norm": 0.9222328066825867,
      "learning_rate": 5e-05,
      "loss": 0.1769,
      "step": 550
    },
    {
      "epoch": 0.6451990632318502,
      "grad_norm": 0.669649600982666,
      "learning_rate": 5e-05,
      "loss": 0.0766,
      "step": 551
    },
    {
      "epoch": 0.6463700234192038,
      "grad_norm": 1.075807809829712,
      "learning_rate": 5e-05,
      "loss": 0.141,
      "step": 552
    },
    {
      "epoch": 0.6475409836065574,
      "grad_norm": 0.8822349309921265,
      "learning_rate": 5e-05,
      "loss": 0.1353,
      "step": 553
    },
    {
      "epoch": 0.6487119437939111,
      "grad_norm": 0.8028813004493713,
      "learning_rate": 5e-05,
      "loss": 0.1311,
      "step": 554
    },
    {
      "epoch": 0.6498829039812647,
      "grad_norm": 0.9644811749458313,
      "learning_rate": 5e-05,
      "loss": 0.1557,
      "step": 555
    },
    {
      "epoch": 0.6510538641686182,
      "grad_norm": 1.3473557233810425,
      "learning_rate": 5e-05,
      "loss": 0.247,
      "step": 556
    },
    {
      "epoch": 0.6522248243559718,
      "grad_norm": 1.0131683349609375,
      "learning_rate": 5e-05,
      "loss": 0.1815,
      "step": 557
    },
    {
      "epoch": 0.6533957845433255,
      "grad_norm": 1.5771515369415283,
      "learning_rate": 5e-05,
      "loss": 0.1376,
      "step": 558
    },
    {
      "epoch": 0.6545667447306791,
      "grad_norm": 0.6172873377799988,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 559
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 0.7690135836601257,
      "learning_rate": 5e-05,
      "loss": 0.1465,
      "step": 560
    },
    {
      "epoch": 0.6569086651053864,
      "grad_norm": 1.0294885635375977,
      "learning_rate": 5e-05,
      "loss": 0.1908,
      "step": 561
    },
    {
      "epoch": 0.65807962529274,
      "grad_norm": 0.8878417015075684,
      "learning_rate": 5e-05,
      "loss": 0.1475,
      "step": 562
    },
    {
      "epoch": 0.6592505854800936,
      "grad_norm": 0.9834650754928589,
      "learning_rate": 5e-05,
      "loss": 0.1868,
      "step": 563
    },
    {
      "epoch": 0.6604215456674473,
      "grad_norm": 0.6735959053039551,
      "learning_rate": 5e-05,
      "loss": 0.1426,
      "step": 564
    },
    {
      "epoch": 0.6615925058548009,
      "grad_norm": 0.7914571166038513,
      "learning_rate": 5e-05,
      "loss": 0.1102,
      "step": 565
    },
    {
      "epoch": 0.6627634660421545,
      "grad_norm": 0.6025668382644653,
      "learning_rate": 5e-05,
      "loss": 0.0895,
      "step": 566
    },
    {
      "epoch": 0.6639344262295082,
      "grad_norm": 0.9930461645126343,
      "learning_rate": 5e-05,
      "loss": 0.1273,
      "step": 567
    },
    {
      "epoch": 0.6651053864168618,
      "grad_norm": 0.6427677273750305,
      "learning_rate": 5e-05,
      "loss": 0.1884,
      "step": 568
    },
    {
      "epoch": 0.6662763466042154,
      "grad_norm": 0.7857393622398376,
      "learning_rate": 5e-05,
      "loss": 0.0707,
      "step": 569
    },
    {
      "epoch": 0.667447306791569,
      "grad_norm": 1.440730094909668,
      "learning_rate": 5e-05,
      "loss": 0.3812,
      "step": 570
    },
    {
      "epoch": 0.6686182669789227,
      "grad_norm": 1.1486625671386719,
      "learning_rate": 5e-05,
      "loss": 0.1801,
      "step": 571
    },
    {
      "epoch": 0.6697892271662763,
      "grad_norm": 1.4806309938430786,
      "learning_rate": 5e-05,
      "loss": 0.2025,
      "step": 572
    },
    {
      "epoch": 0.6709601873536299,
      "grad_norm": 0.8460021615028381,
      "learning_rate": 5e-05,
      "loss": 0.0895,
      "step": 573
    },
    {
      "epoch": 0.6721311475409836,
      "grad_norm": 1.9783200025558472,
      "learning_rate": 5e-05,
      "loss": 0.3134,
      "step": 574
    },
    {
      "epoch": 0.6733021077283372,
      "grad_norm": 0.5956102609634399,
      "learning_rate": 5e-05,
      "loss": 0.1112,
      "step": 575
    },
    {
      "epoch": 0.6744730679156908,
      "grad_norm": 0.6547172665596008,
      "learning_rate": 5e-05,
      "loss": 0.1153,
      "step": 576
    },
    {
      "epoch": 0.6756440281030445,
      "grad_norm": 1.2680208683013916,
      "learning_rate": 5e-05,
      "loss": 0.2482,
      "step": 577
    },
    {
      "epoch": 0.6768149882903981,
      "grad_norm": 0.8077220320701599,
      "learning_rate": 5e-05,
      "loss": 0.1151,
      "step": 578
    },
    {
      "epoch": 0.6779859484777517,
      "grad_norm": 0.9157699346542358,
      "learning_rate": 5e-05,
      "loss": 0.1932,
      "step": 579
    },
    {
      "epoch": 0.6791569086651054,
      "grad_norm": 0.9500836133956909,
      "learning_rate": 5e-05,
      "loss": 0.1624,
      "step": 580
    },
    {
      "epoch": 0.680327868852459,
      "grad_norm": 1.227997899055481,
      "learning_rate": 5e-05,
      "loss": 0.1026,
      "step": 581
    },
    {
      "epoch": 0.6814988290398126,
      "grad_norm": 1.2022666931152344,
      "learning_rate": 5e-05,
      "loss": 0.1828,
      "step": 582
    },
    {
      "epoch": 0.6826697892271663,
      "grad_norm": 0.732578694820404,
      "learning_rate": 5e-05,
      "loss": 0.095,
      "step": 583
    },
    {
      "epoch": 0.6838407494145199,
      "grad_norm": 1.2066528797149658,
      "learning_rate": 5e-05,
      "loss": 0.2219,
      "step": 584
    },
    {
      "epoch": 0.6850117096018735,
      "grad_norm": 1.076611876487732,
      "learning_rate": 5e-05,
      "loss": 0.2063,
      "step": 585
    },
    {
      "epoch": 0.6861826697892272,
      "grad_norm": 1.1938035488128662,
      "learning_rate": 5e-05,
      "loss": 0.1722,
      "step": 586
    },
    {
      "epoch": 0.6873536299765808,
      "grad_norm": 0.8155039548873901,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 587
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 1.1044288873672485,
      "learning_rate": 5e-05,
      "loss": 0.0871,
      "step": 588
    },
    {
      "epoch": 0.689695550351288,
      "grad_norm": 0.8702663779258728,
      "learning_rate": 5e-05,
      "loss": 0.1759,
      "step": 589
    },
    {
      "epoch": 0.6908665105386417,
      "grad_norm": 0.5730457901954651,
      "learning_rate": 5e-05,
      "loss": 0.0818,
      "step": 590
    },
    {
      "epoch": 0.6920374707259953,
      "grad_norm": 0.8457341194152832,
      "learning_rate": 5e-05,
      "loss": 0.1377,
      "step": 591
    },
    {
      "epoch": 0.6932084309133489,
      "grad_norm": 0.8403105139732361,
      "learning_rate": 5e-05,
      "loss": 0.1284,
      "step": 592
    },
    {
      "epoch": 0.6943793911007026,
      "grad_norm": 0.8607668876647949,
      "learning_rate": 5e-05,
      "loss": 0.1227,
      "step": 593
    },
    {
      "epoch": 0.6955503512880562,
      "grad_norm": 0.9438078999519348,
      "learning_rate": 5e-05,
      "loss": 0.2274,
      "step": 594
    },
    {
      "epoch": 0.6967213114754098,
      "grad_norm": 0.5670666098594666,
      "learning_rate": 5e-05,
      "loss": 0.0824,
      "step": 595
    },
    {
      "epoch": 0.6978922716627635,
      "grad_norm": 1.7796343564987183,
      "learning_rate": 5e-05,
      "loss": 0.3199,
      "step": 596
    },
    {
      "epoch": 0.6990632318501171,
      "grad_norm": 0.9395751953125,
      "learning_rate": 5e-05,
      "loss": 0.2236,
      "step": 597
    },
    {
      "epoch": 0.7002341920374707,
      "grad_norm": 0.5699946284294128,
      "learning_rate": 5e-05,
      "loss": 0.056,
      "step": 598
    },
    {
      "epoch": 0.7014051522248244,
      "grad_norm": 0.7130415439605713,
      "learning_rate": 5e-05,
      "loss": 0.2039,
      "step": 599
    },
    {
      "epoch": 0.702576112412178,
      "grad_norm": 1.2220511436462402,
      "learning_rate": 5e-05,
      "loss": 0.2151,
      "step": 600
    },
    {
      "epoch": 0.7037470725995316,
      "grad_norm": 0.9080818891525269,
      "learning_rate": 5e-05,
      "loss": 0.1601,
      "step": 601
    },
    {
      "epoch": 0.7049180327868853,
      "grad_norm": 0.8835509419441223,
      "learning_rate": 5e-05,
      "loss": 0.2554,
      "step": 602
    },
    {
      "epoch": 0.7060889929742389,
      "grad_norm": 0.823339581489563,
      "learning_rate": 5e-05,
      "loss": 0.101,
      "step": 603
    },
    {
      "epoch": 0.7072599531615925,
      "grad_norm": 0.8372727036476135,
      "learning_rate": 5e-05,
      "loss": 0.1708,
      "step": 604
    },
    {
      "epoch": 0.7084309133489461,
      "grad_norm": 0.789702296257019,
      "learning_rate": 5e-05,
      "loss": 0.0833,
      "step": 605
    },
    {
      "epoch": 0.7096018735362998,
      "grad_norm": 0.6684197783470154,
      "learning_rate": 5e-05,
      "loss": 0.1218,
      "step": 606
    },
    {
      "epoch": 0.7107728337236534,
      "grad_norm": 0.8020806312561035,
      "learning_rate": 5e-05,
      "loss": 0.1364,
      "step": 607
    },
    {
      "epoch": 0.711943793911007,
      "grad_norm": 0.9535471200942993,
      "learning_rate": 5e-05,
      "loss": 0.1953,
      "step": 608
    },
    {
      "epoch": 0.7131147540983607,
      "grad_norm": 1.1105843782424927,
      "learning_rate": 5e-05,
      "loss": 0.0942,
      "step": 609
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.9714599847793579,
      "learning_rate": 5e-05,
      "loss": 0.0929,
      "step": 610
    },
    {
      "epoch": 0.7154566744730679,
      "grad_norm": 1.1624946594238281,
      "learning_rate": 5e-05,
      "loss": 0.199,
      "step": 611
    },
    {
      "epoch": 0.7166276346604216,
      "grad_norm": 0.8500527739524841,
      "learning_rate": 5e-05,
      "loss": 0.2726,
      "step": 612
    },
    {
      "epoch": 0.7177985948477752,
      "grad_norm": 1.0866096019744873,
      "learning_rate": 5e-05,
      "loss": 0.1834,
      "step": 613
    },
    {
      "epoch": 0.7189695550351288,
      "grad_norm": 0.6909416317939758,
      "learning_rate": 5e-05,
      "loss": 0.0618,
      "step": 614
    },
    {
      "epoch": 0.7201405152224825,
      "grad_norm": 1.183646559715271,
      "learning_rate": 5e-05,
      "loss": 0.1755,
      "step": 615
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 0.8960665464401245,
      "learning_rate": 5e-05,
      "loss": 0.2149,
      "step": 616
    },
    {
      "epoch": 0.7224824355971897,
      "grad_norm": 1.4094575643539429,
      "learning_rate": 5e-05,
      "loss": 0.143,
      "step": 617
    },
    {
      "epoch": 0.7236533957845434,
      "grad_norm": 0.7909383773803711,
      "learning_rate": 5e-05,
      "loss": 0.1304,
      "step": 618
    },
    {
      "epoch": 0.724824355971897,
      "grad_norm": 1.0250738859176636,
      "learning_rate": 5e-05,
      "loss": 0.209,
      "step": 619
    },
    {
      "epoch": 0.7259953161592506,
      "grad_norm": 0.8611752986907959,
      "learning_rate": 5e-05,
      "loss": 0.0901,
      "step": 620
    },
    {
      "epoch": 0.7271662763466042,
      "grad_norm": 1.4877463579177856,
      "learning_rate": 5e-05,
      "loss": 0.075,
      "step": 621
    },
    {
      "epoch": 0.7283372365339579,
      "grad_norm": 1.1028637886047363,
      "learning_rate": 5e-05,
      "loss": 0.2112,
      "step": 622
    },
    {
      "epoch": 0.7295081967213115,
      "grad_norm": 0.7734993696212769,
      "learning_rate": 5e-05,
      "loss": 0.1218,
      "step": 623
    },
    {
      "epoch": 0.7306791569086651,
      "grad_norm": 1.8475013971328735,
      "learning_rate": 5e-05,
      "loss": 0.3717,
      "step": 624
    },
    {
      "epoch": 0.7318501170960188,
      "grad_norm": 0.7106494903564453,
      "learning_rate": 5e-05,
      "loss": 0.1619,
      "step": 625
    },
    {
      "epoch": 0.7330210772833724,
      "grad_norm": 1.057868242263794,
      "learning_rate": 5e-05,
      "loss": 0.1876,
      "step": 626
    },
    {
      "epoch": 0.734192037470726,
      "grad_norm": 0.9295995235443115,
      "learning_rate": 5e-05,
      "loss": 0.1424,
      "step": 627
    },
    {
      "epoch": 0.7353629976580797,
      "grad_norm": 0.7630143761634827,
      "learning_rate": 5e-05,
      "loss": 0.1414,
      "step": 628
    },
    {
      "epoch": 0.7365339578454333,
      "grad_norm": 0.7555115818977356,
      "learning_rate": 5e-05,
      "loss": 0.132,
      "step": 629
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 1.071128487586975,
      "learning_rate": 5e-05,
      "loss": 0.1146,
      "step": 630
    },
    {
      "epoch": 0.7388758782201406,
      "grad_norm": 0.6235301494598389,
      "learning_rate": 5e-05,
      "loss": 0.1231,
      "step": 631
    },
    {
      "epoch": 0.7400468384074942,
      "grad_norm": 0.7311874628067017,
      "learning_rate": 5e-05,
      "loss": 0.1389,
      "step": 632
    },
    {
      "epoch": 0.7412177985948478,
      "grad_norm": 0.7697297930717468,
      "learning_rate": 5e-05,
      "loss": 0.153,
      "step": 633
    },
    {
      "epoch": 0.7423887587822015,
      "grad_norm": 1.1201761960983276,
      "learning_rate": 5e-05,
      "loss": 0.2274,
      "step": 634
    },
    {
      "epoch": 0.7435597189695551,
      "grad_norm": 1.2606630325317383,
      "learning_rate": 5e-05,
      "loss": 0.2351,
      "step": 635
    },
    {
      "epoch": 0.7447306791569087,
      "grad_norm": 1.2847955226898193,
      "learning_rate": 5e-05,
      "loss": 0.1909,
      "step": 636
    },
    {
      "epoch": 0.7459016393442623,
      "grad_norm": 0.8377935290336609,
      "learning_rate": 5e-05,
      "loss": 0.2231,
      "step": 637
    },
    {
      "epoch": 0.747072599531616,
      "grad_norm": 1.2308648824691772,
      "learning_rate": 5e-05,
      "loss": 0.2608,
      "step": 638
    },
    {
      "epoch": 0.7482435597189696,
      "grad_norm": 0.9164251685142517,
      "learning_rate": 5e-05,
      "loss": 0.1842,
      "step": 639
    },
    {
      "epoch": 0.7494145199063232,
      "grad_norm": 1.3210500478744507,
      "learning_rate": 5e-05,
      "loss": 0.1936,
      "step": 640
    },
    {
      "epoch": 0.7505854800936768,
      "grad_norm": 1.0480705499649048,
      "learning_rate": 5e-05,
      "loss": 0.1082,
      "step": 641
    },
    {
      "epoch": 0.7517564402810304,
      "grad_norm": 0.9060940146446228,
      "learning_rate": 5e-05,
      "loss": 0.1549,
      "step": 642
    },
    {
      "epoch": 0.752927400468384,
      "grad_norm": 0.6193191409111023,
      "learning_rate": 5e-05,
      "loss": 0.0878,
      "step": 643
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 1.097322702407837,
      "learning_rate": 5e-05,
      "loss": 0.1819,
      "step": 644
    },
    {
      "epoch": 0.7552693208430913,
      "grad_norm": 0.55536949634552,
      "learning_rate": 5e-05,
      "loss": 0.1005,
      "step": 645
    },
    {
      "epoch": 0.7564402810304449,
      "grad_norm": 1.1497548818588257,
      "learning_rate": 5e-05,
      "loss": 0.0791,
      "step": 646
    },
    {
      "epoch": 0.7576112412177985,
      "grad_norm": 1.110054850578308,
      "learning_rate": 5e-05,
      "loss": 0.2008,
      "step": 647
    },
    {
      "epoch": 0.7587822014051522,
      "grad_norm": 1.0244812965393066,
      "learning_rate": 5e-05,
      "loss": 0.1809,
      "step": 648
    },
    {
      "epoch": 0.7599531615925058,
      "grad_norm": 1.1705985069274902,
      "learning_rate": 5e-05,
      "loss": 0.1621,
      "step": 649
    },
    {
      "epoch": 0.7611241217798594,
      "grad_norm": 0.6947608590126038,
      "learning_rate": 5e-05,
      "loss": 0.1505,
      "step": 650
    },
    {
      "epoch": 0.7622950819672131,
      "grad_norm": 0.8473238945007324,
      "learning_rate": 5e-05,
      "loss": 0.1634,
      "step": 651
    },
    {
      "epoch": 0.7634660421545667,
      "grad_norm": 0.8327474594116211,
      "learning_rate": 5e-05,
      "loss": 0.0703,
      "step": 652
    },
    {
      "epoch": 0.7646370023419203,
      "grad_norm": 0.6632007360458374,
      "learning_rate": 5e-05,
      "loss": 0.0591,
      "step": 653
    },
    {
      "epoch": 0.765807962529274,
      "grad_norm": 0.9923794865608215,
      "learning_rate": 5e-05,
      "loss": 0.1596,
      "step": 654
    },
    {
      "epoch": 0.7669789227166276,
      "grad_norm": 1.1120245456695557,
      "learning_rate": 5e-05,
      "loss": 0.2125,
      "step": 655
    },
    {
      "epoch": 0.7681498829039812,
      "grad_norm": 0.8811097145080566,
      "learning_rate": 5e-05,
      "loss": 0.0883,
      "step": 656
    },
    {
      "epoch": 0.7693208430913349,
      "grad_norm": 0.8868449926376343,
      "learning_rate": 5e-05,
      "loss": 0.1328,
      "step": 657
    },
    {
      "epoch": 0.7704918032786885,
      "grad_norm": 2.5041446685791016,
      "learning_rate": 5e-05,
      "loss": 0.2334,
      "step": 658
    },
    {
      "epoch": 0.7716627634660421,
      "grad_norm": 0.6937329769134521,
      "learning_rate": 5e-05,
      "loss": 0.1253,
      "step": 659
    },
    {
      "epoch": 0.7728337236533958,
      "grad_norm": 0.4782766103744507,
      "learning_rate": 5e-05,
      "loss": 0.0914,
      "step": 660
    },
    {
      "epoch": 0.7740046838407494,
      "grad_norm": 0.9668214321136475,
      "learning_rate": 5e-05,
      "loss": 0.1662,
      "step": 661
    },
    {
      "epoch": 0.775175644028103,
      "grad_norm": 0.8319258689880371,
      "learning_rate": 5e-05,
      "loss": 0.136,
      "step": 662
    },
    {
      "epoch": 0.7763466042154566,
      "grad_norm": 1.267479658126831,
      "learning_rate": 5e-05,
      "loss": 0.2127,
      "step": 663
    },
    {
      "epoch": 0.7775175644028103,
      "grad_norm": 0.8336775898933411,
      "learning_rate": 5e-05,
      "loss": 0.0759,
      "step": 664
    },
    {
      "epoch": 0.7786885245901639,
      "grad_norm": 0.8548200130462646,
      "learning_rate": 5e-05,
      "loss": 0.1706,
      "step": 665
    },
    {
      "epoch": 0.7798594847775175,
      "grad_norm": 1.132728099822998,
      "learning_rate": 5e-05,
      "loss": 0.175,
      "step": 666
    },
    {
      "epoch": 0.7810304449648712,
      "grad_norm": 0.6861107349395752,
      "learning_rate": 5e-05,
      "loss": 0.0913,
      "step": 667
    },
    {
      "epoch": 0.7822014051522248,
      "grad_norm": 1.7070599794387817,
      "learning_rate": 5e-05,
      "loss": 0.1908,
      "step": 668
    },
    {
      "epoch": 0.7833723653395784,
      "grad_norm": 1.123728632926941,
      "learning_rate": 5e-05,
      "loss": 0.3041,
      "step": 669
    },
    {
      "epoch": 0.7845433255269321,
      "grad_norm": 0.7418849468231201,
      "learning_rate": 5e-05,
      "loss": 0.0842,
      "step": 670
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 1.0950044393539429,
      "learning_rate": 5e-05,
      "loss": 0.1304,
      "step": 671
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 0.992424726486206,
      "learning_rate": 5e-05,
      "loss": 0.1553,
      "step": 672
    },
    {
      "epoch": 0.788056206088993,
      "grad_norm": 0.6898892521858215,
      "learning_rate": 5e-05,
      "loss": 0.0923,
      "step": 673
    },
    {
      "epoch": 0.7892271662763466,
      "grad_norm": 0.8670896887779236,
      "learning_rate": 5e-05,
      "loss": 0.164,
      "step": 674
    },
    {
      "epoch": 0.7903981264637002,
      "grad_norm": 0.8454248309135437,
      "learning_rate": 5e-05,
      "loss": 0.1365,
      "step": 675
    },
    {
      "epoch": 0.7915690866510539,
      "grad_norm": 1.158057451248169,
      "learning_rate": 5e-05,
      "loss": 0.0716,
      "step": 676
    },
    {
      "epoch": 0.7927400468384075,
      "grad_norm": 0.7894881963729858,
      "learning_rate": 5e-05,
      "loss": 0.1826,
      "step": 677
    },
    {
      "epoch": 0.7939110070257611,
      "grad_norm": 1.3844006061553955,
      "learning_rate": 5e-05,
      "loss": 0.114,
      "step": 678
    },
    {
      "epoch": 0.7950819672131147,
      "grad_norm": 1.4769773483276367,
      "learning_rate": 5e-05,
      "loss": 0.258,
      "step": 679
    },
    {
      "epoch": 0.7962529274004684,
      "grad_norm": 0.8764195442199707,
      "learning_rate": 5e-05,
      "loss": 0.1229,
      "step": 680
    },
    {
      "epoch": 0.797423887587822,
      "grad_norm": 0.9511922597885132,
      "learning_rate": 5e-05,
      "loss": 0.1789,
      "step": 681
    },
    {
      "epoch": 0.7985948477751756,
      "grad_norm": 0.8200201988220215,
      "learning_rate": 5e-05,
      "loss": 0.0962,
      "step": 682
    },
    {
      "epoch": 0.7997658079625293,
      "grad_norm": 0.739568293094635,
      "learning_rate": 5e-05,
      "loss": 0.0986,
      "step": 683
    },
    {
      "epoch": 0.8009367681498829,
      "grad_norm": 0.8460868000984192,
      "learning_rate": 5e-05,
      "loss": 0.1752,
      "step": 684
    },
    {
      "epoch": 0.8021077283372365,
      "grad_norm": 1.185300588607788,
      "learning_rate": 5e-05,
      "loss": 0.1526,
      "step": 685
    },
    {
      "epoch": 0.8032786885245902,
      "grad_norm": 0.8924285769462585,
      "learning_rate": 5e-05,
      "loss": 0.1735,
      "step": 686
    },
    {
      "epoch": 0.8044496487119438,
      "grad_norm": 1.0558629035949707,
      "learning_rate": 5e-05,
      "loss": 0.2334,
      "step": 687
    },
    {
      "epoch": 0.8056206088992974,
      "grad_norm": 0.6008822917938232,
      "learning_rate": 5e-05,
      "loss": 0.1004,
      "step": 688
    },
    {
      "epoch": 0.8067915690866511,
      "grad_norm": 1.059585452079773,
      "learning_rate": 5e-05,
      "loss": 0.1272,
      "step": 689
    },
    {
      "epoch": 0.8079625292740047,
      "grad_norm": 0.9933829307556152,
      "learning_rate": 5e-05,
      "loss": 0.1898,
      "step": 690
    },
    {
      "epoch": 0.8091334894613583,
      "grad_norm": 0.877160906791687,
      "learning_rate": 5e-05,
      "loss": 0.1782,
      "step": 691
    },
    {
      "epoch": 0.810304449648712,
      "grad_norm": 0.9498867392539978,
      "learning_rate": 5e-05,
      "loss": 0.1273,
      "step": 692
    },
    {
      "epoch": 0.8114754098360656,
      "grad_norm": 0.7113898992538452,
      "learning_rate": 5e-05,
      "loss": 0.1252,
      "step": 693
    },
    {
      "epoch": 0.8126463700234192,
      "grad_norm": 1.0106168985366821,
      "learning_rate": 5e-05,
      "loss": 0.1429,
      "step": 694
    },
    {
      "epoch": 0.8138173302107728,
      "grad_norm": 1.1633206605911255,
      "learning_rate": 5e-05,
      "loss": 0.2179,
      "step": 695
    },
    {
      "epoch": 0.8149882903981265,
      "grad_norm": 0.864510715007782,
      "learning_rate": 5e-05,
      "loss": 0.0837,
      "step": 696
    },
    {
      "epoch": 0.8161592505854801,
      "grad_norm": 1.0169665813446045,
      "learning_rate": 5e-05,
      "loss": 0.175,
      "step": 697
    },
    {
      "epoch": 0.8173302107728337,
      "grad_norm": 1.83140230178833,
      "learning_rate": 5e-05,
      "loss": 0.1752,
      "step": 698
    },
    {
      "epoch": 0.8185011709601874,
      "grad_norm": 1.2029595375061035,
      "learning_rate": 5e-05,
      "loss": 0.1606,
      "step": 699
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 1.1556851863861084,
      "learning_rate": 5e-05,
      "loss": 0.1597,
      "step": 700
    },
    {
      "epoch": 0.8208430913348946,
      "grad_norm": 1.43149733543396,
      "learning_rate": 5e-05,
      "loss": 0.2269,
      "step": 701
    },
    {
      "epoch": 0.8220140515222483,
      "grad_norm": 1.0586687326431274,
      "learning_rate": 5e-05,
      "loss": 0.2148,
      "step": 702
    },
    {
      "epoch": 0.8231850117096019,
      "grad_norm": 0.8361656665802002,
      "learning_rate": 5e-05,
      "loss": 0.2604,
      "step": 703
    },
    {
      "epoch": 0.8243559718969555,
      "grad_norm": 0.6909567713737488,
      "learning_rate": 5e-05,
      "loss": 0.126,
      "step": 704
    },
    {
      "epoch": 0.8255269320843092,
      "grad_norm": 1.330018401145935,
      "learning_rate": 5e-05,
      "loss": 0.1775,
      "step": 705
    },
    {
      "epoch": 0.8266978922716628,
      "grad_norm": 0.8583828210830688,
      "learning_rate": 5e-05,
      "loss": 0.1664,
      "step": 706
    },
    {
      "epoch": 0.8278688524590164,
      "grad_norm": 0.8828515410423279,
      "learning_rate": 5e-05,
      "loss": 0.089,
      "step": 707
    },
    {
      "epoch": 0.8290398126463701,
      "grad_norm": 0.7534164190292358,
      "learning_rate": 5e-05,
      "loss": 0.0949,
      "step": 708
    },
    {
      "epoch": 0.8302107728337237,
      "grad_norm": 0.733279824256897,
      "learning_rate": 5e-05,
      "loss": 0.1256,
      "step": 709
    },
    {
      "epoch": 0.8313817330210773,
      "grad_norm": 2.065134048461914,
      "learning_rate": 5e-05,
      "loss": 0.1813,
      "step": 710
    },
    {
      "epoch": 0.832552693208431,
      "grad_norm": 0.7384780645370483,
      "learning_rate": 5e-05,
      "loss": 0.1287,
      "step": 711
    },
    {
      "epoch": 0.8337236533957846,
      "grad_norm": 0.7715677618980408,
      "learning_rate": 5e-05,
      "loss": 0.1476,
      "step": 712
    },
    {
      "epoch": 0.8348946135831382,
      "grad_norm": 0.834328830242157,
      "learning_rate": 5e-05,
      "loss": 0.1214,
      "step": 713
    },
    {
      "epoch": 0.8360655737704918,
      "grad_norm": 0.8504607081413269,
      "learning_rate": 5e-05,
      "loss": 0.104,
      "step": 714
    },
    {
      "epoch": 0.8372365339578455,
      "grad_norm": 1.394782543182373,
      "learning_rate": 5e-05,
      "loss": 0.168,
      "step": 715
    },
    {
      "epoch": 0.8384074941451991,
      "grad_norm": 1.0189857482910156,
      "learning_rate": 5e-05,
      "loss": 0.1214,
      "step": 716
    },
    {
      "epoch": 0.8395784543325527,
      "grad_norm": 1.4971064329147339,
      "learning_rate": 5e-05,
      "loss": 0.1863,
      "step": 717
    },
    {
      "epoch": 0.8407494145199064,
      "grad_norm": 0.7854481339454651,
      "learning_rate": 5e-05,
      "loss": 0.1517,
      "step": 718
    },
    {
      "epoch": 0.84192037470726,
      "grad_norm": 0.714378833770752,
      "learning_rate": 5e-05,
      "loss": 0.0898,
      "step": 719
    },
    {
      "epoch": 0.8430913348946136,
      "grad_norm": 1.1327571868896484,
      "learning_rate": 5e-05,
      "loss": 0.1787,
      "step": 720
    },
    {
      "epoch": 0.8442622950819673,
      "grad_norm": 1.0361992120742798,
      "learning_rate": 5e-05,
      "loss": 0.0605,
      "step": 721
    },
    {
      "epoch": 0.8454332552693209,
      "grad_norm": 0.9846572875976562,
      "learning_rate": 5e-05,
      "loss": 0.1422,
      "step": 722
    },
    {
      "epoch": 0.8466042154566745,
      "grad_norm": 0.7938024401664734,
      "learning_rate": 5e-05,
      "loss": 0.0874,
      "step": 723
    },
    {
      "epoch": 0.8477751756440282,
      "grad_norm": 1.024674654006958,
      "learning_rate": 5e-05,
      "loss": 0.2353,
      "step": 724
    },
    {
      "epoch": 0.8489461358313818,
      "grad_norm": 1.0728658437728882,
      "learning_rate": 5e-05,
      "loss": 0.118,
      "step": 725
    },
    {
      "epoch": 0.8501170960187353,
      "grad_norm": 1.2920067310333252,
      "learning_rate": 5e-05,
      "loss": 0.1312,
      "step": 726
    },
    {
      "epoch": 0.8512880562060889,
      "grad_norm": 1.2127825021743774,
      "learning_rate": 5e-05,
      "loss": 0.2846,
      "step": 727
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 0.8768479228019714,
      "learning_rate": 5e-05,
      "loss": 0.1419,
      "step": 728
    },
    {
      "epoch": 0.8536299765807962,
      "grad_norm": 0.7655407786369324,
      "learning_rate": 5e-05,
      "loss": 0.2059,
      "step": 729
    },
    {
      "epoch": 0.8548009367681498,
      "grad_norm": 1.260717511177063,
      "learning_rate": 5e-05,
      "loss": 0.1401,
      "step": 730
    },
    {
      "epoch": 0.8559718969555035,
      "grad_norm": 0.8183658719062805,
      "learning_rate": 5e-05,
      "loss": 0.1098,
      "step": 731
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 1.2751686573028564,
      "learning_rate": 5e-05,
      "loss": 0.1567,
      "step": 732
    },
    {
      "epoch": 0.8583138173302107,
      "grad_norm": 0.7815770506858826,
      "learning_rate": 5e-05,
      "loss": 0.2152,
      "step": 733
    },
    {
      "epoch": 0.8594847775175644,
      "grad_norm": 0.749941885471344,
      "learning_rate": 5e-05,
      "loss": 0.0827,
      "step": 734
    },
    {
      "epoch": 0.860655737704918,
      "grad_norm": 1.0691241025924683,
      "learning_rate": 5e-05,
      "loss": 0.2172,
      "step": 735
    },
    {
      "epoch": 0.8618266978922716,
      "grad_norm": 0.750016450881958,
      "learning_rate": 5e-05,
      "loss": 0.0628,
      "step": 736
    },
    {
      "epoch": 0.8629976580796253,
      "grad_norm": 0.6997646689414978,
      "learning_rate": 5e-05,
      "loss": 0.1004,
      "step": 737
    },
    {
      "epoch": 0.8641686182669789,
      "grad_norm": 0.5628420114517212,
      "learning_rate": 5e-05,
      "loss": 0.127,
      "step": 738
    },
    {
      "epoch": 0.8653395784543325,
      "grad_norm": 0.99720299243927,
      "learning_rate": 5e-05,
      "loss": 0.1499,
      "step": 739
    },
    {
      "epoch": 0.8665105386416861,
      "grad_norm": 0.7716275453567505,
      "learning_rate": 5e-05,
      "loss": 0.1155,
      "step": 740
    },
    {
      "epoch": 0.8676814988290398,
      "grad_norm": 0.9323875308036804,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 741
    },
    {
      "epoch": 0.8688524590163934,
      "grad_norm": 0.652686595916748,
      "learning_rate": 5e-05,
      "loss": 0.1083,
      "step": 742
    },
    {
      "epoch": 0.870023419203747,
      "grad_norm": 1.3472956418991089,
      "learning_rate": 5e-05,
      "loss": 0.212,
      "step": 743
    },
    {
      "epoch": 0.8711943793911007,
      "grad_norm": 0.6327999234199524,
      "learning_rate": 5e-05,
      "loss": 0.1309,
      "step": 744
    },
    {
      "epoch": 0.8723653395784543,
      "grad_norm": 0.7582704424858093,
      "learning_rate": 5e-05,
      "loss": 0.1185,
      "step": 745
    },
    {
      "epoch": 0.8735362997658079,
      "grad_norm": 0.8602433204650879,
      "learning_rate": 5e-05,
      "loss": 0.1526,
      "step": 746
    },
    {
      "epoch": 0.8747072599531616,
      "grad_norm": 0.6715821027755737,
      "learning_rate": 5e-05,
      "loss": 0.0597,
      "step": 747
    },
    {
      "epoch": 0.8758782201405152,
      "grad_norm": 0.7651371955871582,
      "learning_rate": 5e-05,
      "loss": 0.0977,
      "step": 748
    },
    {
      "epoch": 0.8770491803278688,
      "grad_norm": 1.202891230583191,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 749
    },
    {
      "epoch": 0.8782201405152225,
      "grad_norm": 0.8753429055213928,
      "learning_rate": 5e-05,
      "loss": 0.1723,
      "step": 750
    },
    {
      "epoch": 0.8793911007025761,
      "grad_norm": 1.0303019285202026,
      "learning_rate": 5e-05,
      "loss": 0.2401,
      "step": 751
    },
    {
      "epoch": 0.8805620608899297,
      "grad_norm": 1.0220215320587158,
      "learning_rate": 5e-05,
      "loss": 0.0933,
      "step": 752
    },
    {
      "epoch": 0.8817330210772834,
      "grad_norm": 0.9884816408157349,
      "learning_rate": 5e-05,
      "loss": 0.0794,
      "step": 753
    },
    {
      "epoch": 0.882903981264637,
      "grad_norm": 0.850226104259491,
      "learning_rate": 5e-05,
      "loss": 0.0948,
      "step": 754
    },
    {
      "epoch": 0.8840749414519906,
      "grad_norm": 0.841837465763092,
      "learning_rate": 5e-05,
      "loss": 0.0877,
      "step": 755
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 0.8215498924255371,
      "learning_rate": 5e-05,
      "loss": 0.1554,
      "step": 756
    },
    {
      "epoch": 0.8864168618266979,
      "grad_norm": 1.391455888748169,
      "learning_rate": 5e-05,
      "loss": 0.1369,
      "step": 757
    },
    {
      "epoch": 0.8875878220140515,
      "grad_norm": 1.2856954336166382,
      "learning_rate": 5e-05,
      "loss": 0.13,
      "step": 758
    },
    {
      "epoch": 0.8887587822014051,
      "grad_norm": 0.883582353591919,
      "learning_rate": 5e-05,
      "loss": 0.1114,
      "step": 759
    },
    {
      "epoch": 0.8899297423887588,
      "grad_norm": 1.1366149187088013,
      "learning_rate": 5e-05,
      "loss": 0.1616,
      "step": 760
    },
    {
      "epoch": 0.8911007025761124,
      "grad_norm": 0.6097160577774048,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 761
    },
    {
      "epoch": 0.892271662763466,
      "grad_norm": 1.3010061979293823,
      "learning_rate": 5e-05,
      "loss": 0.1473,
      "step": 762
    },
    {
      "epoch": 0.8934426229508197,
      "grad_norm": 1.3788831233978271,
      "learning_rate": 5e-05,
      "loss": 0.3033,
      "step": 763
    },
    {
      "epoch": 0.8946135831381733,
      "grad_norm": 1.1183717250823975,
      "learning_rate": 5e-05,
      "loss": 0.1459,
      "step": 764
    },
    {
      "epoch": 0.8957845433255269,
      "grad_norm": 0.836516797542572,
      "learning_rate": 5e-05,
      "loss": 0.1337,
      "step": 765
    },
    {
      "epoch": 0.8969555035128806,
      "grad_norm": 0.9319727420806885,
      "learning_rate": 5e-05,
      "loss": 0.1581,
      "step": 766
    },
    {
      "epoch": 0.8981264637002342,
      "grad_norm": 1.1597981452941895,
      "learning_rate": 5e-05,
      "loss": 0.1326,
      "step": 767
    },
    {
      "epoch": 0.8992974238875878,
      "grad_norm": 1.4926079511642456,
      "learning_rate": 5e-05,
      "loss": 0.2097,
      "step": 768
    },
    {
      "epoch": 0.9004683840749415,
      "grad_norm": 1.1844003200531006,
      "learning_rate": 5e-05,
      "loss": 0.1136,
      "step": 769
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 0.8442467451095581,
      "learning_rate": 5e-05,
      "loss": 0.1476,
      "step": 770
    },
    {
      "epoch": 0.9028103044496487,
      "grad_norm": 0.8476410508155823,
      "learning_rate": 5e-05,
      "loss": 0.0836,
      "step": 771
    },
    {
      "epoch": 0.9039812646370023,
      "grad_norm": 1.0533775091171265,
      "learning_rate": 5e-05,
      "loss": 0.0892,
      "step": 772
    },
    {
      "epoch": 0.905152224824356,
      "grad_norm": 1.307370662689209,
      "learning_rate": 5e-05,
      "loss": 0.427,
      "step": 773
    },
    {
      "epoch": 0.9063231850117096,
      "grad_norm": 0.8372268080711365,
      "learning_rate": 5e-05,
      "loss": 0.0779,
      "step": 774
    },
    {
      "epoch": 0.9074941451990632,
      "grad_norm": 1.3358979225158691,
      "learning_rate": 5e-05,
      "loss": 0.1937,
      "step": 775
    },
    {
      "epoch": 0.9086651053864169,
      "grad_norm": 0.8680928349494934,
      "learning_rate": 5e-05,
      "loss": 0.2091,
      "step": 776
    },
    {
      "epoch": 0.9098360655737705,
      "grad_norm": 0.7254090309143066,
      "learning_rate": 5e-05,
      "loss": 0.0784,
      "step": 777
    },
    {
      "epoch": 0.9110070257611241,
      "grad_norm": 0.9819979667663574,
      "learning_rate": 5e-05,
      "loss": 0.1145,
      "step": 778
    },
    {
      "epoch": 0.9121779859484778,
      "grad_norm": 0.8914922475814819,
      "learning_rate": 5e-05,
      "loss": 0.1551,
      "step": 779
    },
    {
      "epoch": 0.9133489461358314,
      "grad_norm": 0.9861723184585571,
      "learning_rate": 5e-05,
      "loss": 0.1256,
      "step": 780
    },
    {
      "epoch": 0.914519906323185,
      "grad_norm": 1.0290085077285767,
      "learning_rate": 5e-05,
      "loss": 0.1161,
      "step": 781
    },
    {
      "epoch": 0.9156908665105387,
      "grad_norm": 1.2470219135284424,
      "learning_rate": 5e-05,
      "loss": 0.2322,
      "step": 782
    },
    {
      "epoch": 0.9168618266978923,
      "grad_norm": 1.2556407451629639,
      "learning_rate": 5e-05,
      "loss": 0.2062,
      "step": 783
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 1.0163496732711792,
      "learning_rate": 5e-05,
      "loss": 0.1244,
      "step": 784
    },
    {
      "epoch": 0.9192037470725996,
      "grad_norm": 0.8072014451026917,
      "learning_rate": 5e-05,
      "loss": 0.1097,
      "step": 785
    },
    {
      "epoch": 0.9203747072599532,
      "grad_norm": 0.7435322999954224,
      "learning_rate": 5e-05,
      "loss": 0.1157,
      "step": 786
    },
    {
      "epoch": 0.9215456674473068,
      "grad_norm": 0.9362718462944031,
      "learning_rate": 5e-05,
      "loss": 0.1774,
      "step": 787
    },
    {
      "epoch": 0.9227166276346604,
      "grad_norm": 0.8183640241622925,
      "learning_rate": 5e-05,
      "loss": 0.1154,
      "step": 788
    },
    {
      "epoch": 0.9238875878220141,
      "grad_norm": 0.96253502368927,
      "learning_rate": 5e-05,
      "loss": 0.1193,
      "step": 789
    },
    {
      "epoch": 0.9250585480093677,
      "grad_norm": 1.3124418258666992,
      "learning_rate": 5e-05,
      "loss": 0.2075,
      "step": 790
    },
    {
      "epoch": 0.9262295081967213,
      "grad_norm": 0.6756847500801086,
      "learning_rate": 5e-05,
      "loss": 0.2228,
      "step": 791
    },
    {
      "epoch": 0.927400468384075,
      "grad_norm": 0.884515106678009,
      "learning_rate": 5e-05,
      "loss": 0.1303,
      "step": 792
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 1.431888461112976,
      "learning_rate": 5e-05,
      "loss": 0.1885,
      "step": 793
    },
    {
      "epoch": 0.9297423887587822,
      "grad_norm": 1.445454478263855,
      "learning_rate": 5e-05,
      "loss": 0.1888,
      "step": 794
    },
    {
      "epoch": 0.9309133489461359,
      "grad_norm": 0.5008208751678467,
      "learning_rate": 5e-05,
      "loss": 0.0757,
      "step": 795
    },
    {
      "epoch": 0.9320843091334895,
      "grad_norm": 0.7920925617218018,
      "learning_rate": 5e-05,
      "loss": 0.0868,
      "step": 796
    },
    {
      "epoch": 0.9332552693208431,
      "grad_norm": 0.701777994632721,
      "learning_rate": 5e-05,
      "loss": 0.1015,
      "step": 797
    },
    {
      "epoch": 0.9344262295081968,
      "grad_norm": 0.4087206721305847,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 798
    },
    {
      "epoch": 0.9355971896955504,
      "grad_norm": 0.7309758067131042,
      "learning_rate": 5e-05,
      "loss": 0.0855,
      "step": 799
    },
    {
      "epoch": 0.936768149882904,
      "grad_norm": 0.7940506935119629,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 800
    },
    {
      "epoch": 0.9379391100702577,
      "grad_norm": 1.8417863845825195,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 801
    },
    {
      "epoch": 0.9391100702576113,
      "grad_norm": 1.3072642087936401,
      "learning_rate": 5e-05,
      "loss": 0.2109,
      "step": 802
    },
    {
      "epoch": 0.9402810304449649,
      "grad_norm": 0.8991749286651611,
      "learning_rate": 5e-05,
      "loss": 0.1903,
      "step": 803
    },
    {
      "epoch": 0.9414519906323185,
      "grad_norm": 0.5847378969192505,
      "learning_rate": 5e-05,
      "loss": 0.1003,
      "step": 804
    },
    {
      "epoch": 0.9426229508196722,
      "grad_norm": 1.1724659204483032,
      "learning_rate": 5e-05,
      "loss": 0.1644,
      "step": 805
    },
    {
      "epoch": 0.9437939110070258,
      "grad_norm": 2.1620476245880127,
      "learning_rate": 5e-05,
      "loss": 0.3068,
      "step": 806
    },
    {
      "epoch": 0.9449648711943794,
      "grad_norm": 1.0735819339752197,
      "learning_rate": 5e-05,
      "loss": 0.1204,
      "step": 807
    },
    {
      "epoch": 0.9461358313817331,
      "grad_norm": 1.0180634260177612,
      "learning_rate": 5e-05,
      "loss": 0.1331,
      "step": 808
    },
    {
      "epoch": 0.9473067915690867,
      "grad_norm": 0.9096403121948242,
      "learning_rate": 5e-05,
      "loss": 0.125,
      "step": 809
    },
    {
      "epoch": 0.9484777517564403,
      "grad_norm": 0.6110497117042542,
      "learning_rate": 5e-05,
      "loss": 0.0882,
      "step": 810
    },
    {
      "epoch": 0.949648711943794,
      "grad_norm": 1.114276647567749,
      "learning_rate": 5e-05,
      "loss": 0.1896,
      "step": 811
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 0.7538267374038696,
      "learning_rate": 5e-05,
      "loss": 0.0765,
      "step": 812
    },
    {
      "epoch": 0.9519906323185011,
      "grad_norm": 0.8045324683189392,
      "learning_rate": 5e-05,
      "loss": 0.111,
      "step": 813
    },
    {
      "epoch": 0.9531615925058547,
      "grad_norm": 1.0887980461120605,
      "learning_rate": 5e-05,
      "loss": 0.1717,
      "step": 814
    },
    {
      "epoch": 0.9543325526932084,
      "grad_norm": 0.8649492263793945,
      "learning_rate": 5e-05,
      "loss": 0.0988,
      "step": 815
    },
    {
      "epoch": 0.955503512880562,
      "grad_norm": 0.8887995481491089,
      "learning_rate": 5e-05,
      "loss": 0.1242,
      "step": 816
    },
    {
      "epoch": 0.9566744730679156,
      "grad_norm": 1.1014046669006348,
      "learning_rate": 5e-05,
      "loss": 0.0727,
      "step": 817
    },
    {
      "epoch": 0.9578454332552693,
      "grad_norm": 0.9987875819206238,
      "learning_rate": 5e-05,
      "loss": 0.1233,
      "step": 818
    },
    {
      "epoch": 0.9590163934426229,
      "grad_norm": 1.313987135887146,
      "learning_rate": 5e-05,
      "loss": 0.1856,
      "step": 819
    },
    {
      "epoch": 0.9601873536299765,
      "grad_norm": 0.9135622978210449,
      "learning_rate": 5e-05,
      "loss": 0.1312,
      "step": 820
    },
    {
      "epoch": 0.9613583138173302,
      "grad_norm": 0.9235421419143677,
      "learning_rate": 5e-05,
      "loss": 0.1002,
      "step": 821
    },
    {
      "epoch": 0.9625292740046838,
      "grad_norm": 0.7484820485115051,
      "learning_rate": 5e-05,
      "loss": 0.1376,
      "step": 822
    },
    {
      "epoch": 0.9637002341920374,
      "grad_norm": 0.8948565721511841,
      "learning_rate": 5e-05,
      "loss": 0.1509,
      "step": 823
    },
    {
      "epoch": 0.9648711943793911,
      "grad_norm": 1.0379359722137451,
      "learning_rate": 5e-05,
      "loss": 0.0855,
      "step": 824
    },
    {
      "epoch": 0.9660421545667447,
      "grad_norm": 1.0248948335647583,
      "learning_rate": 5e-05,
      "loss": 0.1566,
      "step": 825
    },
    {
      "epoch": 0.9672131147540983,
      "grad_norm": 0.841266393661499,
      "learning_rate": 5e-05,
      "loss": 0.16,
      "step": 826
    },
    {
      "epoch": 0.968384074941452,
      "grad_norm": 0.8653050661087036,
      "learning_rate": 5e-05,
      "loss": 0.1496,
      "step": 827
    },
    {
      "epoch": 0.9695550351288056,
      "grad_norm": 1.5336843729019165,
      "learning_rate": 5e-05,
      "loss": 0.2861,
      "step": 828
    },
    {
      "epoch": 0.9707259953161592,
      "grad_norm": 0.6687791347503662,
      "learning_rate": 5e-05,
      "loss": 0.1564,
      "step": 829
    },
    {
      "epoch": 0.9718969555035128,
      "grad_norm": 1.0794159173965454,
      "learning_rate": 5e-05,
      "loss": 0.1282,
      "step": 830
    },
    {
      "epoch": 0.9730679156908665,
      "grad_norm": 0.9060345888137817,
      "learning_rate": 5e-05,
      "loss": 0.1494,
      "step": 831
    },
    {
      "epoch": 0.9742388758782201,
      "grad_norm": 1.6687591075897217,
      "learning_rate": 5e-05,
      "loss": 0.136,
      "step": 832
    },
    {
      "epoch": 0.9754098360655737,
      "grad_norm": 1.0682109594345093,
      "learning_rate": 5e-05,
      "loss": 0.1834,
      "step": 833
    },
    {
      "epoch": 0.9765807962529274,
      "grad_norm": 0.9756174087524414,
      "learning_rate": 5e-05,
      "loss": 0.1692,
      "step": 834
    },
    {
      "epoch": 0.977751756440281,
      "grad_norm": 0.871522843837738,
      "learning_rate": 5e-05,
      "loss": 0.1656,
      "step": 835
    },
    {
      "epoch": 0.9789227166276346,
      "grad_norm": 0.8018773198127747,
      "learning_rate": 5e-05,
      "loss": 0.1174,
      "step": 836
    },
    {
      "epoch": 0.9800936768149883,
      "grad_norm": 0.6826831698417664,
      "learning_rate": 5e-05,
      "loss": 0.0543,
      "step": 837
    },
    {
      "epoch": 0.9812646370023419,
      "grad_norm": 1.0488314628601074,
      "learning_rate": 5e-05,
      "loss": 0.1132,
      "step": 838
    },
    {
      "epoch": 0.9824355971896955,
      "grad_norm": 0.797913134098053,
      "learning_rate": 5e-05,
      "loss": 0.1581,
      "step": 839
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 0.8075153231620789,
      "learning_rate": 5e-05,
      "loss": 0.1414,
      "step": 840
    },
    {
      "epoch": 0.9847775175644028,
      "grad_norm": 0.8827168345451355,
      "learning_rate": 5e-05,
      "loss": 0.1352,
      "step": 841
    },
    {
      "epoch": 0.9859484777517564,
      "grad_norm": 1.4172452688217163,
      "learning_rate": 5e-05,
      "loss": 0.2477,
      "step": 842
    },
    {
      "epoch": 0.9871194379391101,
      "grad_norm": 0.6963403820991516,
      "learning_rate": 5e-05,
      "loss": 0.0792,
      "step": 843
    },
    {
      "epoch": 0.9882903981264637,
      "grad_norm": 0.6279473304748535,
      "learning_rate": 5e-05,
      "loss": 0.1079,
      "step": 844
    },
    {
      "epoch": 0.9894613583138173,
      "grad_norm": 1.23873770236969,
      "learning_rate": 5e-05,
      "loss": 0.16,
      "step": 845
    },
    {
      "epoch": 0.990632318501171,
      "grad_norm": 0.7282549142837524,
      "learning_rate": 5e-05,
      "loss": 0.0801,
      "step": 846
    },
    {
      "epoch": 0.9918032786885246,
      "grad_norm": 1.223181962966919,
      "learning_rate": 5e-05,
      "loss": 0.1762,
      "step": 847
    },
    {
      "epoch": 0.9929742388758782,
      "grad_norm": 0.6466009616851807,
      "learning_rate": 5e-05,
      "loss": 0.1871,
      "step": 848
    },
    {
      "epoch": 0.9941451990632318,
      "grad_norm": 0.7450397610664368,
      "learning_rate": 5e-05,
      "loss": 0.1296,
      "step": 849
    },
    {
      "epoch": 0.9953161592505855,
      "grad_norm": 0.9351373314857483,
      "learning_rate": 5e-05,
      "loss": 0.2463,
      "step": 850
    },
    {
      "epoch": 0.9964871194379391,
      "grad_norm": 1.018133282661438,
      "learning_rate": 5e-05,
      "loss": 0.3847,
      "step": 851
    },
    {
      "epoch": 0.9976580796252927,
      "grad_norm": 1.0995874404907227,
      "learning_rate": 5e-05,
      "loss": 0.1644,
      "step": 852
    },
    {
      "epoch": 0.9988290398126464,
      "grad_norm": 0.7507829666137695,
      "learning_rate": 5e-05,
      "loss": 0.1183,
      "step": 853
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8379902243614197,
      "learning_rate": 5e-05,
      "loss": 0.1106,
      "step": 854
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.16541706025600433,
      "eval_runtime": 101.884,
      "eval_samples_per_second": 8.205,
      "eval_steps_per_second": 1.031,
      "step": 854
    },
    {
      "epoch": 1.0011709601873535,
      "grad_norm": 1.151940941810608,
      "learning_rate": 5e-05,
      "loss": 0.2161,
      "step": 855
    },
    {
      "epoch": 1.0023419203747073,
      "grad_norm": 0.7923798561096191,
      "learning_rate": 5e-05,
      "loss": 0.1581,
      "step": 856
    },
    {
      "epoch": 1.0035128805620608,
      "grad_norm": 1.2003233432769775,
      "learning_rate": 5e-05,
      "loss": 0.1455,
      "step": 857
    },
    {
      "epoch": 1.0046838407494145,
      "grad_norm": 0.83486008644104,
      "learning_rate": 5e-05,
      "loss": 0.0868,
      "step": 858
    },
    {
      "epoch": 1.005854800936768,
      "grad_norm": 1.0938670635223389,
      "learning_rate": 5e-05,
      "loss": 0.1528,
      "step": 859
    },
    {
      "epoch": 1.0070257611241218,
      "grad_norm": 1.1934740543365479,
      "learning_rate": 5e-05,
      "loss": 0.093,
      "step": 860
    },
    {
      "epoch": 1.0081967213114753,
      "grad_norm": 1.1235462427139282,
      "learning_rate": 5e-05,
      "loss": 0.2784,
      "step": 861
    },
    {
      "epoch": 1.009367681498829,
      "grad_norm": 1.2917475700378418,
      "learning_rate": 5e-05,
      "loss": 0.0713,
      "step": 862
    },
    {
      "epoch": 1.0105386416861826,
      "grad_norm": 0.6646299958229065,
      "learning_rate": 5e-05,
      "loss": 0.0774,
      "step": 863
    },
    {
      "epoch": 1.0117096018735363,
      "grad_norm": 2.8837783336639404,
      "learning_rate": 5e-05,
      "loss": 0.0811,
      "step": 864
    },
    {
      "epoch": 1.0128805620608898,
      "grad_norm": 0.7792184352874756,
      "learning_rate": 5e-05,
      "loss": 0.1115,
      "step": 865
    },
    {
      "epoch": 1.0140515222482436,
      "grad_norm": 0.8769311904907227,
      "learning_rate": 5e-05,
      "loss": 0.1477,
      "step": 866
    },
    {
      "epoch": 1.015222482435597,
      "grad_norm": 0.6966947317123413,
      "learning_rate": 5e-05,
      "loss": 0.0922,
      "step": 867
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 0.7227500081062317,
      "learning_rate": 5e-05,
      "loss": 0.0659,
      "step": 868
    },
    {
      "epoch": 1.0175644028103044,
      "grad_norm": 1.0803000926971436,
      "learning_rate": 5e-05,
      "loss": 0.0738,
      "step": 869
    },
    {
      "epoch": 1.018735362997658,
      "grad_norm": 1.035835862159729,
      "learning_rate": 5e-05,
      "loss": 0.1186,
      "step": 870
    },
    {
      "epoch": 1.0199063231850116,
      "grad_norm": 0.9034751653671265,
      "learning_rate": 5e-05,
      "loss": 0.0604,
      "step": 871
    },
    {
      "epoch": 1.0210772833723654,
      "grad_norm": 0.8745287656784058,
      "learning_rate": 5e-05,
      "loss": 0.083,
      "step": 872
    },
    {
      "epoch": 1.0222482435597189,
      "grad_norm": 0.8004432320594788,
      "learning_rate": 5e-05,
      "loss": 0.1258,
      "step": 873
    },
    {
      "epoch": 1.0234192037470726,
      "grad_norm": 1.2528855800628662,
      "learning_rate": 5e-05,
      "loss": 0.1258,
      "step": 874
    },
    {
      "epoch": 1.0245901639344261,
      "grad_norm": 0.7040932178497314,
      "learning_rate": 5e-05,
      "loss": 0.0826,
      "step": 875
    },
    {
      "epoch": 1.0257611241217799,
      "grad_norm": 0.9953970313072205,
      "learning_rate": 5e-05,
      "loss": 0.1244,
      "step": 876
    },
    {
      "epoch": 1.0269320843091334,
      "grad_norm": 0.7510917782783508,
      "learning_rate": 5e-05,
      "loss": 0.0985,
      "step": 877
    },
    {
      "epoch": 1.0281030444964872,
      "grad_norm": 1.552765130996704,
      "learning_rate": 5e-05,
      "loss": 0.2073,
      "step": 878
    },
    {
      "epoch": 1.0292740046838407,
      "grad_norm": 0.7167999744415283,
      "learning_rate": 5e-05,
      "loss": 0.088,
      "step": 879
    },
    {
      "epoch": 1.0304449648711944,
      "grad_norm": 0.9328511357307434,
      "learning_rate": 5e-05,
      "loss": 0.0744,
      "step": 880
    },
    {
      "epoch": 1.031615925058548,
      "grad_norm": 1.5405513048171997,
      "learning_rate": 5e-05,
      "loss": 0.1671,
      "step": 881
    },
    {
      "epoch": 1.0327868852459017,
      "grad_norm": 0.9469549059867859,
      "learning_rate": 5e-05,
      "loss": 0.0569,
      "step": 882
    },
    {
      "epoch": 1.0339578454332552,
      "grad_norm": 1.0070390701293945,
      "learning_rate": 5e-05,
      "loss": 0.1238,
      "step": 883
    },
    {
      "epoch": 1.035128805620609,
      "grad_norm": 0.8337781429290771,
      "learning_rate": 5e-05,
      "loss": 0.1001,
      "step": 884
    },
    {
      "epoch": 1.0362997658079625,
      "grad_norm": 1.6136713027954102,
      "learning_rate": 5e-05,
      "loss": 0.0869,
      "step": 885
    },
    {
      "epoch": 1.0374707259953162,
      "grad_norm": 1.758420467376709,
      "learning_rate": 5e-05,
      "loss": 0.1495,
      "step": 886
    },
    {
      "epoch": 1.0386416861826697,
      "grad_norm": 0.8973942995071411,
      "learning_rate": 5e-05,
      "loss": 0.1524,
      "step": 887
    },
    {
      "epoch": 1.0398126463700235,
      "grad_norm": 1.0018770694732666,
      "learning_rate": 5e-05,
      "loss": 0.1377,
      "step": 888
    },
    {
      "epoch": 1.040983606557377,
      "grad_norm": 0.806682288646698,
      "learning_rate": 5e-05,
      "loss": 0.1096,
      "step": 889
    },
    {
      "epoch": 1.0421545667447307,
      "grad_norm": 0.8303607702255249,
      "learning_rate": 5e-05,
      "loss": 0.1704,
      "step": 890
    },
    {
      "epoch": 1.0433255269320842,
      "grad_norm": 1.1701346635818481,
      "learning_rate": 5e-05,
      "loss": 0.1806,
      "step": 891
    },
    {
      "epoch": 1.044496487119438,
      "grad_norm": 0.8356302976608276,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 892
    },
    {
      "epoch": 1.0456674473067915,
      "grad_norm": 0.7098332047462463,
      "learning_rate": 5e-05,
      "loss": 0.0608,
      "step": 893
    },
    {
      "epoch": 1.0468384074941453,
      "grad_norm": 0.7484983801841736,
      "learning_rate": 5e-05,
      "loss": 0.0612,
      "step": 894
    },
    {
      "epoch": 1.0480093676814988,
      "grad_norm": 1.0162099599838257,
      "learning_rate": 5e-05,
      "loss": 0.0838,
      "step": 895
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 0.7796477675437927,
      "learning_rate": 5e-05,
      "loss": 0.1266,
      "step": 896
    },
    {
      "epoch": 1.050351288056206,
      "grad_norm": 0.9740942120552063,
      "learning_rate": 5e-05,
      "loss": 0.104,
      "step": 897
    },
    {
      "epoch": 1.0515222482435598,
      "grad_norm": 0.47828152775764465,
      "learning_rate": 5e-05,
      "loss": 0.0658,
      "step": 898
    },
    {
      "epoch": 1.0526932084309133,
      "grad_norm": 0.7781299352645874,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 899
    },
    {
      "epoch": 1.053864168618267,
      "grad_norm": 0.7576349377632141,
      "learning_rate": 5e-05,
      "loss": 0.0971,
      "step": 900
    },
    {
      "epoch": 1.0550351288056206,
      "grad_norm": 0.6864163279533386,
      "learning_rate": 5e-05,
      "loss": 0.0649,
      "step": 901
    },
    {
      "epoch": 1.0562060889929743,
      "grad_norm": 0.7485500574111938,
      "learning_rate": 5e-05,
      "loss": 0.07,
      "step": 902
    },
    {
      "epoch": 1.0573770491803278,
      "grad_norm": 0.6622416973114014,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 903
    },
    {
      "epoch": 1.0585480093676816,
      "grad_norm": 0.8400798439979553,
      "learning_rate": 5e-05,
      "loss": 0.1022,
      "step": 904
    },
    {
      "epoch": 1.059718969555035,
      "grad_norm": 1.2274916172027588,
      "learning_rate": 5e-05,
      "loss": 0.102,
      "step": 905
    },
    {
      "epoch": 1.0608899297423888,
      "grad_norm": 0.8824913501739502,
      "learning_rate": 5e-05,
      "loss": 0.113,
      "step": 906
    },
    {
      "epoch": 1.0620608899297423,
      "grad_norm": 1.3109914064407349,
      "learning_rate": 5e-05,
      "loss": 0.0966,
      "step": 907
    },
    {
      "epoch": 1.063231850117096,
      "grad_norm": 0.9098508954048157,
      "learning_rate": 5e-05,
      "loss": 0.0652,
      "step": 908
    },
    {
      "epoch": 1.0644028103044496,
      "grad_norm": 1.2313554286956787,
      "learning_rate": 5e-05,
      "loss": 0.1263,
      "step": 909
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 0.9115283489227295,
      "learning_rate": 5e-05,
      "loss": 0.108,
      "step": 910
    },
    {
      "epoch": 1.0667447306791569,
      "grad_norm": 1.0448311567306519,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 911
    },
    {
      "epoch": 1.0679156908665106,
      "grad_norm": 0.7749192118644714,
      "learning_rate": 5e-05,
      "loss": 0.1722,
      "step": 912
    },
    {
      "epoch": 1.0690866510538641,
      "grad_norm": 0.8930205702781677,
      "learning_rate": 5e-05,
      "loss": 0.0826,
      "step": 913
    },
    {
      "epoch": 1.0702576112412179,
      "grad_norm": 1.165476679801941,
      "learning_rate": 5e-05,
      "loss": 0.101,
      "step": 914
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.8813146352767944,
      "learning_rate": 5e-05,
      "loss": 0.0511,
      "step": 915
    },
    {
      "epoch": 1.0725995316159251,
      "grad_norm": 1.0445953607559204,
      "learning_rate": 5e-05,
      "loss": 0.1184,
      "step": 916
    },
    {
      "epoch": 1.0737704918032787,
      "grad_norm": 0.6158308386802673,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 917
    },
    {
      "epoch": 1.0749414519906324,
      "grad_norm": 0.7244027256965637,
      "learning_rate": 5e-05,
      "loss": 0.0711,
      "step": 918
    },
    {
      "epoch": 1.076112412177986,
      "grad_norm": 0.900812566280365,
      "learning_rate": 5e-05,
      "loss": 0.0399,
      "step": 919
    },
    {
      "epoch": 1.0772833723653397,
      "grad_norm": 0.6156314015388489,
      "learning_rate": 5e-05,
      "loss": 0.0685,
      "step": 920
    },
    {
      "epoch": 1.0784543325526932,
      "grad_norm": 0.8689392805099487,
      "learning_rate": 5e-05,
      "loss": 0.0811,
      "step": 921
    },
    {
      "epoch": 1.079625292740047,
      "grad_norm": 0.5036198496818542,
      "learning_rate": 5e-05,
      "loss": 0.0612,
      "step": 922
    },
    {
      "epoch": 1.0807962529274004,
      "grad_norm": 1.132161021232605,
      "learning_rate": 5e-05,
      "loss": 0.1262,
      "step": 923
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 1.2214133739471436,
      "learning_rate": 5e-05,
      "loss": 0.1427,
      "step": 924
    },
    {
      "epoch": 1.0831381733021077,
      "grad_norm": 0.8428274989128113,
      "learning_rate": 5e-05,
      "loss": 0.0781,
      "step": 925
    },
    {
      "epoch": 1.0843091334894615,
      "grad_norm": 0.7314708828926086,
      "learning_rate": 5e-05,
      "loss": 0.1237,
      "step": 926
    },
    {
      "epoch": 1.085480093676815,
      "grad_norm": 1.4622050523757935,
      "learning_rate": 5e-05,
      "loss": 0.2849,
      "step": 927
    },
    {
      "epoch": 1.0866510538641687,
      "grad_norm": 0.7445735931396484,
      "learning_rate": 5e-05,
      "loss": 0.1272,
      "step": 928
    },
    {
      "epoch": 1.0878220140515222,
      "grad_norm": 1.3007895946502686,
      "learning_rate": 5e-05,
      "loss": 0.214,
      "step": 929
    },
    {
      "epoch": 1.088992974238876,
      "grad_norm": 0.8255482316017151,
      "learning_rate": 5e-05,
      "loss": 0.0831,
      "step": 930
    },
    {
      "epoch": 1.0901639344262295,
      "grad_norm": 1.3279696702957153,
      "learning_rate": 5e-05,
      "loss": 0.1482,
      "step": 931
    },
    {
      "epoch": 1.0913348946135832,
      "grad_norm": 1.3484432697296143,
      "learning_rate": 5e-05,
      "loss": 0.0675,
      "step": 932
    },
    {
      "epoch": 1.0925058548009368,
      "grad_norm": 1.0447899103164673,
      "learning_rate": 5e-05,
      "loss": 0.1568,
      "step": 933
    },
    {
      "epoch": 1.0936768149882905,
      "grad_norm": 0.9567832350730896,
      "learning_rate": 5e-05,
      "loss": 0.0769,
      "step": 934
    },
    {
      "epoch": 1.094847775175644,
      "grad_norm": 1.0293687582015991,
      "learning_rate": 5e-05,
      "loss": 0.0975,
      "step": 935
    },
    {
      "epoch": 1.0960187353629975,
      "grad_norm": 1.0805299282073975,
      "learning_rate": 5e-05,
      "loss": 0.1316,
      "step": 936
    },
    {
      "epoch": 1.0971896955503513,
      "grad_norm": 0.8852304220199585,
      "learning_rate": 5e-05,
      "loss": 0.0786,
      "step": 937
    },
    {
      "epoch": 1.098360655737705,
      "grad_norm": 0.6092891097068787,
      "learning_rate": 5e-05,
      "loss": 0.0644,
      "step": 938
    },
    {
      "epoch": 1.0995316159250585,
      "grad_norm": 1.2921152114868164,
      "learning_rate": 5e-05,
      "loss": 0.0933,
      "step": 939
    },
    {
      "epoch": 1.100702576112412,
      "grad_norm": 0.5970662236213684,
      "learning_rate": 5e-05,
      "loss": 0.0729,
      "step": 940
    },
    {
      "epoch": 1.1018735362997658,
      "grad_norm": 1.001516342163086,
      "learning_rate": 5e-05,
      "loss": 0.0638,
      "step": 941
    },
    {
      "epoch": 1.1030444964871196,
      "grad_norm": 1.713270902633667,
      "learning_rate": 5e-05,
      "loss": 0.1178,
      "step": 942
    },
    {
      "epoch": 1.104215456674473,
      "grad_norm": 0.6208271980285645,
      "learning_rate": 5e-05,
      "loss": 0.0513,
      "step": 943
    },
    {
      "epoch": 1.1053864168618266,
      "grad_norm": 0.8759042024612427,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 944
    },
    {
      "epoch": 1.1065573770491803,
      "grad_norm": 1.627718448638916,
      "learning_rate": 5e-05,
      "loss": 0.1906,
      "step": 945
    },
    {
      "epoch": 1.1077283372365339,
      "grad_norm": 0.7605425715446472,
      "learning_rate": 5e-05,
      "loss": 0.092,
      "step": 946
    },
    {
      "epoch": 1.1088992974238876,
      "grad_norm": 0.8740405440330505,
      "learning_rate": 5e-05,
      "loss": 0.1278,
      "step": 947
    },
    {
      "epoch": 1.1100702576112411,
      "grad_norm": 0.6376877427101135,
      "learning_rate": 5e-05,
      "loss": 0.0502,
      "step": 948
    },
    {
      "epoch": 1.1112412177985949,
      "grad_norm": 0.9336209297180176,
      "learning_rate": 5e-05,
      "loss": 0.0658,
      "step": 949
    },
    {
      "epoch": 1.1124121779859484,
      "grad_norm": 0.9416927695274353,
      "learning_rate": 5e-05,
      "loss": 0.1921,
      "step": 950
    },
    {
      "epoch": 1.1135831381733021,
      "grad_norm": 0.7761294841766357,
      "learning_rate": 5e-05,
      "loss": 0.1058,
      "step": 951
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 1.3780417442321777,
      "learning_rate": 5e-05,
      "loss": 0.1532,
      "step": 952
    },
    {
      "epoch": 1.1159250585480094,
      "grad_norm": 2.336538314819336,
      "learning_rate": 5e-05,
      "loss": 0.1225,
      "step": 953
    },
    {
      "epoch": 1.117096018735363,
      "grad_norm": 0.8807106018066406,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 954
    },
    {
      "epoch": 1.1182669789227166,
      "grad_norm": 1.012447476387024,
      "learning_rate": 5e-05,
      "loss": 0.215,
      "step": 955
    },
    {
      "epoch": 1.1194379391100702,
      "grad_norm": 0.906728208065033,
      "learning_rate": 5e-05,
      "loss": 0.049,
      "step": 956
    },
    {
      "epoch": 1.120608899297424,
      "grad_norm": 1.1421211957931519,
      "learning_rate": 5e-05,
      "loss": 0.2224,
      "step": 957
    },
    {
      "epoch": 1.1217798594847774,
      "grad_norm": 1.1028995513916016,
      "learning_rate": 5e-05,
      "loss": 0.2837,
      "step": 958
    },
    {
      "epoch": 1.1229508196721312,
      "grad_norm": 0.8003747463226318,
      "learning_rate": 5e-05,
      "loss": 0.0665,
      "step": 959
    },
    {
      "epoch": 1.1241217798594847,
      "grad_norm": 1.5939431190490723,
      "learning_rate": 5e-05,
      "loss": 0.1585,
      "step": 960
    },
    {
      "epoch": 1.1252927400468384,
      "grad_norm": 0.6096866130828857,
      "learning_rate": 5e-05,
      "loss": 0.1812,
      "step": 961
    },
    {
      "epoch": 1.126463700234192,
      "grad_norm": 0.6827800273895264,
      "learning_rate": 5e-05,
      "loss": 0.0989,
      "step": 962
    },
    {
      "epoch": 1.1276346604215457,
      "grad_norm": 1.070713996887207,
      "learning_rate": 5e-05,
      "loss": 0.1749,
      "step": 963
    },
    {
      "epoch": 1.1288056206088992,
      "grad_norm": 1.0329498052597046,
      "learning_rate": 5e-05,
      "loss": 0.1344,
      "step": 964
    },
    {
      "epoch": 1.129976580796253,
      "grad_norm": 0.6190810203552246,
      "learning_rate": 5e-05,
      "loss": 0.0877,
      "step": 965
    },
    {
      "epoch": 1.1311475409836065,
      "grad_norm": 1.1008936166763306,
      "learning_rate": 5e-05,
      "loss": 0.1269,
      "step": 966
    },
    {
      "epoch": 1.1323185011709602,
      "grad_norm": 0.6926875710487366,
      "learning_rate": 5e-05,
      "loss": 0.0617,
      "step": 967
    },
    {
      "epoch": 1.1334894613583137,
      "grad_norm": 0.7648265361785889,
      "learning_rate": 5e-05,
      "loss": 0.1051,
      "step": 968
    },
    {
      "epoch": 1.1346604215456675,
      "grad_norm": 1.0498571395874023,
      "learning_rate": 5e-05,
      "loss": 0.113,
      "step": 969
    },
    {
      "epoch": 1.135831381733021,
      "grad_norm": 0.868440568447113,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 970
    },
    {
      "epoch": 1.1370023419203747,
      "grad_norm": 0.8016441464424133,
      "learning_rate": 5e-05,
      "loss": 0.0668,
      "step": 971
    },
    {
      "epoch": 1.1381733021077283,
      "grad_norm": 1.3270835876464844,
      "learning_rate": 5e-05,
      "loss": 0.3009,
      "step": 972
    },
    {
      "epoch": 1.139344262295082,
      "grad_norm": 0.6909616589546204,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 973
    },
    {
      "epoch": 1.1405152224824355,
      "grad_norm": 0.8843189477920532,
      "learning_rate": 5e-05,
      "loss": 0.0927,
      "step": 974
    },
    {
      "epoch": 1.1416861826697893,
      "grad_norm": 0.8712651133537292,
      "learning_rate": 5e-05,
      "loss": 0.1182,
      "step": 975
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.8307519555091858,
      "learning_rate": 5e-05,
      "loss": 0.0795,
      "step": 976
    },
    {
      "epoch": 1.1440281030444965,
      "grad_norm": 0.8849190473556519,
      "learning_rate": 5e-05,
      "loss": 0.0948,
      "step": 977
    },
    {
      "epoch": 1.14519906323185,
      "grad_norm": 0.8481617569923401,
      "learning_rate": 5e-05,
      "loss": 0.1612,
      "step": 978
    },
    {
      "epoch": 1.1463700234192038,
      "grad_norm": 1.051766276359558,
      "learning_rate": 5e-05,
      "loss": 0.0926,
      "step": 979
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 1.0437809228897095,
      "learning_rate": 5e-05,
      "loss": 0.136,
      "step": 980
    },
    {
      "epoch": 1.148711943793911,
      "grad_norm": 0.9364490509033203,
      "learning_rate": 5e-05,
      "loss": 0.1645,
      "step": 981
    },
    {
      "epoch": 1.1498829039812646,
      "grad_norm": 0.5945702195167542,
      "learning_rate": 5e-05,
      "loss": 0.0562,
      "step": 982
    },
    {
      "epoch": 1.1510538641686183,
      "grad_norm": 1.5547276735305786,
      "learning_rate": 5e-05,
      "loss": 0.1153,
      "step": 983
    },
    {
      "epoch": 1.1522248243559718,
      "grad_norm": 1.7712916135787964,
      "learning_rate": 5e-05,
      "loss": 0.2304,
      "step": 984
    },
    {
      "epoch": 1.1533957845433256,
      "grad_norm": 0.7487919330596924,
      "learning_rate": 5e-05,
      "loss": 0.0905,
      "step": 985
    },
    {
      "epoch": 1.154566744730679,
      "grad_norm": 0.8909608125686646,
      "learning_rate": 5e-05,
      "loss": 0.1183,
      "step": 986
    },
    {
      "epoch": 1.1557377049180328,
      "grad_norm": 1.6846628189086914,
      "learning_rate": 5e-05,
      "loss": 0.0833,
      "step": 987
    },
    {
      "epoch": 1.1569086651053864,
      "grad_norm": 0.9794958233833313,
      "learning_rate": 5e-05,
      "loss": 0.2001,
      "step": 988
    },
    {
      "epoch": 1.1580796252927401,
      "grad_norm": 1.0939748287200928,
      "learning_rate": 5e-05,
      "loss": 0.1821,
      "step": 989
    },
    {
      "epoch": 1.1592505854800936,
      "grad_norm": 0.7225865125656128,
      "learning_rate": 5e-05,
      "loss": 0.1196,
      "step": 990
    },
    {
      "epoch": 1.1604215456674474,
      "grad_norm": 0.9110050797462463,
      "learning_rate": 5e-05,
      "loss": 0.0853,
      "step": 991
    },
    {
      "epoch": 1.161592505854801,
      "grad_norm": 0.7433254718780518,
      "learning_rate": 5e-05,
      "loss": 0.0946,
      "step": 992
    },
    {
      "epoch": 1.1627634660421546,
      "grad_norm": 0.716233491897583,
      "learning_rate": 5e-05,
      "loss": 0.0468,
      "step": 993
    },
    {
      "epoch": 1.1639344262295082,
      "grad_norm": 0.8094121217727661,
      "learning_rate": 5e-05,
      "loss": 0.1283,
      "step": 994
    },
    {
      "epoch": 1.165105386416862,
      "grad_norm": 1.0160804986953735,
      "learning_rate": 5e-05,
      "loss": 0.2042,
      "step": 995
    },
    {
      "epoch": 1.1662763466042154,
      "grad_norm": 1.5628324747085571,
      "learning_rate": 5e-05,
      "loss": 0.1619,
      "step": 996
    },
    {
      "epoch": 1.1674473067915692,
      "grad_norm": 0.8230119943618774,
      "learning_rate": 5e-05,
      "loss": 0.1454,
      "step": 997
    },
    {
      "epoch": 1.1686182669789227,
      "grad_norm": 0.941114068031311,
      "learning_rate": 5e-05,
      "loss": 0.1313,
      "step": 998
    },
    {
      "epoch": 1.1697892271662764,
      "grad_norm": 0.6463528275489807,
      "learning_rate": 5e-05,
      "loss": 0.098,
      "step": 999
    },
    {
      "epoch": 1.17096018735363,
      "grad_norm": 1.3051587343215942,
      "learning_rate": 5e-05,
      "loss": 0.0682,
      "step": 1000
    },
    {
      "epoch": 1.1721311475409837,
      "grad_norm": 0.6679378151893616,
      "learning_rate": 5e-05,
      "loss": 0.1187,
      "step": 1001
    },
    {
      "epoch": 1.1733021077283372,
      "grad_norm": 0.5873035192489624,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 1002
    },
    {
      "epoch": 1.174473067915691,
      "grad_norm": 1.0574883222579956,
      "learning_rate": 5e-05,
      "loss": 0.1453,
      "step": 1003
    },
    {
      "epoch": 1.1756440281030445,
      "grad_norm": 0.8725888729095459,
      "learning_rate": 5e-05,
      "loss": 0.1009,
      "step": 1004
    },
    {
      "epoch": 1.1768149882903982,
      "grad_norm": 1.1221481561660767,
      "learning_rate": 5e-05,
      "loss": 0.1414,
      "step": 1005
    },
    {
      "epoch": 1.1779859484777517,
      "grad_norm": 0.5382229685783386,
      "learning_rate": 5e-05,
      "loss": 0.0388,
      "step": 1006
    },
    {
      "epoch": 1.1791569086651055,
      "grad_norm": 1.6495563983917236,
      "learning_rate": 5e-05,
      "loss": 0.1089,
      "step": 1007
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 1.6916277408599854,
      "learning_rate": 5e-05,
      "loss": 0.0541,
      "step": 1008
    },
    {
      "epoch": 1.1814988290398127,
      "grad_norm": 0.5968879461288452,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 1009
    },
    {
      "epoch": 1.1826697892271663,
      "grad_norm": 1.2649567127227783,
      "learning_rate": 5e-05,
      "loss": 0.0911,
      "step": 1010
    },
    {
      "epoch": 1.18384074941452,
      "grad_norm": 1.1336688995361328,
      "learning_rate": 5e-05,
      "loss": 0.1347,
      "step": 1011
    },
    {
      "epoch": 1.1850117096018735,
      "grad_norm": 1.3277208805084229,
      "learning_rate": 5e-05,
      "loss": 0.1439,
      "step": 1012
    },
    {
      "epoch": 1.1861826697892273,
      "grad_norm": 1.7551823854446411,
      "learning_rate": 5e-05,
      "loss": 0.1424,
      "step": 1013
    },
    {
      "epoch": 1.1873536299765808,
      "grad_norm": 1.2057291269302368,
      "learning_rate": 5e-05,
      "loss": 0.1409,
      "step": 1014
    },
    {
      "epoch": 1.1885245901639343,
      "grad_norm": 0.9955047965049744,
      "learning_rate": 5e-05,
      "loss": 0.098,
      "step": 1015
    },
    {
      "epoch": 1.189695550351288,
      "grad_norm": 0.9968352317810059,
      "learning_rate": 5e-05,
      "loss": 0.1092,
      "step": 1016
    },
    {
      "epoch": 1.1908665105386418,
      "grad_norm": 0.7426109313964844,
      "learning_rate": 5e-05,
      "loss": 0.0735,
      "step": 1017
    },
    {
      "epoch": 1.1920374707259953,
      "grad_norm": 0.8697065711021423,
      "learning_rate": 5e-05,
      "loss": 0.0581,
      "step": 1018
    },
    {
      "epoch": 1.1932084309133488,
      "grad_norm": 0.6568493843078613,
      "learning_rate": 5e-05,
      "loss": 0.1026,
      "step": 1019
    },
    {
      "epoch": 1.1943793911007026,
      "grad_norm": 1.2729089260101318,
      "learning_rate": 5e-05,
      "loss": 0.1601,
      "step": 1020
    },
    {
      "epoch": 1.1955503512880563,
      "grad_norm": 1.6168220043182373,
      "learning_rate": 5e-05,
      "loss": 0.1605,
      "step": 1021
    },
    {
      "epoch": 1.1967213114754098,
      "grad_norm": 1.082131266593933,
      "learning_rate": 5e-05,
      "loss": 0.2207,
      "step": 1022
    },
    {
      "epoch": 1.1978922716627634,
      "grad_norm": 0.8541383743286133,
      "learning_rate": 5e-05,
      "loss": 0.0828,
      "step": 1023
    },
    {
      "epoch": 1.199063231850117,
      "grad_norm": 0.8689570426940918,
      "learning_rate": 5e-05,
      "loss": 0.0931,
      "step": 1024
    },
    {
      "epoch": 1.2002341920374708,
      "grad_norm": 0.8741928339004517,
      "learning_rate": 5e-05,
      "loss": 0.1497,
      "step": 1025
    },
    {
      "epoch": 1.2014051522248244,
      "grad_norm": 0.7830703854560852,
      "learning_rate": 5e-05,
      "loss": 0.0968,
      "step": 1026
    },
    {
      "epoch": 1.2025761124121779,
      "grad_norm": 1.318927526473999,
      "learning_rate": 5e-05,
      "loss": 0.1397,
      "step": 1027
    },
    {
      "epoch": 1.2037470725995316,
      "grad_norm": 0.5266023278236389,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1028
    },
    {
      "epoch": 1.2049180327868854,
      "grad_norm": 0.9473868608474731,
      "learning_rate": 5e-05,
      "loss": 0.1243,
      "step": 1029
    },
    {
      "epoch": 1.2060889929742389,
      "grad_norm": 0.882849395275116,
      "learning_rate": 5e-05,
      "loss": 0.0802,
      "step": 1030
    },
    {
      "epoch": 1.2072599531615924,
      "grad_norm": 0.5162063837051392,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 1031
    },
    {
      "epoch": 1.2084309133489461,
      "grad_norm": 0.7734959721565247,
      "learning_rate": 5e-05,
      "loss": 0.0783,
      "step": 1032
    },
    {
      "epoch": 1.2096018735362999,
      "grad_norm": 0.7181130051612854,
      "learning_rate": 5e-05,
      "loss": 0.057,
      "step": 1033
    },
    {
      "epoch": 1.2107728337236534,
      "grad_norm": 0.9366224408149719,
      "learning_rate": 5e-05,
      "loss": 0.1058,
      "step": 1034
    },
    {
      "epoch": 1.211943793911007,
      "grad_norm": 1.0336859226226807,
      "learning_rate": 5e-05,
      "loss": 0.1997,
      "step": 1035
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 0.59991455078125,
      "learning_rate": 5e-05,
      "loss": 0.091,
      "step": 1036
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 0.8937666416168213,
      "learning_rate": 5e-05,
      "loss": 0.1217,
      "step": 1037
    },
    {
      "epoch": 1.215456674473068,
      "grad_norm": 0.6452416181564331,
      "learning_rate": 5e-05,
      "loss": 0.0782,
      "step": 1038
    },
    {
      "epoch": 1.2166276346604215,
      "grad_norm": 1.6096382141113281,
      "learning_rate": 5e-05,
      "loss": 0.2476,
      "step": 1039
    },
    {
      "epoch": 1.2177985948477752,
      "grad_norm": 0.6541365385055542,
      "learning_rate": 5e-05,
      "loss": 0.0879,
      "step": 1040
    },
    {
      "epoch": 1.2189695550351287,
      "grad_norm": 0.9784449338912964,
      "learning_rate": 5e-05,
      "loss": 0.103,
      "step": 1041
    },
    {
      "epoch": 1.2201405152224825,
      "grad_norm": 0.9161689877510071,
      "learning_rate": 5e-05,
      "loss": 0.1016,
      "step": 1042
    },
    {
      "epoch": 1.221311475409836,
      "grad_norm": 0.8426845669746399,
      "learning_rate": 5e-05,
      "loss": 0.0991,
      "step": 1043
    },
    {
      "epoch": 1.2224824355971897,
      "grad_norm": 1.2674845457077026,
      "learning_rate": 5e-05,
      "loss": 0.2645,
      "step": 1044
    },
    {
      "epoch": 1.2236533957845432,
      "grad_norm": 0.9405121803283691,
      "learning_rate": 5e-05,
      "loss": 0.1797,
      "step": 1045
    },
    {
      "epoch": 1.224824355971897,
      "grad_norm": 0.8116763234138489,
      "learning_rate": 5e-05,
      "loss": 0.0922,
      "step": 1046
    },
    {
      "epoch": 1.2259953161592505,
      "grad_norm": 0.6850758194923401,
      "learning_rate": 5e-05,
      "loss": 0.0752,
      "step": 1047
    },
    {
      "epoch": 1.2271662763466042,
      "grad_norm": 0.9333342909812927,
      "learning_rate": 5e-05,
      "loss": 0.0745,
      "step": 1048
    },
    {
      "epoch": 1.2283372365339578,
      "grad_norm": 0.7847335338592529,
      "learning_rate": 5e-05,
      "loss": 0.0913,
      "step": 1049
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 1.0539494752883911,
      "learning_rate": 5e-05,
      "loss": 0.0579,
      "step": 1050
    },
    {
      "epoch": 1.230679156908665,
      "grad_norm": 1.0894551277160645,
      "learning_rate": 5e-05,
      "loss": 0.153,
      "step": 1051
    },
    {
      "epoch": 1.2318501170960188,
      "grad_norm": 0.9188913702964783,
      "learning_rate": 5e-05,
      "loss": 0.0799,
      "step": 1052
    },
    {
      "epoch": 1.2330210772833723,
      "grad_norm": 0.7822161316871643,
      "learning_rate": 5e-05,
      "loss": 0.1116,
      "step": 1053
    },
    {
      "epoch": 1.234192037470726,
      "grad_norm": 0.7304376363754272,
      "learning_rate": 5e-05,
      "loss": 0.0438,
      "step": 1054
    },
    {
      "epoch": 1.2353629976580796,
      "grad_norm": 0.8031301498413086,
      "learning_rate": 5e-05,
      "loss": 0.0879,
      "step": 1055
    },
    {
      "epoch": 1.2365339578454333,
      "grad_norm": 1.1515816450119019,
      "learning_rate": 5e-05,
      "loss": 0.2081,
      "step": 1056
    },
    {
      "epoch": 1.2377049180327868,
      "grad_norm": 0.9484239220619202,
      "learning_rate": 5e-05,
      "loss": 0.0576,
      "step": 1057
    },
    {
      "epoch": 1.2388758782201406,
      "grad_norm": 0.7405797839164734,
      "learning_rate": 5e-05,
      "loss": 0.0528,
      "step": 1058
    },
    {
      "epoch": 1.240046838407494,
      "grad_norm": 1.684134840965271,
      "learning_rate": 5e-05,
      "loss": 0.131,
      "step": 1059
    },
    {
      "epoch": 1.2412177985948478,
      "grad_norm": 0.8090125918388367,
      "learning_rate": 5e-05,
      "loss": 0.0962,
      "step": 1060
    },
    {
      "epoch": 1.2423887587822013,
      "grad_norm": 0.51863032579422,
      "learning_rate": 5e-05,
      "loss": 0.037,
      "step": 1061
    },
    {
      "epoch": 1.243559718969555,
      "grad_norm": 0.7835404872894287,
      "learning_rate": 5e-05,
      "loss": 0.0976,
      "step": 1062
    },
    {
      "epoch": 1.2447306791569086,
      "grad_norm": 0.8359652161598206,
      "learning_rate": 5e-05,
      "loss": 0.096,
      "step": 1063
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 1.342238187789917,
      "learning_rate": 5e-05,
      "loss": 0.2054,
      "step": 1064
    },
    {
      "epoch": 1.2470725995316159,
      "grad_norm": 0.8264142274856567,
      "learning_rate": 5e-05,
      "loss": 0.055,
      "step": 1065
    },
    {
      "epoch": 1.2482435597189696,
      "grad_norm": 1.088131070137024,
      "learning_rate": 5e-05,
      "loss": 0.0736,
      "step": 1066
    },
    {
      "epoch": 1.2494145199063231,
      "grad_norm": 1.801243782043457,
      "learning_rate": 5e-05,
      "loss": 0.0748,
      "step": 1067
    },
    {
      "epoch": 1.2505854800936769,
      "grad_norm": 0.7763508558273315,
      "learning_rate": 5e-05,
      "loss": 0.0588,
      "step": 1068
    },
    {
      "epoch": 1.2517564402810304,
      "grad_norm": 0.6443403363227844,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 1069
    },
    {
      "epoch": 1.2529274004683841,
      "grad_norm": 0.6315490007400513,
      "learning_rate": 5e-05,
      "loss": 0.0415,
      "step": 1070
    },
    {
      "epoch": 1.2540983606557377,
      "grad_norm": 0.562595546245575,
      "learning_rate": 5e-05,
      "loss": 0.0599,
      "step": 1071
    },
    {
      "epoch": 1.2552693208430914,
      "grad_norm": 0.48830753564834595,
      "learning_rate": 5e-05,
      "loss": 0.0641,
      "step": 1072
    },
    {
      "epoch": 1.256440281030445,
      "grad_norm": 0.8471103310585022,
      "learning_rate": 5e-05,
      "loss": 0.0551,
      "step": 1073
    },
    {
      "epoch": 1.2576112412177987,
      "grad_norm": 0.7866370677947998,
      "learning_rate": 5e-05,
      "loss": 0.0758,
      "step": 1074
    },
    {
      "epoch": 1.2587822014051522,
      "grad_norm": 1.0710947513580322,
      "learning_rate": 5e-05,
      "loss": 0.0814,
      "step": 1075
    },
    {
      "epoch": 1.259953161592506,
      "grad_norm": 0.5522665977478027,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 1076
    },
    {
      "epoch": 1.2611241217798594,
      "grad_norm": 0.929943323135376,
      "learning_rate": 5e-05,
      "loss": 0.123,
      "step": 1077
    },
    {
      "epoch": 1.2622950819672132,
      "grad_norm": 0.9613716006278992,
      "learning_rate": 5e-05,
      "loss": 0.1391,
      "step": 1078
    },
    {
      "epoch": 1.2634660421545667,
      "grad_norm": 0.8849643468856812,
      "learning_rate": 5e-05,
      "loss": 0.0972,
      "step": 1079
    },
    {
      "epoch": 1.2646370023419204,
      "grad_norm": 0.914352297782898,
      "learning_rate": 5e-05,
      "loss": 0.0765,
      "step": 1080
    },
    {
      "epoch": 1.265807962529274,
      "grad_norm": 1.2070180177688599,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 1081
    },
    {
      "epoch": 1.2669789227166277,
      "grad_norm": 1.3281670808792114,
      "learning_rate": 5e-05,
      "loss": 0.1214,
      "step": 1082
    },
    {
      "epoch": 1.2681498829039812,
      "grad_norm": 1.2199058532714844,
      "learning_rate": 5e-05,
      "loss": 0.0924,
      "step": 1083
    },
    {
      "epoch": 1.269320843091335,
      "grad_norm": 1.3124661445617676,
      "learning_rate": 5e-05,
      "loss": 0.2804,
      "step": 1084
    },
    {
      "epoch": 1.2704918032786885,
      "grad_norm": 0.6481671333312988,
      "learning_rate": 5e-05,
      "loss": 0.0491,
      "step": 1085
    },
    {
      "epoch": 1.2716627634660422,
      "grad_norm": 0.8243889808654785,
      "learning_rate": 5e-05,
      "loss": 0.1182,
      "step": 1086
    },
    {
      "epoch": 1.2728337236533958,
      "grad_norm": 1.2439526319503784,
      "learning_rate": 5e-05,
      "loss": 0.0782,
      "step": 1087
    },
    {
      "epoch": 1.2740046838407495,
      "grad_norm": 1.0573139190673828,
      "learning_rate": 5e-05,
      "loss": 0.0622,
      "step": 1088
    },
    {
      "epoch": 1.275175644028103,
      "grad_norm": 0.6602525115013123,
      "learning_rate": 5e-05,
      "loss": 0.0501,
      "step": 1089
    },
    {
      "epoch": 1.2763466042154565,
      "grad_norm": 1.0120625495910645,
      "learning_rate": 5e-05,
      "loss": 0.0678,
      "step": 1090
    },
    {
      "epoch": 1.2775175644028103,
      "grad_norm": 0.8888760209083557,
      "learning_rate": 5e-05,
      "loss": 0.0832,
      "step": 1091
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 1.3051173686981201,
      "learning_rate": 5e-05,
      "loss": 0.0835,
      "step": 1092
    },
    {
      "epoch": 1.2798594847775175,
      "grad_norm": 1.0282355546951294,
      "learning_rate": 5e-05,
      "loss": 0.1533,
      "step": 1093
    },
    {
      "epoch": 1.281030444964871,
      "grad_norm": 1.2703596353530884,
      "learning_rate": 5e-05,
      "loss": 0.1538,
      "step": 1094
    },
    {
      "epoch": 1.2822014051522248,
      "grad_norm": 1.2676563262939453,
      "learning_rate": 5e-05,
      "loss": 0.0966,
      "step": 1095
    },
    {
      "epoch": 1.2833723653395785,
      "grad_norm": 0.8923988342285156,
      "learning_rate": 5e-05,
      "loss": 0.0694,
      "step": 1096
    },
    {
      "epoch": 1.284543325526932,
      "grad_norm": 1.7959671020507812,
      "learning_rate": 5e-05,
      "loss": 0.121,
      "step": 1097
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.028248906135559,
      "learning_rate": 5e-05,
      "loss": 0.1141,
      "step": 1098
    },
    {
      "epoch": 1.2868852459016393,
      "grad_norm": 0.5485435128211975,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 1099
    },
    {
      "epoch": 1.288056206088993,
      "grad_norm": 0.8268666863441467,
      "learning_rate": 5e-05,
      "loss": 0.0691,
      "step": 1100
    },
    {
      "epoch": 1.2892271662763466,
      "grad_norm": 0.8268030881881714,
      "learning_rate": 5e-05,
      "loss": 0.0502,
      "step": 1101
    },
    {
      "epoch": 1.2903981264637001,
      "grad_norm": 0.7741153836250305,
      "learning_rate": 5e-05,
      "loss": 0.1011,
      "step": 1102
    },
    {
      "epoch": 1.2915690866510539,
      "grad_norm": 1.1838024854660034,
      "learning_rate": 5e-05,
      "loss": 0.0818,
      "step": 1103
    },
    {
      "epoch": 1.2927400468384076,
      "grad_norm": 0.779233992099762,
      "learning_rate": 5e-05,
      "loss": 0.0496,
      "step": 1104
    },
    {
      "epoch": 1.2939110070257611,
      "grad_norm": 1.1858351230621338,
      "learning_rate": 5e-05,
      "loss": 0.155,
      "step": 1105
    },
    {
      "epoch": 1.2950819672131146,
      "grad_norm": 0.8030239343643188,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 1106
    },
    {
      "epoch": 1.2962529274004684,
      "grad_norm": 1.5130316019058228,
      "learning_rate": 5e-05,
      "loss": 0.1474,
      "step": 1107
    },
    {
      "epoch": 1.2974238875878221,
      "grad_norm": 1.6133562326431274,
      "learning_rate": 5e-05,
      "loss": 0.0798,
      "step": 1108
    },
    {
      "epoch": 1.2985948477751756,
      "grad_norm": 0.9603919386863708,
      "learning_rate": 5e-05,
      "loss": 0.0825,
      "step": 1109
    },
    {
      "epoch": 1.2997658079625292,
      "grad_norm": 1.7431405782699585,
      "learning_rate": 5e-05,
      "loss": 0.1848,
      "step": 1110
    },
    {
      "epoch": 1.300936768149883,
      "grad_norm": 0.7769522666931152,
      "learning_rate": 5e-05,
      "loss": 0.0677,
      "step": 1111
    },
    {
      "epoch": 1.3021077283372366,
      "grad_norm": 1.2706377506256104,
      "learning_rate": 5e-05,
      "loss": 0.141,
      "step": 1112
    },
    {
      "epoch": 1.3032786885245902,
      "grad_norm": 1.15388023853302,
      "learning_rate": 5e-05,
      "loss": 0.1027,
      "step": 1113
    },
    {
      "epoch": 1.3044496487119437,
      "grad_norm": 1.0607088804244995,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1114
    },
    {
      "epoch": 1.3056206088992974,
      "grad_norm": 1.1572345495224,
      "learning_rate": 5e-05,
      "loss": 0.1036,
      "step": 1115
    },
    {
      "epoch": 1.3067915690866512,
      "grad_norm": 1.1906712055206299,
      "learning_rate": 5e-05,
      "loss": 0.1209,
      "step": 1116
    },
    {
      "epoch": 1.3079625292740047,
      "grad_norm": 1.5551565885543823,
      "learning_rate": 5e-05,
      "loss": 0.2129,
      "step": 1117
    },
    {
      "epoch": 1.3091334894613582,
      "grad_norm": 1.1599557399749756,
      "learning_rate": 5e-05,
      "loss": 0.1087,
      "step": 1118
    },
    {
      "epoch": 1.310304449648712,
      "grad_norm": 2.3004794120788574,
      "learning_rate": 5e-05,
      "loss": 0.1256,
      "step": 1119
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 1.249252200126648,
      "learning_rate": 5e-05,
      "loss": 0.1097,
      "step": 1120
    },
    {
      "epoch": 1.3126463700234192,
      "grad_norm": 0.6937034130096436,
      "learning_rate": 5e-05,
      "loss": 0.0425,
      "step": 1121
    },
    {
      "epoch": 1.3138173302107727,
      "grad_norm": 0.6494044065475464,
      "learning_rate": 5e-05,
      "loss": 0.0588,
      "step": 1122
    },
    {
      "epoch": 1.3149882903981265,
      "grad_norm": 0.8463778495788574,
      "learning_rate": 5e-05,
      "loss": 0.1024,
      "step": 1123
    },
    {
      "epoch": 1.3161592505854802,
      "grad_norm": 1.6450974941253662,
      "learning_rate": 5e-05,
      "loss": 0.1452,
      "step": 1124
    },
    {
      "epoch": 1.3173302107728337,
      "grad_norm": 0.5608121156692505,
      "learning_rate": 5e-05,
      "loss": 0.0382,
      "step": 1125
    },
    {
      "epoch": 1.3185011709601873,
      "grad_norm": 0.6310554146766663,
      "learning_rate": 5e-05,
      "loss": 0.0429,
      "step": 1126
    },
    {
      "epoch": 1.319672131147541,
      "grad_norm": 0.9234839677810669,
      "learning_rate": 5e-05,
      "loss": 0.1414,
      "step": 1127
    },
    {
      "epoch": 1.3208430913348947,
      "grad_norm": 0.8095186352729797,
      "learning_rate": 5e-05,
      "loss": 0.131,
      "step": 1128
    },
    {
      "epoch": 1.3220140515222483,
      "grad_norm": 0.7917485237121582,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 1129
    },
    {
      "epoch": 1.3231850117096018,
      "grad_norm": 0.9281219244003296,
      "learning_rate": 5e-05,
      "loss": 0.0753,
      "step": 1130
    },
    {
      "epoch": 1.3243559718969555,
      "grad_norm": 0.8592994809150696,
      "learning_rate": 5e-05,
      "loss": 0.1046,
      "step": 1131
    },
    {
      "epoch": 1.325526932084309,
      "grad_norm": 0.47575485706329346,
      "learning_rate": 5e-05,
      "loss": 0.0187,
      "step": 1132
    },
    {
      "epoch": 1.3266978922716628,
      "grad_norm": 1.1895469427108765,
      "learning_rate": 5e-05,
      "loss": 0.0862,
      "step": 1133
    },
    {
      "epoch": 1.3278688524590163,
      "grad_norm": 0.7227762341499329,
      "learning_rate": 5e-05,
      "loss": 0.0508,
      "step": 1134
    },
    {
      "epoch": 1.32903981264637,
      "grad_norm": 0.9591220021247864,
      "learning_rate": 5e-05,
      "loss": 0.114,
      "step": 1135
    },
    {
      "epoch": 1.3302107728337236,
      "grad_norm": 0.6516973376274109,
      "learning_rate": 5e-05,
      "loss": 0.0706,
      "step": 1136
    },
    {
      "epoch": 1.3313817330210773,
      "grad_norm": 0.7495483160018921,
      "learning_rate": 5e-05,
      "loss": 0.0812,
      "step": 1137
    },
    {
      "epoch": 1.3325526932084308,
      "grad_norm": 0.74342280626297,
      "learning_rate": 5e-05,
      "loss": 0.054,
      "step": 1138
    },
    {
      "epoch": 1.3337236533957846,
      "grad_norm": 0.8264617323875427,
      "learning_rate": 5e-05,
      "loss": 0.0902,
      "step": 1139
    },
    {
      "epoch": 1.334894613583138,
      "grad_norm": 1.1616790294647217,
      "learning_rate": 5e-05,
      "loss": 0.2244,
      "step": 1140
    },
    {
      "epoch": 1.3360655737704918,
      "grad_norm": 0.6063963174819946,
      "learning_rate": 5e-05,
      "loss": 0.0526,
      "step": 1141
    },
    {
      "epoch": 1.3372365339578454,
      "grad_norm": 0.6889930367469788,
      "learning_rate": 5e-05,
      "loss": 0.0666,
      "step": 1142
    },
    {
      "epoch": 1.338407494145199,
      "grad_norm": 0.5449569225311279,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1143
    },
    {
      "epoch": 1.3395784543325526,
      "grad_norm": 0.9179254770278931,
      "learning_rate": 5e-05,
      "loss": 0.0659,
      "step": 1144
    },
    {
      "epoch": 1.3407494145199064,
      "grad_norm": 0.7213162779808044,
      "learning_rate": 5e-05,
      "loss": 0.1056,
      "step": 1145
    },
    {
      "epoch": 1.3419203747072599,
      "grad_norm": 0.5305948853492737,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 1146
    },
    {
      "epoch": 1.3430913348946136,
      "grad_norm": 0.8108440637588501,
      "learning_rate": 5e-05,
      "loss": 0.1142,
      "step": 1147
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 0.9546011090278625,
      "learning_rate": 5e-05,
      "loss": 0.1009,
      "step": 1148
    },
    {
      "epoch": 1.345433255269321,
      "grad_norm": 0.9314043521881104,
      "learning_rate": 5e-05,
      "loss": 0.0889,
      "step": 1149
    },
    {
      "epoch": 1.3466042154566744,
      "grad_norm": 0.6161046028137207,
      "learning_rate": 5e-05,
      "loss": 0.0544,
      "step": 1150
    },
    {
      "epoch": 1.3477751756440282,
      "grad_norm": 1.097762942314148,
      "learning_rate": 5e-05,
      "loss": 0.1507,
      "step": 1151
    },
    {
      "epoch": 1.3489461358313817,
      "grad_norm": 0.9297349452972412,
      "learning_rate": 5e-05,
      "loss": 0.0761,
      "step": 1152
    },
    {
      "epoch": 1.3501170960187354,
      "grad_norm": 1.3348497152328491,
      "learning_rate": 5e-05,
      "loss": 0.1167,
      "step": 1153
    },
    {
      "epoch": 1.351288056206089,
      "grad_norm": 1.036624550819397,
      "learning_rate": 5e-05,
      "loss": 0.1106,
      "step": 1154
    },
    {
      "epoch": 1.3524590163934427,
      "grad_norm": 1.0212881565093994,
      "learning_rate": 5e-05,
      "loss": 0.1486,
      "step": 1155
    },
    {
      "epoch": 1.3536299765807962,
      "grad_norm": 0.9089561104774475,
      "learning_rate": 5e-05,
      "loss": 0.1701,
      "step": 1156
    },
    {
      "epoch": 1.35480093676815,
      "grad_norm": 1.0555942058563232,
      "learning_rate": 5e-05,
      "loss": 0.1707,
      "step": 1157
    },
    {
      "epoch": 1.3559718969555035,
      "grad_norm": 1.933549165725708,
      "learning_rate": 5e-05,
      "loss": 0.0974,
      "step": 1158
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 0.6937889456748962,
      "learning_rate": 5e-05,
      "loss": 0.0982,
      "step": 1159
    },
    {
      "epoch": 1.3583138173302107,
      "grad_norm": 1.1503028869628906,
      "learning_rate": 5e-05,
      "loss": 0.2048,
      "step": 1160
    },
    {
      "epoch": 1.3594847775175645,
      "grad_norm": 1.0827654600143433,
      "learning_rate": 5e-05,
      "loss": 0.0731,
      "step": 1161
    },
    {
      "epoch": 1.360655737704918,
      "grad_norm": 0.7169076800346375,
      "learning_rate": 5e-05,
      "loss": 0.0591,
      "step": 1162
    },
    {
      "epoch": 1.3618266978922717,
      "grad_norm": 0.6836904883384705,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 1163
    },
    {
      "epoch": 1.3629976580796253,
      "grad_norm": 1.249148964881897,
      "learning_rate": 5e-05,
      "loss": 0.1215,
      "step": 1164
    },
    {
      "epoch": 1.364168618266979,
      "grad_norm": 0.5864876508712769,
      "learning_rate": 5e-05,
      "loss": 0.0687,
      "step": 1165
    },
    {
      "epoch": 1.3653395784543325,
      "grad_norm": 1.237378478050232,
      "learning_rate": 5e-05,
      "loss": 0.0738,
      "step": 1166
    },
    {
      "epoch": 1.3665105386416863,
      "grad_norm": 1.0258926153182983,
      "learning_rate": 5e-05,
      "loss": 0.0963,
      "step": 1167
    },
    {
      "epoch": 1.3676814988290398,
      "grad_norm": 1.6903479099273682,
      "learning_rate": 5e-05,
      "loss": 0.1884,
      "step": 1168
    },
    {
      "epoch": 1.3688524590163935,
      "grad_norm": 0.8404015898704529,
      "learning_rate": 5e-05,
      "loss": 0.1259,
      "step": 1169
    },
    {
      "epoch": 1.370023419203747,
      "grad_norm": 0.7215494513511658,
      "learning_rate": 5e-05,
      "loss": 0.1112,
      "step": 1170
    },
    {
      "epoch": 1.3711943793911008,
      "grad_norm": 0.8114854097366333,
      "learning_rate": 5e-05,
      "loss": 0.0937,
      "step": 1171
    },
    {
      "epoch": 1.3723653395784543,
      "grad_norm": 0.695408284664154,
      "learning_rate": 5e-05,
      "loss": 0.0574,
      "step": 1172
    },
    {
      "epoch": 1.373536299765808,
      "grad_norm": 0.7164169549942017,
      "learning_rate": 5e-05,
      "loss": 0.0806,
      "step": 1173
    },
    {
      "epoch": 1.3747072599531616,
      "grad_norm": 0.6425553560256958,
      "learning_rate": 5e-05,
      "loss": 0.0927,
      "step": 1174
    },
    {
      "epoch": 1.3758782201405153,
      "grad_norm": 1.0413872003555298,
      "learning_rate": 5e-05,
      "loss": 0.0982,
      "step": 1175
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 0.7777528762817383,
      "learning_rate": 5e-05,
      "loss": 0.189,
      "step": 1176
    },
    {
      "epoch": 1.3782201405152223,
      "grad_norm": 0.9344272017478943,
      "learning_rate": 5e-05,
      "loss": 0.0884,
      "step": 1177
    },
    {
      "epoch": 1.379391100702576,
      "grad_norm": 1.0602842569351196,
      "learning_rate": 5e-05,
      "loss": 0.115,
      "step": 1178
    },
    {
      "epoch": 1.3805620608899298,
      "grad_norm": 0.7103217244148254,
      "learning_rate": 5e-05,
      "loss": 0.0839,
      "step": 1179
    },
    {
      "epoch": 1.3817330210772834,
      "grad_norm": 1.2307782173156738,
      "learning_rate": 5e-05,
      "loss": 0.0725,
      "step": 1180
    },
    {
      "epoch": 1.3829039812646369,
      "grad_norm": 0.6951695680618286,
      "learning_rate": 5e-05,
      "loss": 0.0713,
      "step": 1181
    },
    {
      "epoch": 1.3840749414519906,
      "grad_norm": 0.7367772459983826,
      "learning_rate": 5e-05,
      "loss": 0.053,
      "step": 1182
    },
    {
      "epoch": 1.3852459016393444,
      "grad_norm": 0.5631988644599915,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1183
    },
    {
      "epoch": 1.3864168618266979,
      "grad_norm": 0.8141743540763855,
      "learning_rate": 5e-05,
      "loss": 0.0755,
      "step": 1184
    },
    {
      "epoch": 1.3875878220140514,
      "grad_norm": 0.9064000248908997,
      "learning_rate": 5e-05,
      "loss": 0.119,
      "step": 1185
    },
    {
      "epoch": 1.3887587822014051,
      "grad_norm": 0.9751806855201721,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 1186
    },
    {
      "epoch": 1.3899297423887589,
      "grad_norm": 1.0075085163116455,
      "learning_rate": 5e-05,
      "loss": 0.0943,
      "step": 1187
    },
    {
      "epoch": 1.3911007025761124,
      "grad_norm": 0.4691350758075714,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 1188
    },
    {
      "epoch": 1.392271662763466,
      "grad_norm": 0.7213039398193359,
      "learning_rate": 5e-05,
      "loss": 0.0938,
      "step": 1189
    },
    {
      "epoch": 1.3934426229508197,
      "grad_norm": 0.9512443542480469,
      "learning_rate": 5e-05,
      "loss": 0.0748,
      "step": 1190
    },
    {
      "epoch": 1.3946135831381734,
      "grad_norm": 0.7228028774261475,
      "learning_rate": 5e-05,
      "loss": 0.0511,
      "step": 1191
    },
    {
      "epoch": 1.395784543325527,
      "grad_norm": 1.303415060043335,
      "learning_rate": 5e-05,
      "loss": 0.0539,
      "step": 1192
    },
    {
      "epoch": 1.3969555035128804,
      "grad_norm": 1.4253019094467163,
      "learning_rate": 5e-05,
      "loss": 0.1338,
      "step": 1193
    },
    {
      "epoch": 1.3981264637002342,
      "grad_norm": 0.9842239618301392,
      "learning_rate": 5e-05,
      "loss": 0.1854,
      "step": 1194
    },
    {
      "epoch": 1.399297423887588,
      "grad_norm": 1.2086031436920166,
      "learning_rate": 5e-05,
      "loss": 0.0602,
      "step": 1195
    },
    {
      "epoch": 1.4004683840749415,
      "grad_norm": 1.5514246225357056,
      "learning_rate": 5e-05,
      "loss": 0.0651,
      "step": 1196
    },
    {
      "epoch": 1.401639344262295,
      "grad_norm": 0.787655234336853,
      "learning_rate": 5e-05,
      "loss": 0.0499,
      "step": 1197
    },
    {
      "epoch": 1.4028103044496487,
      "grad_norm": 0.7303980588912964,
      "learning_rate": 5e-05,
      "loss": 0.0929,
      "step": 1198
    },
    {
      "epoch": 1.4039812646370025,
      "grad_norm": 0.8825626969337463,
      "learning_rate": 5e-05,
      "loss": 0.0953,
      "step": 1199
    },
    {
      "epoch": 1.405152224824356,
      "grad_norm": 0.8994195461273193,
      "learning_rate": 5e-05,
      "loss": 0.0925,
      "step": 1200
    },
    {
      "epoch": 1.4063231850117095,
      "grad_norm": 1.0853664875030518,
      "learning_rate": 5e-05,
      "loss": 0.0885,
      "step": 1201
    },
    {
      "epoch": 1.4074941451990632,
      "grad_norm": 1.021958827972412,
      "learning_rate": 5e-05,
      "loss": 0.1436,
      "step": 1202
    },
    {
      "epoch": 1.408665105386417,
      "grad_norm": 1.6475460529327393,
      "learning_rate": 5e-05,
      "loss": 0.5732,
      "step": 1203
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 0.8107694983482361,
      "learning_rate": 5e-05,
      "loss": 0.0707,
      "step": 1204
    },
    {
      "epoch": 1.411007025761124,
      "grad_norm": 1.137439250946045,
      "learning_rate": 5e-05,
      "loss": 0.1402,
      "step": 1205
    },
    {
      "epoch": 1.4121779859484778,
      "grad_norm": 1.6414402723312378,
      "learning_rate": 5e-05,
      "loss": 0.1281,
      "step": 1206
    },
    {
      "epoch": 1.4133489461358315,
      "grad_norm": 0.9428650736808777,
      "learning_rate": 5e-05,
      "loss": 0.0704,
      "step": 1207
    },
    {
      "epoch": 1.414519906323185,
      "grad_norm": 0.7394885420799255,
      "learning_rate": 5e-05,
      "loss": 0.0899,
      "step": 1208
    },
    {
      "epoch": 1.4156908665105385,
      "grad_norm": 0.8204353451728821,
      "learning_rate": 5e-05,
      "loss": 0.1417,
      "step": 1209
    },
    {
      "epoch": 1.4168618266978923,
      "grad_norm": 1.2561571598052979,
      "learning_rate": 5e-05,
      "loss": 0.0932,
      "step": 1210
    },
    {
      "epoch": 1.418032786885246,
      "grad_norm": 1.219125747680664,
      "learning_rate": 5e-05,
      "loss": 0.0885,
      "step": 1211
    },
    {
      "epoch": 1.4192037470725996,
      "grad_norm": 1.106386423110962,
      "learning_rate": 5e-05,
      "loss": 0.1532,
      "step": 1212
    },
    {
      "epoch": 1.420374707259953,
      "grad_norm": 1.0888917446136475,
      "learning_rate": 5e-05,
      "loss": 0.0692,
      "step": 1213
    },
    {
      "epoch": 1.4215456674473068,
      "grad_norm": 1.273828148841858,
      "learning_rate": 5e-05,
      "loss": 0.1171,
      "step": 1214
    },
    {
      "epoch": 1.4227166276346606,
      "grad_norm": 0.6668046116828918,
      "learning_rate": 5e-05,
      "loss": 0.0786,
      "step": 1215
    },
    {
      "epoch": 1.423887587822014,
      "grad_norm": 1.428311824798584,
      "learning_rate": 5e-05,
      "loss": 0.2117,
      "step": 1216
    },
    {
      "epoch": 1.4250585480093676,
      "grad_norm": 0.5551325082778931,
      "learning_rate": 5e-05,
      "loss": 0.0463,
      "step": 1217
    },
    {
      "epoch": 1.4262295081967213,
      "grad_norm": 0.9876304864883423,
      "learning_rate": 5e-05,
      "loss": 0.1888,
      "step": 1218
    },
    {
      "epoch": 1.4274004683840749,
      "grad_norm": 0.9083410501480103,
      "learning_rate": 5e-05,
      "loss": 0.1474,
      "step": 1219
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.2471592426300049,
      "learning_rate": 5e-05,
      "loss": 0.1249,
      "step": 1220
    },
    {
      "epoch": 1.4297423887587821,
      "grad_norm": 0.9153249859809875,
      "learning_rate": 5e-05,
      "loss": 0.2487,
      "step": 1221
    },
    {
      "epoch": 1.4309133489461359,
      "grad_norm": 1.0094572305679321,
      "learning_rate": 5e-05,
      "loss": 0.0766,
      "step": 1222
    },
    {
      "epoch": 1.4320843091334894,
      "grad_norm": 0.8580992221832275,
      "learning_rate": 5e-05,
      "loss": 0.0896,
      "step": 1223
    },
    {
      "epoch": 1.4332552693208431,
      "grad_norm": 0.9593281149864197,
      "learning_rate": 5e-05,
      "loss": 0.1049,
      "step": 1224
    },
    {
      "epoch": 1.4344262295081966,
      "grad_norm": 1.1150237321853638,
      "learning_rate": 5e-05,
      "loss": 0.1151,
      "step": 1225
    },
    {
      "epoch": 1.4355971896955504,
      "grad_norm": 1.123017430305481,
      "learning_rate": 5e-05,
      "loss": 0.0537,
      "step": 1226
    },
    {
      "epoch": 1.436768149882904,
      "grad_norm": 0.7691742777824402,
      "learning_rate": 5e-05,
      "loss": 0.0725,
      "step": 1227
    },
    {
      "epoch": 1.4379391100702577,
      "grad_norm": 0.5195634365081787,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1228
    },
    {
      "epoch": 1.4391100702576112,
      "grad_norm": 1.4802324771881104,
      "learning_rate": 5e-05,
      "loss": 0.1133,
      "step": 1229
    },
    {
      "epoch": 1.440281030444965,
      "grad_norm": 0.5516698956489563,
      "learning_rate": 5e-05,
      "loss": 0.0598,
      "step": 1230
    },
    {
      "epoch": 1.4414519906323184,
      "grad_norm": 0.7685068845748901,
      "learning_rate": 5e-05,
      "loss": 0.0471,
      "step": 1231
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 0.6336219310760498,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 1232
    },
    {
      "epoch": 1.4437939110070257,
      "grad_norm": 1.354211449623108,
      "learning_rate": 5e-05,
      "loss": 0.1002,
      "step": 1233
    },
    {
      "epoch": 1.4449648711943794,
      "grad_norm": 1.1439435482025146,
      "learning_rate": 5e-05,
      "loss": 0.0679,
      "step": 1234
    },
    {
      "epoch": 1.446135831381733,
      "grad_norm": 3.221923351287842,
      "learning_rate": 5e-05,
      "loss": 0.1348,
      "step": 1235
    },
    {
      "epoch": 1.4473067915690867,
      "grad_norm": 1.0500472784042358,
      "learning_rate": 5e-05,
      "loss": 0.1277,
      "step": 1236
    },
    {
      "epoch": 1.4484777517564402,
      "grad_norm": 2.7795956134796143,
      "learning_rate": 5e-05,
      "loss": 0.1548,
      "step": 1237
    },
    {
      "epoch": 1.449648711943794,
      "grad_norm": 1.1130776405334473,
      "learning_rate": 5e-05,
      "loss": 0.1095,
      "step": 1238
    },
    {
      "epoch": 1.4508196721311475,
      "grad_norm": 0.9772530794143677,
      "learning_rate": 5e-05,
      "loss": 0.1173,
      "step": 1239
    },
    {
      "epoch": 1.4519906323185012,
      "grad_norm": 0.9451904296875,
      "learning_rate": 5e-05,
      "loss": 0.0715,
      "step": 1240
    },
    {
      "epoch": 1.4531615925058547,
      "grad_norm": 0.469209760427475,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 1241
    },
    {
      "epoch": 1.4543325526932085,
      "grad_norm": 1.093381643295288,
      "learning_rate": 5e-05,
      "loss": 0.0447,
      "step": 1242
    },
    {
      "epoch": 1.455503512880562,
      "grad_norm": 0.8711250424385071,
      "learning_rate": 5e-05,
      "loss": 0.0722,
      "step": 1243
    },
    {
      "epoch": 1.4566744730679158,
      "grad_norm": 0.7474488615989685,
      "learning_rate": 5e-05,
      "loss": 0.0608,
      "step": 1244
    },
    {
      "epoch": 1.4578454332552693,
      "grad_norm": 0.9453068971633911,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1245
    },
    {
      "epoch": 1.459016393442623,
      "grad_norm": 0.9480428099632263,
      "learning_rate": 5e-05,
      "loss": 0.0787,
      "step": 1246
    },
    {
      "epoch": 1.4601873536299765,
      "grad_norm": 1.6020517349243164,
      "learning_rate": 5e-05,
      "loss": 0.0822,
      "step": 1247
    },
    {
      "epoch": 1.4613583138173303,
      "grad_norm": 1.024552583694458,
      "learning_rate": 5e-05,
      "loss": 0.045,
      "step": 1248
    },
    {
      "epoch": 1.4625292740046838,
      "grad_norm": 1.2105180025100708,
      "learning_rate": 5e-05,
      "loss": 0.1159,
      "step": 1249
    },
    {
      "epoch": 1.4637002341920375,
      "grad_norm": 0.6307325959205627,
      "learning_rate": 5e-05,
      "loss": 0.1055,
      "step": 1250
    },
    {
      "epoch": 1.464871194379391,
      "grad_norm": 1.224774718284607,
      "learning_rate": 5e-05,
      "loss": 0.1548,
      "step": 1251
    },
    {
      "epoch": 1.4660421545667448,
      "grad_norm": 1.1330299377441406,
      "learning_rate": 5e-05,
      "loss": 0.1817,
      "step": 1252
    },
    {
      "epoch": 1.4672131147540983,
      "grad_norm": 0.8392689824104309,
      "learning_rate": 5e-05,
      "loss": 0.105,
      "step": 1253
    },
    {
      "epoch": 1.468384074941452,
      "grad_norm": 2.097712993621826,
      "learning_rate": 5e-05,
      "loss": 0.1236,
      "step": 1254
    },
    {
      "epoch": 1.4695550351288056,
      "grad_norm": 1.6929594278335571,
      "learning_rate": 5e-05,
      "loss": 0.0836,
      "step": 1255
    },
    {
      "epoch": 1.4707259953161593,
      "grad_norm": 0.7818142771720886,
      "learning_rate": 5e-05,
      "loss": 0.1093,
      "step": 1256
    },
    {
      "epoch": 1.4718969555035128,
      "grad_norm": 1.8798551559448242,
      "learning_rate": 5e-05,
      "loss": 0.1369,
      "step": 1257
    },
    {
      "epoch": 1.4730679156908666,
      "grad_norm": 1.4170260429382324,
      "learning_rate": 5e-05,
      "loss": 0.0596,
      "step": 1258
    },
    {
      "epoch": 1.4742388758782201,
      "grad_norm": 0.8070353269577026,
      "learning_rate": 5e-05,
      "loss": 0.0463,
      "step": 1259
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 0.8597272038459778,
      "learning_rate": 5e-05,
      "loss": 0.064,
      "step": 1260
    },
    {
      "epoch": 1.4765807962529274,
      "grad_norm": 0.9548535346984863,
      "learning_rate": 5e-05,
      "loss": 0.0481,
      "step": 1261
    },
    {
      "epoch": 1.4777517564402811,
      "grad_norm": 1.1702731847763062,
      "learning_rate": 5e-05,
      "loss": 0.1161,
      "step": 1262
    },
    {
      "epoch": 1.4789227166276346,
      "grad_norm": 0.7371427416801453,
      "learning_rate": 5e-05,
      "loss": 0.0486,
      "step": 1263
    },
    {
      "epoch": 1.4800936768149882,
      "grad_norm": 1.6355818510055542,
      "learning_rate": 5e-05,
      "loss": 0.1672,
      "step": 1264
    },
    {
      "epoch": 1.481264637002342,
      "grad_norm": 1.0489213466644287,
      "learning_rate": 5e-05,
      "loss": 0.1249,
      "step": 1265
    },
    {
      "epoch": 1.4824355971896956,
      "grad_norm": 1.1197336912155151,
      "learning_rate": 5e-05,
      "loss": 0.0793,
      "step": 1266
    },
    {
      "epoch": 1.4836065573770492,
      "grad_norm": 1.0806546211242676,
      "learning_rate": 5e-05,
      "loss": 0.095,
      "step": 1267
    },
    {
      "epoch": 1.4847775175644027,
      "grad_norm": 1.1046438217163086,
      "learning_rate": 5e-05,
      "loss": 0.0748,
      "step": 1268
    },
    {
      "epoch": 1.4859484777517564,
      "grad_norm": 1.0858399868011475,
      "learning_rate": 5e-05,
      "loss": 0.1139,
      "step": 1269
    },
    {
      "epoch": 1.4871194379391102,
      "grad_norm": 0.8213140964508057,
      "learning_rate": 5e-05,
      "loss": 0.1118,
      "step": 1270
    },
    {
      "epoch": 1.4882903981264637,
      "grad_norm": 1.2642018795013428,
      "learning_rate": 5e-05,
      "loss": 0.1181,
      "step": 1271
    },
    {
      "epoch": 1.4894613583138172,
      "grad_norm": 0.8721972107887268,
      "learning_rate": 5e-05,
      "loss": 0.0753,
      "step": 1272
    },
    {
      "epoch": 1.490632318501171,
      "grad_norm": 1.3899743556976318,
      "learning_rate": 5e-05,
      "loss": 0.1218,
      "step": 1273
    },
    {
      "epoch": 1.4918032786885247,
      "grad_norm": 0.7995927929878235,
      "learning_rate": 5e-05,
      "loss": 0.1181,
      "step": 1274
    },
    {
      "epoch": 1.4929742388758782,
      "grad_norm": 0.6714556217193604,
      "learning_rate": 5e-05,
      "loss": 0.0511,
      "step": 1275
    },
    {
      "epoch": 1.4941451990632317,
      "grad_norm": 1.0646604299545288,
      "learning_rate": 5e-05,
      "loss": 0.1514,
      "step": 1276
    },
    {
      "epoch": 1.4953161592505855,
      "grad_norm": 2.0643529891967773,
      "learning_rate": 5e-05,
      "loss": 0.1297,
      "step": 1277
    },
    {
      "epoch": 1.4964871194379392,
      "grad_norm": 0.5969094038009644,
      "learning_rate": 5e-05,
      "loss": 0.1204,
      "step": 1278
    },
    {
      "epoch": 1.4976580796252927,
      "grad_norm": 1.6536610126495361,
      "learning_rate": 5e-05,
      "loss": 0.0924,
      "step": 1279
    },
    {
      "epoch": 1.4988290398126463,
      "grad_norm": 0.7961277365684509,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 1280
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.0000004768371582,
      "learning_rate": 5e-05,
      "loss": 0.0641,
      "step": 1281
    },
    {
      "epoch": 1.5011709601873537,
      "grad_norm": 0.8288332223892212,
      "learning_rate": 5e-05,
      "loss": 0.0699,
      "step": 1282
    },
    {
      "epoch": 1.5023419203747073,
      "grad_norm": 0.6923251152038574,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 1283
    },
    {
      "epoch": 1.5035128805620608,
      "grad_norm": 1.3352218866348267,
      "learning_rate": 5e-05,
      "loss": 0.2849,
      "step": 1284
    },
    {
      "epoch": 1.5046838407494145,
      "grad_norm": 1.4869319200515747,
      "learning_rate": 5e-05,
      "loss": 0.1042,
      "step": 1285
    },
    {
      "epoch": 1.5058548009367683,
      "grad_norm": 1.3370386362075806,
      "learning_rate": 5e-05,
      "loss": 0.3092,
      "step": 1286
    },
    {
      "epoch": 1.5070257611241218,
      "grad_norm": 0.6747315526008606,
      "learning_rate": 5e-05,
      "loss": 0.0712,
      "step": 1287
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 1.43172025680542,
      "learning_rate": 5e-05,
      "loss": 0.3925,
      "step": 1288
    },
    {
      "epoch": 1.509367681498829,
      "grad_norm": 1.4730877876281738,
      "learning_rate": 5e-05,
      "loss": 0.0819,
      "step": 1289
    },
    {
      "epoch": 1.5105386416861828,
      "grad_norm": 1.1659717559814453,
      "learning_rate": 5e-05,
      "loss": 0.0904,
      "step": 1290
    },
    {
      "epoch": 1.5117096018735363,
      "grad_norm": 1.0692061185836792,
      "learning_rate": 5e-05,
      "loss": 0.1849,
      "step": 1291
    },
    {
      "epoch": 1.5128805620608898,
      "grad_norm": 1.5623555183410645,
      "learning_rate": 5e-05,
      "loss": 0.1418,
      "step": 1292
    },
    {
      "epoch": 1.5140515222482436,
      "grad_norm": 0.680046796798706,
      "learning_rate": 5e-05,
      "loss": 0.0456,
      "step": 1293
    },
    {
      "epoch": 1.5152224824355973,
      "grad_norm": 0.36371108889579773,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 1294
    },
    {
      "epoch": 1.5163934426229508,
      "grad_norm": 1.0286616086959839,
      "learning_rate": 5e-05,
      "loss": 0.1121,
      "step": 1295
    },
    {
      "epoch": 1.5175644028103044,
      "grad_norm": 1.0089164972305298,
      "learning_rate": 5e-05,
      "loss": 0.1972,
      "step": 1296
    },
    {
      "epoch": 1.518735362997658,
      "grad_norm": 1.2714223861694336,
      "learning_rate": 5e-05,
      "loss": 0.0698,
      "step": 1297
    },
    {
      "epoch": 1.5199063231850118,
      "grad_norm": 0.8165070414543152,
      "learning_rate": 5e-05,
      "loss": 0.0931,
      "step": 1298
    },
    {
      "epoch": 1.5210772833723654,
      "grad_norm": 0.8070008754730225,
      "learning_rate": 5e-05,
      "loss": 0.0858,
      "step": 1299
    },
    {
      "epoch": 1.5222482435597189,
      "grad_norm": 0.6983533501625061,
      "learning_rate": 5e-05,
      "loss": 0.0595,
      "step": 1300
    },
    {
      "epoch": 1.5234192037470726,
      "grad_norm": 0.4742627441883087,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 1301
    },
    {
      "epoch": 1.5245901639344264,
      "grad_norm": 0.5880192518234253,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 1302
    },
    {
      "epoch": 1.5257611241217799,
      "grad_norm": 0.9491169452667236,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 1303
    },
    {
      "epoch": 1.5269320843091334,
      "grad_norm": 1.0715985298156738,
      "learning_rate": 5e-05,
      "loss": 0.1298,
      "step": 1304
    },
    {
      "epoch": 1.5281030444964872,
      "grad_norm": 1.0264296531677246,
      "learning_rate": 5e-05,
      "loss": 0.2107,
      "step": 1305
    },
    {
      "epoch": 1.529274004683841,
      "grad_norm": 0.48325324058532715,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 1306
    },
    {
      "epoch": 1.5304449648711944,
      "grad_norm": 0.9399946928024292,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 1307
    },
    {
      "epoch": 1.531615925058548,
      "grad_norm": 1.1400262117385864,
      "learning_rate": 5e-05,
      "loss": 0.1076,
      "step": 1308
    },
    {
      "epoch": 1.5327868852459017,
      "grad_norm": 0.9202368259429932,
      "learning_rate": 5e-05,
      "loss": 0.1169,
      "step": 1309
    },
    {
      "epoch": 1.5339578454332554,
      "grad_norm": 1.076656460762024,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 1310
    },
    {
      "epoch": 1.535128805620609,
      "grad_norm": 0.8636196851730347,
      "learning_rate": 5e-05,
      "loss": 0.1079,
      "step": 1311
    },
    {
      "epoch": 1.5362997658079625,
      "grad_norm": 1.0879871845245361,
      "learning_rate": 5e-05,
      "loss": 0.0793,
      "step": 1312
    },
    {
      "epoch": 1.5374707259953162,
      "grad_norm": 0.9489806294441223,
      "learning_rate": 5e-05,
      "loss": 0.0879,
      "step": 1313
    },
    {
      "epoch": 1.53864168618267,
      "grad_norm": 0.6772197484970093,
      "learning_rate": 5e-05,
      "loss": 0.1215,
      "step": 1314
    },
    {
      "epoch": 1.5398126463700235,
      "grad_norm": 1.421826958656311,
      "learning_rate": 5e-05,
      "loss": 0.0882,
      "step": 1315
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 0.9413748383522034,
      "learning_rate": 5e-05,
      "loss": 0.0896,
      "step": 1316
    },
    {
      "epoch": 1.5421545667447307,
      "grad_norm": 1.6448874473571777,
      "learning_rate": 5e-05,
      "loss": 0.225,
      "step": 1317
    },
    {
      "epoch": 1.5433255269320845,
      "grad_norm": 0.5933279395103455,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1318
    },
    {
      "epoch": 1.544496487119438,
      "grad_norm": 0.8172178268432617,
      "learning_rate": 5e-05,
      "loss": 0.1067,
      "step": 1319
    },
    {
      "epoch": 1.5456674473067915,
      "grad_norm": 1.029373288154602,
      "learning_rate": 5e-05,
      "loss": 0.0648,
      "step": 1320
    },
    {
      "epoch": 1.5468384074941453,
      "grad_norm": 1.21181321144104,
      "learning_rate": 5e-05,
      "loss": 0.0964,
      "step": 1321
    },
    {
      "epoch": 1.548009367681499,
      "grad_norm": 1.0269744396209717,
      "learning_rate": 5e-05,
      "loss": 0.0743,
      "step": 1322
    },
    {
      "epoch": 1.5491803278688525,
      "grad_norm": 1.3950375318527222,
      "learning_rate": 5e-05,
      "loss": 0.1514,
      "step": 1323
    },
    {
      "epoch": 1.550351288056206,
      "grad_norm": 1.1027674674987793,
      "learning_rate": 5e-05,
      "loss": 0.0933,
      "step": 1324
    },
    {
      "epoch": 1.5515222482435598,
      "grad_norm": 1.2953592538833618,
      "learning_rate": 5e-05,
      "loss": 0.2409,
      "step": 1325
    },
    {
      "epoch": 1.5526932084309133,
      "grad_norm": 0.9309158325195312,
      "learning_rate": 5e-05,
      "loss": 0.0598,
      "step": 1326
    },
    {
      "epoch": 1.5538641686182668,
      "grad_norm": 1.2408771514892578,
      "learning_rate": 5e-05,
      "loss": 0.0483,
      "step": 1327
    },
    {
      "epoch": 1.5550351288056206,
      "grad_norm": 0.887667179107666,
      "learning_rate": 5e-05,
      "loss": 0.0529,
      "step": 1328
    },
    {
      "epoch": 1.5562060889929743,
      "grad_norm": 0.9596744179725647,
      "learning_rate": 5e-05,
      "loss": 0.1293,
      "step": 1329
    },
    {
      "epoch": 1.5573770491803278,
      "grad_norm": 0.8362852334976196,
      "learning_rate": 5e-05,
      "loss": 0.0618,
      "step": 1330
    },
    {
      "epoch": 1.5585480093676813,
      "grad_norm": 0.5871784090995789,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 1331
    },
    {
      "epoch": 1.559718969555035,
      "grad_norm": 1.0721107721328735,
      "learning_rate": 5e-05,
      "loss": 0.1649,
      "step": 1332
    },
    {
      "epoch": 1.5608899297423888,
      "grad_norm": 1.381234049797058,
      "learning_rate": 5e-05,
      "loss": 0.0923,
      "step": 1333
    },
    {
      "epoch": 1.5620608899297423,
      "grad_norm": 1.4765558242797852,
      "learning_rate": 5e-05,
      "loss": 0.2355,
      "step": 1334
    },
    {
      "epoch": 1.5632318501170959,
      "grad_norm": 0.9957626461982727,
      "learning_rate": 5e-05,
      "loss": 0.1071,
      "step": 1335
    },
    {
      "epoch": 1.5644028103044496,
      "grad_norm": 1.3720457553863525,
      "learning_rate": 5e-05,
      "loss": 0.1025,
      "step": 1336
    },
    {
      "epoch": 1.5655737704918034,
      "grad_norm": 0.6653452515602112,
      "learning_rate": 5e-05,
      "loss": 0.0644,
      "step": 1337
    },
    {
      "epoch": 1.5667447306791569,
      "grad_norm": 1.0647895336151123,
      "learning_rate": 5e-05,
      "loss": 0.0786,
      "step": 1338
    },
    {
      "epoch": 1.5679156908665104,
      "grad_norm": 1.4273662567138672,
      "learning_rate": 5e-05,
      "loss": 0.1197,
      "step": 1339
    },
    {
      "epoch": 1.5690866510538641,
      "grad_norm": 1.0475122928619385,
      "learning_rate": 5e-05,
      "loss": 0.0842,
      "step": 1340
    },
    {
      "epoch": 1.5702576112412179,
      "grad_norm": 0.765363335609436,
      "learning_rate": 5e-05,
      "loss": 0.0633,
      "step": 1341
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.7229285836219788,
      "learning_rate": 5e-05,
      "loss": 0.089,
      "step": 1342
    },
    {
      "epoch": 1.572599531615925,
      "grad_norm": 1.0834089517593384,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 1343
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 0.9300017952919006,
      "learning_rate": 5e-05,
      "loss": 0.0788,
      "step": 1344
    },
    {
      "epoch": 1.5749414519906324,
      "grad_norm": 1.146364688873291,
      "learning_rate": 5e-05,
      "loss": 0.2314,
      "step": 1345
    },
    {
      "epoch": 1.576112412177986,
      "grad_norm": 0.9030820727348328,
      "learning_rate": 5e-05,
      "loss": 0.0644,
      "step": 1346
    },
    {
      "epoch": 1.5772833723653394,
      "grad_norm": 1.647379994392395,
      "learning_rate": 5e-05,
      "loss": 0.1746,
      "step": 1347
    },
    {
      "epoch": 1.5784543325526932,
      "grad_norm": 1.312289834022522,
      "learning_rate": 5e-05,
      "loss": 0.0613,
      "step": 1348
    },
    {
      "epoch": 1.579625292740047,
      "grad_norm": 1.534754753112793,
      "learning_rate": 5e-05,
      "loss": 0.1777,
      "step": 1349
    },
    {
      "epoch": 1.5807962529274004,
      "grad_norm": 0.74978107213974,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 1350
    },
    {
      "epoch": 1.581967213114754,
      "grad_norm": 1.3308589458465576,
      "learning_rate": 5e-05,
      "loss": 0.1994,
      "step": 1351
    },
    {
      "epoch": 1.5831381733021077,
      "grad_norm": 1.5101338624954224,
      "learning_rate": 5e-05,
      "loss": 0.0995,
      "step": 1352
    },
    {
      "epoch": 1.5843091334894615,
      "grad_norm": 1.0726021528244019,
      "learning_rate": 5e-05,
      "loss": 0.0667,
      "step": 1353
    },
    {
      "epoch": 1.585480093676815,
      "grad_norm": 1.0196508169174194,
      "learning_rate": 5e-05,
      "loss": 0.1604,
      "step": 1354
    },
    {
      "epoch": 1.5866510538641685,
      "grad_norm": 1.0657320022583008,
      "learning_rate": 5e-05,
      "loss": 0.0838,
      "step": 1355
    },
    {
      "epoch": 1.5878220140515222,
      "grad_norm": 0.9751629829406738,
      "learning_rate": 5e-05,
      "loss": 0.1687,
      "step": 1356
    },
    {
      "epoch": 1.588992974238876,
      "grad_norm": 0.8637505769729614,
      "learning_rate": 5e-05,
      "loss": 0.1547,
      "step": 1357
    },
    {
      "epoch": 1.5901639344262295,
      "grad_norm": 0.7198772430419922,
      "learning_rate": 5e-05,
      "loss": 0.0522,
      "step": 1358
    },
    {
      "epoch": 1.591334894613583,
      "grad_norm": 0.8978272080421448,
      "learning_rate": 5e-05,
      "loss": 0.0638,
      "step": 1359
    },
    {
      "epoch": 1.5925058548009368,
      "grad_norm": 0.8135743737220764,
      "learning_rate": 5e-05,
      "loss": 0.0816,
      "step": 1360
    },
    {
      "epoch": 1.5936768149882905,
      "grad_norm": 0.6771301031112671,
      "learning_rate": 5e-05,
      "loss": 0.0742,
      "step": 1361
    },
    {
      "epoch": 1.594847775175644,
      "grad_norm": 0.5416641235351562,
      "learning_rate": 5e-05,
      "loss": 0.0454,
      "step": 1362
    },
    {
      "epoch": 1.5960187353629975,
      "grad_norm": 1.4737331867218018,
      "learning_rate": 5e-05,
      "loss": 0.153,
      "step": 1363
    },
    {
      "epoch": 1.5971896955503513,
      "grad_norm": 0.5982581973075867,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1364
    },
    {
      "epoch": 1.598360655737705,
      "grad_norm": 0.5513244867324829,
      "learning_rate": 5e-05,
      "loss": 0.051,
      "step": 1365
    },
    {
      "epoch": 1.5995316159250585,
      "grad_norm": 0.9936093091964722,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 1366
    },
    {
      "epoch": 1.600702576112412,
      "grad_norm": 0.6162351369857788,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 1367
    },
    {
      "epoch": 1.6018735362997658,
      "grad_norm": 1.4601308107376099,
      "learning_rate": 5e-05,
      "loss": 0.1977,
      "step": 1368
    },
    {
      "epoch": 1.6030444964871196,
      "grad_norm": 0.9227622151374817,
      "learning_rate": 5e-05,
      "loss": 0.0497,
      "step": 1369
    },
    {
      "epoch": 1.604215456674473,
      "grad_norm": 1.8855139017105103,
      "learning_rate": 5e-05,
      "loss": 0.0682,
      "step": 1370
    },
    {
      "epoch": 1.6053864168618266,
      "grad_norm": 1.007501482963562,
      "learning_rate": 5e-05,
      "loss": 0.0865,
      "step": 1371
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 0.9542959928512573,
      "learning_rate": 5e-05,
      "loss": 0.1485,
      "step": 1372
    },
    {
      "epoch": 1.607728337236534,
      "grad_norm": 1.3893063068389893,
      "learning_rate": 5e-05,
      "loss": 0.118,
      "step": 1373
    },
    {
      "epoch": 1.6088992974238876,
      "grad_norm": 0.7505009770393372,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 1374
    },
    {
      "epoch": 1.6100702576112411,
      "grad_norm": 0.7635769844055176,
      "learning_rate": 5e-05,
      "loss": 0.0912,
      "step": 1375
    },
    {
      "epoch": 1.6112412177985949,
      "grad_norm": 0.7339370250701904,
      "learning_rate": 5e-05,
      "loss": 0.0719,
      "step": 1376
    },
    {
      "epoch": 1.6124121779859486,
      "grad_norm": 0.9747689366340637,
      "learning_rate": 5e-05,
      "loss": 0.1069,
      "step": 1377
    },
    {
      "epoch": 1.6135831381733021,
      "grad_norm": 1.5551780462265015,
      "learning_rate": 5e-05,
      "loss": 0.0412,
      "step": 1378
    },
    {
      "epoch": 1.6147540983606556,
      "grad_norm": 0.9838870167732239,
      "learning_rate": 5e-05,
      "loss": 0.1781,
      "step": 1379
    },
    {
      "epoch": 1.6159250585480094,
      "grad_norm": 0.9426593780517578,
      "learning_rate": 5e-05,
      "loss": 0.22,
      "step": 1380
    },
    {
      "epoch": 1.6170960187353631,
      "grad_norm": 0.9972395896911621,
      "learning_rate": 5e-05,
      "loss": 0.1103,
      "step": 1381
    },
    {
      "epoch": 1.6182669789227166,
      "grad_norm": 0.9909510016441345,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1382
    },
    {
      "epoch": 1.6194379391100702,
      "grad_norm": 0.6671304702758789,
      "learning_rate": 5e-05,
      "loss": 0.0962,
      "step": 1383
    },
    {
      "epoch": 1.620608899297424,
      "grad_norm": 1.0957039594650269,
      "learning_rate": 5e-05,
      "loss": 0.0878,
      "step": 1384
    },
    {
      "epoch": 1.6217798594847777,
      "grad_norm": 0.5984320640563965,
      "learning_rate": 5e-05,
      "loss": 0.0592,
      "step": 1385
    },
    {
      "epoch": 1.6229508196721312,
      "grad_norm": 0.6819044947624207,
      "learning_rate": 5e-05,
      "loss": 0.0527,
      "step": 1386
    },
    {
      "epoch": 1.6241217798594847,
      "grad_norm": 0.8221101760864258,
      "learning_rate": 5e-05,
      "loss": 0.1896,
      "step": 1387
    },
    {
      "epoch": 1.6252927400468384,
      "grad_norm": 1.0206559896469116,
      "learning_rate": 5e-05,
      "loss": 0.1166,
      "step": 1388
    },
    {
      "epoch": 1.6264637002341922,
      "grad_norm": 0.6018528342247009,
      "learning_rate": 5e-05,
      "loss": 0.0832,
      "step": 1389
    },
    {
      "epoch": 1.6276346604215457,
      "grad_norm": 1.0116900205612183,
      "learning_rate": 5e-05,
      "loss": 0.0948,
      "step": 1390
    },
    {
      "epoch": 1.6288056206088992,
      "grad_norm": 1.0668933391571045,
      "learning_rate": 5e-05,
      "loss": 0.0573,
      "step": 1391
    },
    {
      "epoch": 1.629976580796253,
      "grad_norm": 0.8476468920707703,
      "learning_rate": 5e-05,
      "loss": 0.0528,
      "step": 1392
    },
    {
      "epoch": 1.6311475409836067,
      "grad_norm": 0.7461889982223511,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 1393
    },
    {
      "epoch": 1.6323185011709602,
      "grad_norm": 0.7920565605163574,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 1394
    },
    {
      "epoch": 1.6334894613583137,
      "grad_norm": 0.7371179461479187,
      "learning_rate": 5e-05,
      "loss": 0.0724,
      "step": 1395
    },
    {
      "epoch": 1.6346604215456675,
      "grad_norm": 1.0653191804885864,
      "learning_rate": 5e-05,
      "loss": 0.1002,
      "step": 1396
    },
    {
      "epoch": 1.6358313817330212,
      "grad_norm": 1.0800734758377075,
      "learning_rate": 5e-05,
      "loss": 0.1397,
      "step": 1397
    },
    {
      "epoch": 1.6370023419203747,
      "grad_norm": 0.5778550505638123,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1398
    },
    {
      "epoch": 1.6381733021077283,
      "grad_norm": 1.2435455322265625,
      "learning_rate": 5e-05,
      "loss": 0.2151,
      "step": 1399
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 0.5714565515518188,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 1400
    },
    {
      "epoch": 1.6405152224824358,
      "grad_norm": 0.6957547664642334,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 1401
    },
    {
      "epoch": 1.6416861826697893,
      "grad_norm": 0.8038197159767151,
      "learning_rate": 5e-05,
      "loss": 0.0867,
      "step": 1402
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 0.9675960540771484,
      "learning_rate": 5e-05,
      "loss": 0.1024,
      "step": 1403
    },
    {
      "epoch": 1.6440281030444965,
      "grad_norm": 0.9055091142654419,
      "learning_rate": 5e-05,
      "loss": 0.1111,
      "step": 1404
    },
    {
      "epoch": 1.6451990632318503,
      "grad_norm": 1.1415334939956665,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 1405
    },
    {
      "epoch": 1.6463700234192038,
      "grad_norm": 1.4727530479431152,
      "learning_rate": 5e-05,
      "loss": 0.1031,
      "step": 1406
    },
    {
      "epoch": 1.6475409836065573,
      "grad_norm": 0.5257972478866577,
      "learning_rate": 5e-05,
      "loss": 0.0197,
      "step": 1407
    },
    {
      "epoch": 1.648711943793911,
      "grad_norm": 1.05034601688385,
      "learning_rate": 5e-05,
      "loss": 0.1233,
      "step": 1408
    },
    {
      "epoch": 1.6498829039812648,
      "grad_norm": 0.9267873167991638,
      "learning_rate": 5e-05,
      "loss": 0.0599,
      "step": 1409
    },
    {
      "epoch": 1.651053864168618,
      "grad_norm": 1.3238446712493896,
      "learning_rate": 5e-05,
      "loss": 0.1469,
      "step": 1410
    },
    {
      "epoch": 1.6522248243559718,
      "grad_norm": 0.8370455503463745,
      "learning_rate": 5e-05,
      "loss": 0.0481,
      "step": 1411
    },
    {
      "epoch": 1.6533957845433256,
      "grad_norm": 0.8004166483879089,
      "learning_rate": 5e-05,
      "loss": 0.0696,
      "step": 1412
    },
    {
      "epoch": 1.654566744730679,
      "grad_norm": 0.7936726808547974,
      "learning_rate": 5e-05,
      "loss": 0.127,
      "step": 1413
    },
    {
      "epoch": 1.6557377049180326,
      "grad_norm": 0.9206997156143188,
      "learning_rate": 5e-05,
      "loss": 0.1773,
      "step": 1414
    },
    {
      "epoch": 1.6569086651053864,
      "grad_norm": 1.257904052734375,
      "learning_rate": 5e-05,
      "loss": 0.082,
      "step": 1415
    },
    {
      "epoch": 1.6580796252927401,
      "grad_norm": 1.044830083847046,
      "learning_rate": 5e-05,
      "loss": 0.0404,
      "step": 1416
    },
    {
      "epoch": 1.6592505854800936,
      "grad_norm": 1.260611653327942,
      "learning_rate": 5e-05,
      "loss": 0.1124,
      "step": 1417
    },
    {
      "epoch": 1.6604215456674472,
      "grad_norm": 0.5810041427612305,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 1418
    },
    {
      "epoch": 1.661592505854801,
      "grad_norm": 1.3153175115585327,
      "learning_rate": 5e-05,
      "loss": 0.1169,
      "step": 1419
    },
    {
      "epoch": 1.6627634660421546,
      "grad_norm": 0.7556731104850769,
      "learning_rate": 5e-05,
      "loss": 0.1349,
      "step": 1420
    },
    {
      "epoch": 1.6639344262295082,
      "grad_norm": 0.9166858792304993,
      "learning_rate": 5e-05,
      "loss": 0.0491,
      "step": 1421
    },
    {
      "epoch": 1.6651053864168617,
      "grad_norm": 0.8037160038948059,
      "learning_rate": 5e-05,
      "loss": 0.0472,
      "step": 1422
    },
    {
      "epoch": 1.6662763466042154,
      "grad_norm": 0.5067633986473083,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 1423
    },
    {
      "epoch": 1.6674473067915692,
      "grad_norm": 0.6714938879013062,
      "learning_rate": 5e-05,
      "loss": 0.1024,
      "step": 1424
    },
    {
      "epoch": 1.6686182669789227,
      "grad_norm": 0.45404553413391113,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 1425
    },
    {
      "epoch": 1.6697892271662762,
      "grad_norm": 0.2109605222940445,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 1426
    },
    {
      "epoch": 1.67096018735363,
      "grad_norm": 1.1551426649093628,
      "learning_rate": 5e-05,
      "loss": 0.0894,
      "step": 1427
    },
    {
      "epoch": 1.6721311475409837,
      "grad_norm": 1.2504373788833618,
      "learning_rate": 5e-05,
      "loss": 0.1014,
      "step": 1428
    },
    {
      "epoch": 1.6733021077283372,
      "grad_norm": 2.6157608032226562,
      "learning_rate": 5e-05,
      "loss": 0.1129,
      "step": 1429
    },
    {
      "epoch": 1.6744730679156907,
      "grad_norm": 0.7132817506790161,
      "learning_rate": 5e-05,
      "loss": 0.0684,
      "step": 1430
    },
    {
      "epoch": 1.6756440281030445,
      "grad_norm": 1.712977409362793,
      "learning_rate": 5e-05,
      "loss": 0.0898,
      "step": 1431
    },
    {
      "epoch": 1.6768149882903982,
      "grad_norm": 0.5152232646942139,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 1432
    },
    {
      "epoch": 1.6779859484777517,
      "grad_norm": 1.2964365482330322,
      "learning_rate": 5e-05,
      "loss": 0.0968,
      "step": 1433
    },
    {
      "epoch": 1.6791569086651053,
      "grad_norm": 1.037431001663208,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 1434
    },
    {
      "epoch": 1.680327868852459,
      "grad_norm": 1.0789915323257446,
      "learning_rate": 5e-05,
      "loss": 0.1638,
      "step": 1435
    },
    {
      "epoch": 1.6814988290398127,
      "grad_norm": 1.2120252847671509,
      "learning_rate": 5e-05,
      "loss": 0.0984,
      "step": 1436
    },
    {
      "epoch": 1.6826697892271663,
      "grad_norm": 1.317799687385559,
      "learning_rate": 5e-05,
      "loss": 0.0665,
      "step": 1437
    },
    {
      "epoch": 1.6838407494145198,
      "grad_norm": 2.0610079765319824,
      "learning_rate": 5e-05,
      "loss": 0.1913,
      "step": 1438
    },
    {
      "epoch": 1.6850117096018735,
      "grad_norm": 1.5320656299591064,
      "learning_rate": 5e-05,
      "loss": 0.0847,
      "step": 1439
    },
    {
      "epoch": 1.6861826697892273,
      "grad_norm": 0.9364979267120361,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 1440
    },
    {
      "epoch": 1.6873536299765808,
      "grad_norm": 0.4315788745880127,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 1441
    },
    {
      "epoch": 1.6885245901639343,
      "grad_norm": 1.193460464477539,
      "learning_rate": 5e-05,
      "loss": 0.1032,
      "step": 1442
    },
    {
      "epoch": 1.689695550351288,
      "grad_norm": 0.9438802003860474,
      "learning_rate": 5e-05,
      "loss": 0.1288,
      "step": 1443
    },
    {
      "epoch": 1.6908665105386418,
      "grad_norm": 0.7294426560401917,
      "learning_rate": 5e-05,
      "loss": 0.0698,
      "step": 1444
    },
    {
      "epoch": 1.6920374707259953,
      "grad_norm": 0.972893238067627,
      "learning_rate": 5e-05,
      "loss": 0.1426,
      "step": 1445
    },
    {
      "epoch": 1.6932084309133488,
      "grad_norm": 0.8597643375396729,
      "learning_rate": 5e-05,
      "loss": 0.1131,
      "step": 1446
    },
    {
      "epoch": 1.6943793911007026,
      "grad_norm": 0.9338300824165344,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1447
    },
    {
      "epoch": 1.6955503512880563,
      "grad_norm": 0.707971453666687,
      "learning_rate": 5e-05,
      "loss": 0.0602,
      "step": 1448
    },
    {
      "epoch": 1.6967213114754098,
      "grad_norm": 0.663945198059082,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 1449
    },
    {
      "epoch": 1.6978922716627634,
      "grad_norm": 1.0236411094665527,
      "learning_rate": 5e-05,
      "loss": 0.0471,
      "step": 1450
    },
    {
      "epoch": 1.699063231850117,
      "grad_norm": 0.3927362561225891,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 1451
    },
    {
      "epoch": 1.7002341920374708,
      "grad_norm": 1.29240882396698,
      "learning_rate": 5e-05,
      "loss": 0.0814,
      "step": 1452
    },
    {
      "epoch": 1.7014051522248244,
      "grad_norm": 1.1841837167739868,
      "learning_rate": 5e-05,
      "loss": 0.098,
      "step": 1453
    },
    {
      "epoch": 1.7025761124121779,
      "grad_norm": 1.3511887788772583,
      "learning_rate": 5e-05,
      "loss": 0.1254,
      "step": 1454
    },
    {
      "epoch": 1.7037470725995316,
      "grad_norm": 1.040862798690796,
      "learning_rate": 5e-05,
      "loss": 0.1315,
      "step": 1455
    },
    {
      "epoch": 1.7049180327868854,
      "grad_norm": 0.7258453965187073,
      "learning_rate": 5e-05,
      "loss": 0.0733,
      "step": 1456
    },
    {
      "epoch": 1.7060889929742389,
      "grad_norm": 1.508918046951294,
      "learning_rate": 5e-05,
      "loss": 0.1427,
      "step": 1457
    },
    {
      "epoch": 1.7072599531615924,
      "grad_norm": 0.8424428701400757,
      "learning_rate": 5e-05,
      "loss": 0.0465,
      "step": 1458
    },
    {
      "epoch": 1.7084309133489461,
      "grad_norm": 0.6532805562019348,
      "learning_rate": 5e-05,
      "loss": 0.1334,
      "step": 1459
    },
    {
      "epoch": 1.7096018735362999,
      "grad_norm": 0.6446035504341125,
      "learning_rate": 5e-05,
      "loss": 0.059,
      "step": 1460
    },
    {
      "epoch": 1.7107728337236534,
      "grad_norm": 1.0698679685592651,
      "learning_rate": 5e-05,
      "loss": 0.1264,
      "step": 1461
    },
    {
      "epoch": 1.711943793911007,
      "grad_norm": 1.032824158668518,
      "learning_rate": 5e-05,
      "loss": 0.1337,
      "step": 1462
    },
    {
      "epoch": 1.7131147540983607,
      "grad_norm": 0.9637680649757385,
      "learning_rate": 5e-05,
      "loss": 0.0843,
      "step": 1463
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.2683138847351074,
      "learning_rate": 5e-05,
      "loss": 0.0625,
      "step": 1464
    },
    {
      "epoch": 1.715456674473068,
      "grad_norm": 0.6019258499145508,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 1465
    },
    {
      "epoch": 1.7166276346604215,
      "grad_norm": 0.9623497128486633,
      "learning_rate": 5e-05,
      "loss": 0.1279,
      "step": 1466
    },
    {
      "epoch": 1.7177985948477752,
      "grad_norm": 0.8846211433410645,
      "learning_rate": 5e-05,
      "loss": 0.0823,
      "step": 1467
    },
    {
      "epoch": 1.718969555035129,
      "grad_norm": 1.6504755020141602,
      "learning_rate": 5e-05,
      "loss": 0.1992,
      "step": 1468
    },
    {
      "epoch": 1.7201405152224825,
      "grad_norm": 0.4803541302680969,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 1469
    },
    {
      "epoch": 1.721311475409836,
      "grad_norm": 1.137683629989624,
      "learning_rate": 5e-05,
      "loss": 0.1616,
      "step": 1470
    },
    {
      "epoch": 1.7224824355971897,
      "grad_norm": 0.940217912197113,
      "learning_rate": 5e-05,
      "loss": 0.0635,
      "step": 1471
    },
    {
      "epoch": 1.7236533957845435,
      "grad_norm": 0.6892257332801819,
      "learning_rate": 5e-05,
      "loss": 0.0546,
      "step": 1472
    },
    {
      "epoch": 1.724824355971897,
      "grad_norm": 1.3120323419570923,
      "learning_rate": 5e-05,
      "loss": 0.053,
      "step": 1473
    },
    {
      "epoch": 1.7259953161592505,
      "grad_norm": 1.1177449226379395,
      "learning_rate": 5e-05,
      "loss": 0.0799,
      "step": 1474
    },
    {
      "epoch": 1.7271662763466042,
      "grad_norm": 2.081210136413574,
      "learning_rate": 5e-05,
      "loss": 0.1278,
      "step": 1475
    },
    {
      "epoch": 1.728337236533958,
      "grad_norm": 0.7833505868911743,
      "learning_rate": 5e-05,
      "loss": 0.1,
      "step": 1476
    },
    {
      "epoch": 1.7295081967213115,
      "grad_norm": 0.7730314135551453,
      "learning_rate": 5e-05,
      "loss": 0.0956,
      "step": 1477
    },
    {
      "epoch": 1.730679156908665,
      "grad_norm": 0.6477564573287964,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 1478
    },
    {
      "epoch": 1.7318501170960188,
      "grad_norm": 1.2340679168701172,
      "learning_rate": 5e-05,
      "loss": 0.0745,
      "step": 1479
    },
    {
      "epoch": 1.7330210772833725,
      "grad_norm": 0.6637773513793945,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 1480
    },
    {
      "epoch": 1.734192037470726,
      "grad_norm": 1.6625367403030396,
      "learning_rate": 5e-05,
      "loss": 0.1234,
      "step": 1481
    },
    {
      "epoch": 1.7353629976580796,
      "grad_norm": 0.8582313656806946,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 1482
    },
    {
      "epoch": 1.7365339578454333,
      "grad_norm": 1.1544493436813354,
      "learning_rate": 5e-05,
      "loss": 0.0609,
      "step": 1483
    },
    {
      "epoch": 1.737704918032787,
      "grad_norm": 1.2734251022338867,
      "learning_rate": 5e-05,
      "loss": 0.1609,
      "step": 1484
    },
    {
      "epoch": 1.7388758782201406,
      "grad_norm": 0.6116646528244019,
      "learning_rate": 5e-05,
      "loss": 0.0526,
      "step": 1485
    },
    {
      "epoch": 1.740046838407494,
      "grad_norm": 1.1808507442474365,
      "learning_rate": 5e-05,
      "loss": 0.0869,
      "step": 1486
    },
    {
      "epoch": 1.7412177985948478,
      "grad_norm": 0.5603042840957642,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 1487
    },
    {
      "epoch": 1.7423887587822016,
      "grad_norm": 0.9259191751480103,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 1488
    },
    {
      "epoch": 1.743559718969555,
      "grad_norm": 0.7131378650665283,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 1489
    },
    {
      "epoch": 1.7447306791569086,
      "grad_norm": 0.728728711605072,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 1490
    },
    {
      "epoch": 1.7459016393442623,
      "grad_norm": 1.291527271270752,
      "learning_rate": 5e-05,
      "loss": 0.207,
      "step": 1491
    },
    {
      "epoch": 1.747072599531616,
      "grad_norm": 0.7640989422798157,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 1492
    },
    {
      "epoch": 1.7482435597189696,
      "grad_norm": 0.7873395681381226,
      "learning_rate": 5e-05,
      "loss": 0.0507,
      "step": 1493
    },
    {
      "epoch": 1.7494145199063231,
      "grad_norm": 0.617953896522522,
      "learning_rate": 5e-05,
      "loss": 0.0465,
      "step": 1494
    },
    {
      "epoch": 1.7505854800936769,
      "grad_norm": 1.255511999130249,
      "learning_rate": 5e-05,
      "loss": 0.1104,
      "step": 1495
    },
    {
      "epoch": 1.7517564402810304,
      "grad_norm": 1.8948394060134888,
      "learning_rate": 5e-05,
      "loss": 0.1115,
      "step": 1496
    },
    {
      "epoch": 1.752927400468384,
      "grad_norm": 0.7849718332290649,
      "learning_rate": 5e-05,
      "loss": 0.0469,
      "step": 1497
    },
    {
      "epoch": 1.7540983606557377,
      "grad_norm": 0.6083075404167175,
      "learning_rate": 5e-05,
      "loss": 0.1075,
      "step": 1498
    },
    {
      "epoch": 1.7552693208430914,
      "grad_norm": 0.6786718368530273,
      "learning_rate": 5e-05,
      "loss": 0.0583,
      "step": 1499
    },
    {
      "epoch": 1.756440281030445,
      "grad_norm": 0.8964380621910095,
      "learning_rate": 5e-05,
      "loss": 0.0711,
      "step": 1500
    },
    {
      "epoch": 1.7576112412177984,
      "grad_norm": 1.112647533416748,
      "learning_rate": 5e-05,
      "loss": 0.0956,
      "step": 1501
    },
    {
      "epoch": 1.7587822014051522,
      "grad_norm": 1.017396092414856,
      "learning_rate": 5e-05,
      "loss": 0.0791,
      "step": 1502
    },
    {
      "epoch": 1.759953161592506,
      "grad_norm": 0.6514520645141602,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 1503
    },
    {
      "epoch": 1.7611241217798594,
      "grad_norm": 0.9390989542007446,
      "learning_rate": 5e-05,
      "loss": 0.0736,
      "step": 1504
    },
    {
      "epoch": 1.762295081967213,
      "grad_norm": 1.9065415859222412,
      "learning_rate": 5e-05,
      "loss": 0.1188,
      "step": 1505
    },
    {
      "epoch": 1.7634660421545667,
      "grad_norm": 1.0098873376846313,
      "learning_rate": 5e-05,
      "loss": 0.0789,
      "step": 1506
    },
    {
      "epoch": 1.7646370023419204,
      "grad_norm": 0.8224272727966309,
      "learning_rate": 5e-05,
      "loss": 0.1028,
      "step": 1507
    },
    {
      "epoch": 1.765807962529274,
      "grad_norm": 0.901310384273529,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 1508
    },
    {
      "epoch": 1.7669789227166275,
      "grad_norm": 1.4777305126190186,
      "learning_rate": 5e-05,
      "loss": 0.0895,
      "step": 1509
    },
    {
      "epoch": 1.7681498829039812,
      "grad_norm": 0.7350836396217346,
      "learning_rate": 5e-05,
      "loss": 0.0484,
      "step": 1510
    },
    {
      "epoch": 1.769320843091335,
      "grad_norm": 1.3371715545654297,
      "learning_rate": 5e-05,
      "loss": 0.2011,
      "step": 1511
    },
    {
      "epoch": 1.7704918032786885,
      "grad_norm": 0.9521992802619934,
      "learning_rate": 5e-05,
      "loss": 0.0546,
      "step": 1512
    },
    {
      "epoch": 1.771662763466042,
      "grad_norm": 0.8671524524688721,
      "learning_rate": 5e-05,
      "loss": 0.0524,
      "step": 1513
    },
    {
      "epoch": 1.7728337236533958,
      "grad_norm": 0.9422122240066528,
      "learning_rate": 5e-05,
      "loss": 0.0642,
      "step": 1514
    },
    {
      "epoch": 1.7740046838407495,
      "grad_norm": 1.1509287357330322,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 1515
    },
    {
      "epoch": 1.775175644028103,
      "grad_norm": 1.357936978340149,
      "learning_rate": 5e-05,
      "loss": 0.0612,
      "step": 1516
    },
    {
      "epoch": 1.7763466042154565,
      "grad_norm": 0.9107847213745117,
      "learning_rate": 5e-05,
      "loss": 0.0611,
      "step": 1517
    },
    {
      "epoch": 1.7775175644028103,
      "grad_norm": 1.9542176723480225,
      "learning_rate": 5e-05,
      "loss": 0.1283,
      "step": 1518
    },
    {
      "epoch": 1.778688524590164,
      "grad_norm": 1.7796863317489624,
      "learning_rate": 5e-05,
      "loss": 0.1694,
      "step": 1519
    },
    {
      "epoch": 1.7798594847775175,
      "grad_norm": 0.6070377230644226,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 1520
    },
    {
      "epoch": 1.781030444964871,
      "grad_norm": 0.8339092135429382,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 1521
    },
    {
      "epoch": 1.7822014051522248,
      "grad_norm": 1.7917113304138184,
      "learning_rate": 5e-05,
      "loss": 0.1459,
      "step": 1522
    },
    {
      "epoch": 1.7833723653395785,
      "grad_norm": 0.9235424995422363,
      "learning_rate": 5e-05,
      "loss": 0.1127,
      "step": 1523
    },
    {
      "epoch": 1.784543325526932,
      "grad_norm": 0.7716054916381836,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 1524
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 1.2171598672866821,
      "learning_rate": 5e-05,
      "loss": 0.0572,
      "step": 1525
    },
    {
      "epoch": 1.7868852459016393,
      "grad_norm": 0.8650402426719666,
      "learning_rate": 5e-05,
      "loss": 0.0603,
      "step": 1526
    },
    {
      "epoch": 1.788056206088993,
      "grad_norm": 1.3004783391952515,
      "learning_rate": 5e-05,
      "loss": 0.1353,
      "step": 1527
    },
    {
      "epoch": 1.7892271662763466,
      "grad_norm": 0.9456977844238281,
      "learning_rate": 5e-05,
      "loss": 0.1168,
      "step": 1528
    },
    {
      "epoch": 1.7903981264637001,
      "grad_norm": 1.5797871351242065,
      "learning_rate": 5e-05,
      "loss": 0.1459,
      "step": 1529
    },
    {
      "epoch": 1.7915690866510539,
      "grad_norm": 1.3328185081481934,
      "learning_rate": 5e-05,
      "loss": 0.0827,
      "step": 1530
    },
    {
      "epoch": 1.7927400468384076,
      "grad_norm": 0.7751338481903076,
      "learning_rate": 5e-05,
      "loss": 0.0832,
      "step": 1531
    },
    {
      "epoch": 1.7939110070257611,
      "grad_norm": 0.6728726625442505,
      "learning_rate": 5e-05,
      "loss": 0.0522,
      "step": 1532
    },
    {
      "epoch": 1.7950819672131146,
      "grad_norm": 0.9266440868377686,
      "learning_rate": 5e-05,
      "loss": 0.1047,
      "step": 1533
    },
    {
      "epoch": 1.7962529274004684,
      "grad_norm": 2.791710138320923,
      "learning_rate": 5e-05,
      "loss": 0.1913,
      "step": 1534
    },
    {
      "epoch": 1.7974238875878221,
      "grad_norm": 6.229873180389404,
      "learning_rate": 5e-05,
      "loss": 0.1372,
      "step": 1535
    },
    {
      "epoch": 1.7985948477751756,
      "grad_norm": 1.7564241886138916,
      "learning_rate": 5e-05,
      "loss": 0.0896,
      "step": 1536
    },
    {
      "epoch": 1.7997658079625292,
      "grad_norm": 2.1235644817352295,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 1537
    },
    {
      "epoch": 1.800936768149883,
      "grad_norm": 0.7967101335525513,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 1538
    },
    {
      "epoch": 1.8021077283372366,
      "grad_norm": 0.7888146042823792,
      "learning_rate": 5e-05,
      "loss": 0.0862,
      "step": 1539
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 1.0990092754364014,
      "learning_rate": 5e-05,
      "loss": 0.0632,
      "step": 1540
    },
    {
      "epoch": 1.8044496487119437,
      "grad_norm": 0.8468794226646423,
      "learning_rate": 5e-05,
      "loss": 0.1134,
      "step": 1541
    },
    {
      "epoch": 1.8056206088992974,
      "grad_norm": 0.848587155342102,
      "learning_rate": 5e-05,
      "loss": 0.0705,
      "step": 1542
    },
    {
      "epoch": 1.8067915690866512,
      "grad_norm": 1.3308414220809937,
      "learning_rate": 5e-05,
      "loss": 0.1693,
      "step": 1543
    },
    {
      "epoch": 1.8079625292740047,
      "grad_norm": 1.0898360013961792,
      "learning_rate": 5e-05,
      "loss": 0.0446,
      "step": 1544
    },
    {
      "epoch": 1.8091334894613582,
      "grad_norm": 0.7463451623916626,
      "learning_rate": 5e-05,
      "loss": 0.0464,
      "step": 1545
    },
    {
      "epoch": 1.810304449648712,
      "grad_norm": 0.7327055335044861,
      "learning_rate": 5e-05,
      "loss": 0.0445,
      "step": 1546
    },
    {
      "epoch": 1.8114754098360657,
      "grad_norm": 1.1266533136367798,
      "learning_rate": 5e-05,
      "loss": 0.0944,
      "step": 1547
    },
    {
      "epoch": 1.8126463700234192,
      "grad_norm": 1.0273183584213257,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1548
    },
    {
      "epoch": 1.8138173302107727,
      "grad_norm": 1.0848755836486816,
      "learning_rate": 5e-05,
      "loss": 0.0529,
      "step": 1549
    },
    {
      "epoch": 1.8149882903981265,
      "grad_norm": 1.2129515409469604,
      "learning_rate": 5e-05,
      "loss": 0.0632,
      "step": 1550
    },
    {
      "epoch": 1.8161592505854802,
      "grad_norm": 1.105787992477417,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1551
    },
    {
      "epoch": 1.8173302107728337,
      "grad_norm": 1.0458033084869385,
      "learning_rate": 5e-05,
      "loss": 0.0511,
      "step": 1552
    },
    {
      "epoch": 1.8185011709601873,
      "grad_norm": 0.7855525612831116,
      "learning_rate": 5e-05,
      "loss": 0.067,
      "step": 1553
    },
    {
      "epoch": 1.819672131147541,
      "grad_norm": 0.6512848734855652,
      "learning_rate": 5e-05,
      "loss": 0.0392,
      "step": 1554
    },
    {
      "epoch": 1.8208430913348947,
      "grad_norm": 0.8478198051452637,
      "learning_rate": 5e-05,
      "loss": 0.0362,
      "step": 1555
    },
    {
      "epoch": 1.8220140515222483,
      "grad_norm": 1.3462233543395996,
      "learning_rate": 5e-05,
      "loss": 0.0583,
      "step": 1556
    },
    {
      "epoch": 1.8231850117096018,
      "grad_norm": 0.8101717233657837,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 1557
    },
    {
      "epoch": 1.8243559718969555,
      "grad_norm": 0.746505618095398,
      "learning_rate": 5e-05,
      "loss": 0.0749,
      "step": 1558
    },
    {
      "epoch": 1.8255269320843093,
      "grad_norm": 0.7388828992843628,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 1559
    },
    {
      "epoch": 1.8266978922716628,
      "grad_norm": 0.903550386428833,
      "learning_rate": 5e-05,
      "loss": 0.0704,
      "step": 1560
    },
    {
      "epoch": 1.8278688524590163,
      "grad_norm": 0.8960045576095581,
      "learning_rate": 5e-05,
      "loss": 0.0545,
      "step": 1561
    },
    {
      "epoch": 1.82903981264637,
      "grad_norm": 1.1467539072036743,
      "learning_rate": 5e-05,
      "loss": 0.0615,
      "step": 1562
    },
    {
      "epoch": 1.8302107728337238,
      "grad_norm": 0.7387193441390991,
      "learning_rate": 5e-05,
      "loss": 0.0913,
      "step": 1563
    },
    {
      "epoch": 1.8313817330210773,
      "grad_norm": 0.9680453538894653,
      "learning_rate": 5e-05,
      "loss": 0.1045,
      "step": 1564
    },
    {
      "epoch": 1.8325526932084308,
      "grad_norm": 1.1926462650299072,
      "learning_rate": 5e-05,
      "loss": 0.0544,
      "step": 1565
    },
    {
      "epoch": 1.8337236533957846,
      "grad_norm": 1.1686712503433228,
      "learning_rate": 5e-05,
      "loss": 0.1108,
      "step": 1566
    },
    {
      "epoch": 1.8348946135831383,
      "grad_norm": 1.6914713382720947,
      "learning_rate": 5e-05,
      "loss": 0.0797,
      "step": 1567
    },
    {
      "epoch": 1.8360655737704918,
      "grad_norm": 2.0019304752349854,
      "learning_rate": 5e-05,
      "loss": 0.1538,
      "step": 1568
    },
    {
      "epoch": 1.8372365339578454,
      "grad_norm": 0.879401445388794,
      "learning_rate": 5e-05,
      "loss": 0.0627,
      "step": 1569
    },
    {
      "epoch": 1.838407494145199,
      "grad_norm": 0.5341501235961914,
      "learning_rate": 5e-05,
      "loss": 0.0648,
      "step": 1570
    },
    {
      "epoch": 1.8395784543325528,
      "grad_norm": 0.4935768246650696,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1571
    },
    {
      "epoch": 1.8407494145199064,
      "grad_norm": 1.1282134056091309,
      "learning_rate": 5e-05,
      "loss": 0.1094,
      "step": 1572
    },
    {
      "epoch": 1.8419203747072599,
      "grad_norm": 1.919262409210205,
      "learning_rate": 5e-05,
      "loss": 0.0859,
      "step": 1573
    },
    {
      "epoch": 1.8430913348946136,
      "grad_norm": 1.7395073175430298,
      "learning_rate": 5e-05,
      "loss": 0.0786,
      "step": 1574
    },
    {
      "epoch": 1.8442622950819674,
      "grad_norm": 0.8090342283248901,
      "learning_rate": 5e-05,
      "loss": 0.0548,
      "step": 1575
    },
    {
      "epoch": 1.845433255269321,
      "grad_norm": 0.9484445452690125,
      "learning_rate": 5e-05,
      "loss": 0.041,
      "step": 1576
    },
    {
      "epoch": 1.8466042154566744,
      "grad_norm": 0.6431611180305481,
      "learning_rate": 5e-05,
      "loss": 0.0966,
      "step": 1577
    },
    {
      "epoch": 1.8477751756440282,
      "grad_norm": 0.988149881362915,
      "learning_rate": 5e-05,
      "loss": 0.1208,
      "step": 1578
    },
    {
      "epoch": 1.848946135831382,
      "grad_norm": 1.2433078289031982,
      "learning_rate": 5e-05,
      "loss": 0.0704,
      "step": 1579
    },
    {
      "epoch": 1.8501170960187352,
      "grad_norm": 1.549638271331787,
      "learning_rate": 5e-05,
      "loss": 0.1055,
      "step": 1580
    },
    {
      "epoch": 1.851288056206089,
      "grad_norm": 0.3930216431617737,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 1581
    },
    {
      "epoch": 1.8524590163934427,
      "grad_norm": 0.8718817830085754,
      "learning_rate": 5e-05,
      "loss": 0.0913,
      "step": 1582
    },
    {
      "epoch": 1.8536299765807962,
      "grad_norm": 0.8748255372047424,
      "learning_rate": 5e-05,
      "loss": 0.111,
      "step": 1583
    },
    {
      "epoch": 1.8548009367681497,
      "grad_norm": 0.5985451936721802,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 1584
    },
    {
      "epoch": 1.8559718969555035,
      "grad_norm": 0.4675622284412384,
      "learning_rate": 5e-05,
      "loss": 0.0636,
      "step": 1585
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.6500037312507629,
      "learning_rate": 5e-05,
      "loss": 0.0825,
      "step": 1586
    },
    {
      "epoch": 1.8583138173302107,
      "grad_norm": 0.6800000667572021,
      "learning_rate": 5e-05,
      "loss": 0.0466,
      "step": 1587
    },
    {
      "epoch": 1.8594847775175642,
      "grad_norm": 0.7047803997993469,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 1588
    },
    {
      "epoch": 1.860655737704918,
      "grad_norm": 0.8539684414863586,
      "learning_rate": 5e-05,
      "loss": 0.0951,
      "step": 1589
    },
    {
      "epoch": 1.8618266978922717,
      "grad_norm": 1.0429909229278564,
      "learning_rate": 5e-05,
      "loss": 0.0724,
      "step": 1590
    },
    {
      "epoch": 1.8629976580796253,
      "grad_norm": 1.373196005821228,
      "learning_rate": 5e-05,
      "loss": 0.1927,
      "step": 1591
    },
    {
      "epoch": 1.8641686182669788,
      "grad_norm": 0.5546277165412903,
      "learning_rate": 5e-05,
      "loss": 0.0408,
      "step": 1592
    },
    {
      "epoch": 1.8653395784543325,
      "grad_norm": 0.5293512344360352,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 1593
    },
    {
      "epoch": 1.8665105386416863,
      "grad_norm": 0.948876678943634,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 1594
    },
    {
      "epoch": 1.8676814988290398,
      "grad_norm": 0.676456868648529,
      "learning_rate": 5e-05,
      "loss": 0.0761,
      "step": 1595
    },
    {
      "epoch": 1.8688524590163933,
      "grad_norm": 0.8978118896484375,
      "learning_rate": 5e-05,
      "loss": 0.108,
      "step": 1596
    },
    {
      "epoch": 1.870023419203747,
      "grad_norm": 0.8043511509895325,
      "learning_rate": 5e-05,
      "loss": 0.1096,
      "step": 1597
    },
    {
      "epoch": 1.8711943793911008,
      "grad_norm": 0.9805892705917358,
      "learning_rate": 5e-05,
      "loss": 0.0666,
      "step": 1598
    },
    {
      "epoch": 1.8723653395784543,
      "grad_norm": 0.7211447358131409,
      "learning_rate": 5e-05,
      "loss": 0.0826,
      "step": 1599
    },
    {
      "epoch": 1.8735362997658078,
      "grad_norm": 1.2567598819732666,
      "learning_rate": 5e-05,
      "loss": 0.1642,
      "step": 1600
    },
    {
      "epoch": 1.8747072599531616,
      "grad_norm": 1.2579855918884277,
      "learning_rate": 5e-05,
      "loss": 0.0986,
      "step": 1601
    },
    {
      "epoch": 1.8758782201405153,
      "grad_norm": 1.068959355354309,
      "learning_rate": 5e-05,
      "loss": 0.0622,
      "step": 1602
    },
    {
      "epoch": 1.8770491803278688,
      "grad_norm": 0.7582881450653076,
      "learning_rate": 5e-05,
      "loss": 0.0734,
      "step": 1603
    },
    {
      "epoch": 1.8782201405152223,
      "grad_norm": 1.0473687648773193,
      "learning_rate": 5e-05,
      "loss": 0.0695,
      "step": 1604
    },
    {
      "epoch": 1.879391100702576,
      "grad_norm": 1.1531535387039185,
      "learning_rate": 5e-05,
      "loss": 0.0866,
      "step": 1605
    },
    {
      "epoch": 1.8805620608899298,
      "grad_norm": 0.7097874879837036,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 1606
    },
    {
      "epoch": 1.8817330210772834,
      "grad_norm": 0.8765671849250793,
      "learning_rate": 5e-05,
      "loss": 0.0892,
      "step": 1607
    },
    {
      "epoch": 1.8829039812646369,
      "grad_norm": 0.8858269453048706,
      "learning_rate": 5e-05,
      "loss": 0.0693,
      "step": 1608
    },
    {
      "epoch": 1.8840749414519906,
      "grad_norm": 0.9945666790008545,
      "learning_rate": 5e-05,
      "loss": 0.1965,
      "step": 1609
    },
    {
      "epoch": 1.8852459016393444,
      "grad_norm": 1.3439956903457642,
      "learning_rate": 5e-05,
      "loss": 0.0802,
      "step": 1610
    },
    {
      "epoch": 1.8864168618266979,
      "grad_norm": 0.486629456281662,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 1611
    },
    {
      "epoch": 1.8875878220140514,
      "grad_norm": 1.1816471815109253,
      "learning_rate": 5e-05,
      "loss": 0.1255,
      "step": 1612
    },
    {
      "epoch": 1.8887587822014051,
      "grad_norm": 0.944142758846283,
      "learning_rate": 5e-05,
      "loss": 0.0699,
      "step": 1613
    },
    {
      "epoch": 1.8899297423887589,
      "grad_norm": 0.8677887916564941,
      "learning_rate": 5e-05,
      "loss": 0.087,
      "step": 1614
    },
    {
      "epoch": 1.8911007025761124,
      "grad_norm": 1.4522408246994019,
      "learning_rate": 5e-05,
      "loss": 0.0662,
      "step": 1615
    },
    {
      "epoch": 1.892271662763466,
      "grad_norm": 0.5758202075958252,
      "learning_rate": 5e-05,
      "loss": 0.0562,
      "step": 1616
    },
    {
      "epoch": 1.8934426229508197,
      "grad_norm": 1.609742522239685,
      "learning_rate": 5e-05,
      "loss": 0.0705,
      "step": 1617
    },
    {
      "epoch": 1.8946135831381734,
      "grad_norm": 1.3227615356445312,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 1618
    },
    {
      "epoch": 1.895784543325527,
      "grad_norm": 1.121018409729004,
      "learning_rate": 5e-05,
      "loss": 0.1618,
      "step": 1619
    },
    {
      "epoch": 1.8969555035128804,
      "grad_norm": 0.5165947079658508,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 1620
    },
    {
      "epoch": 1.8981264637002342,
      "grad_norm": 1.2465085983276367,
      "learning_rate": 5e-05,
      "loss": 0.0832,
      "step": 1621
    },
    {
      "epoch": 1.899297423887588,
      "grad_norm": 1.1241780519485474,
      "learning_rate": 5e-05,
      "loss": 0.0516,
      "step": 1622
    },
    {
      "epoch": 1.9004683840749415,
      "grad_norm": 0.988499641418457,
      "learning_rate": 5e-05,
      "loss": 0.0714,
      "step": 1623
    },
    {
      "epoch": 1.901639344262295,
      "grad_norm": 1.1574716567993164,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 1624
    },
    {
      "epoch": 1.9028103044496487,
      "grad_norm": 0.9647327065467834,
      "learning_rate": 5e-05,
      "loss": 0.0675,
      "step": 1625
    },
    {
      "epoch": 1.9039812646370025,
      "grad_norm": 1.1202434301376343,
      "learning_rate": 5e-05,
      "loss": 0.1103,
      "step": 1626
    },
    {
      "epoch": 1.905152224824356,
      "grad_norm": 0.96470046043396,
      "learning_rate": 5e-05,
      "loss": 0.0672,
      "step": 1627
    },
    {
      "epoch": 1.9063231850117095,
      "grad_norm": 1.0954195261001587,
      "learning_rate": 5e-05,
      "loss": 0.0622,
      "step": 1628
    },
    {
      "epoch": 1.9074941451990632,
      "grad_norm": 0.9042385220527649,
      "learning_rate": 5e-05,
      "loss": 0.0711,
      "step": 1629
    },
    {
      "epoch": 1.908665105386417,
      "grad_norm": 1.3995596170425415,
      "learning_rate": 5e-05,
      "loss": 0.1424,
      "step": 1630
    },
    {
      "epoch": 1.9098360655737705,
      "grad_norm": 0.48009419441223145,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 1631
    },
    {
      "epoch": 1.911007025761124,
      "grad_norm": 1.5025397539138794,
      "learning_rate": 5e-05,
      "loss": 0.0784,
      "step": 1632
    },
    {
      "epoch": 1.9121779859484778,
      "grad_norm": 0.7327423095703125,
      "learning_rate": 5e-05,
      "loss": 0.1,
      "step": 1633
    },
    {
      "epoch": 1.9133489461358315,
      "grad_norm": 1.4353890419006348,
      "learning_rate": 5e-05,
      "loss": 0.0873,
      "step": 1634
    },
    {
      "epoch": 1.914519906323185,
      "grad_norm": 0.5344592928886414,
      "learning_rate": 5e-05,
      "loss": 0.0386,
      "step": 1635
    },
    {
      "epoch": 1.9156908665105385,
      "grad_norm": 1.5384941101074219,
      "learning_rate": 5e-05,
      "loss": 0.0767,
      "step": 1636
    },
    {
      "epoch": 1.9168618266978923,
      "grad_norm": 2.1275546550750732,
      "learning_rate": 5e-05,
      "loss": 0.0781,
      "step": 1637
    },
    {
      "epoch": 1.918032786885246,
      "grad_norm": 0.5893155932426453,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 1638
    },
    {
      "epoch": 1.9192037470725996,
      "grad_norm": 1.5744127035140991,
      "learning_rate": 5e-05,
      "loss": 0.1497,
      "step": 1639
    },
    {
      "epoch": 1.920374707259953,
      "grad_norm": 0.7976338267326355,
      "learning_rate": 5e-05,
      "loss": 0.0774,
      "step": 1640
    },
    {
      "epoch": 1.9215456674473068,
      "grad_norm": 0.8872117400169373,
      "learning_rate": 5e-05,
      "loss": 0.1139,
      "step": 1641
    },
    {
      "epoch": 1.9227166276346606,
      "grad_norm": 1.0150974988937378,
      "learning_rate": 5e-05,
      "loss": 0.0198,
      "step": 1642
    },
    {
      "epoch": 1.923887587822014,
      "grad_norm": 0.7934679388999939,
      "learning_rate": 5e-05,
      "loss": 0.0462,
      "step": 1643
    },
    {
      "epoch": 1.9250585480093676,
      "grad_norm": 0.844943106174469,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 1644
    },
    {
      "epoch": 1.9262295081967213,
      "grad_norm": 1.315199851989746,
      "learning_rate": 5e-05,
      "loss": 0.0583,
      "step": 1645
    },
    {
      "epoch": 1.927400468384075,
      "grad_norm": 0.5515837669372559,
      "learning_rate": 5e-05,
      "loss": 0.054,
      "step": 1646
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 0.4146546721458435,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 1647
    },
    {
      "epoch": 1.9297423887587821,
      "grad_norm": 0.7901695370674133,
      "learning_rate": 5e-05,
      "loss": 0.1116,
      "step": 1648
    },
    {
      "epoch": 1.9309133489461359,
      "grad_norm": 1.421716332435608,
      "learning_rate": 5e-05,
      "loss": 0.3482,
      "step": 1649
    },
    {
      "epoch": 1.9320843091334896,
      "grad_norm": 0.9317842125892639,
      "learning_rate": 5e-05,
      "loss": 0.094,
      "step": 1650
    },
    {
      "epoch": 1.9332552693208431,
      "grad_norm": 1.0144319534301758,
      "learning_rate": 5e-05,
      "loss": 0.1239,
      "step": 1651
    },
    {
      "epoch": 1.9344262295081966,
      "grad_norm": 0.8004108667373657,
      "learning_rate": 5e-05,
      "loss": 0.1165,
      "step": 1652
    },
    {
      "epoch": 1.9355971896955504,
      "grad_norm": 1.3346511125564575,
      "learning_rate": 5e-05,
      "loss": 0.1127,
      "step": 1653
    },
    {
      "epoch": 1.9367681498829041,
      "grad_norm": 1.195380449295044,
      "learning_rate": 5e-05,
      "loss": 0.0703,
      "step": 1654
    },
    {
      "epoch": 1.9379391100702577,
      "grad_norm": 0.754807710647583,
      "learning_rate": 5e-05,
      "loss": 0.0661,
      "step": 1655
    },
    {
      "epoch": 1.9391100702576112,
      "grad_norm": 1.4250777959823608,
      "learning_rate": 5e-05,
      "loss": 0.0693,
      "step": 1656
    },
    {
      "epoch": 1.940281030444965,
      "grad_norm": 1.3044383525848389,
      "learning_rate": 5e-05,
      "loss": 0.0911,
      "step": 1657
    },
    {
      "epoch": 1.9414519906323187,
      "grad_norm": 0.7318621277809143,
      "learning_rate": 5e-05,
      "loss": 0.0608,
      "step": 1658
    },
    {
      "epoch": 1.9426229508196722,
      "grad_norm": 0.7380045056343079,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 1659
    },
    {
      "epoch": 1.9437939110070257,
      "grad_norm": 0.960515022277832,
      "learning_rate": 5e-05,
      "loss": 0.1199,
      "step": 1660
    },
    {
      "epoch": 1.9449648711943794,
      "grad_norm": 0.6085923314094543,
      "learning_rate": 5e-05,
      "loss": 0.0671,
      "step": 1661
    },
    {
      "epoch": 1.9461358313817332,
      "grad_norm": 1.0804243087768555,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 1662
    },
    {
      "epoch": 1.9473067915690867,
      "grad_norm": 1.029897928237915,
      "learning_rate": 5e-05,
      "loss": 0.0841,
      "step": 1663
    },
    {
      "epoch": 1.9484777517564402,
      "grad_norm": 0.870849072933197,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 1664
    },
    {
      "epoch": 1.949648711943794,
      "grad_norm": 0.9278104305267334,
      "learning_rate": 5e-05,
      "loss": 0.0571,
      "step": 1665
    },
    {
      "epoch": 1.9508196721311475,
      "grad_norm": 0.5971968173980713,
      "learning_rate": 5e-05,
      "loss": 0.1045,
      "step": 1666
    },
    {
      "epoch": 1.951990632318501,
      "grad_norm": 0.86765456199646,
      "learning_rate": 5e-05,
      "loss": 0.0795,
      "step": 1667
    },
    {
      "epoch": 1.9531615925058547,
      "grad_norm": 0.8563480377197266,
      "learning_rate": 5e-05,
      "loss": 0.0647,
      "step": 1668
    },
    {
      "epoch": 1.9543325526932085,
      "grad_norm": 1.3442497253417969,
      "learning_rate": 5e-05,
      "loss": 0.1049,
      "step": 1669
    },
    {
      "epoch": 1.955503512880562,
      "grad_norm": 0.870881974697113,
      "learning_rate": 5e-05,
      "loss": 0.0517,
      "step": 1670
    },
    {
      "epoch": 1.9566744730679155,
      "grad_norm": 1.2341490983963013,
      "learning_rate": 5e-05,
      "loss": 0.0857,
      "step": 1671
    },
    {
      "epoch": 1.9578454332552693,
      "grad_norm": 0.8850175738334656,
      "learning_rate": 5e-05,
      "loss": 0.0394,
      "step": 1672
    },
    {
      "epoch": 1.959016393442623,
      "grad_norm": 0.9859949946403503,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1673
    },
    {
      "epoch": 1.9601873536299765,
      "grad_norm": 1.0861644744873047,
      "learning_rate": 5e-05,
      "loss": 0.1113,
      "step": 1674
    },
    {
      "epoch": 1.96135831381733,
      "grad_norm": 1.1362650394439697,
      "learning_rate": 5e-05,
      "loss": 0.1098,
      "step": 1675
    },
    {
      "epoch": 1.9625292740046838,
      "grad_norm": 0.6268506646156311,
      "learning_rate": 5e-05,
      "loss": 0.0606,
      "step": 1676
    },
    {
      "epoch": 1.9637002341920375,
      "grad_norm": 0.8296578526496887,
      "learning_rate": 5e-05,
      "loss": 0.0763,
      "step": 1677
    },
    {
      "epoch": 1.964871194379391,
      "grad_norm": 0.7916064262390137,
      "learning_rate": 5e-05,
      "loss": 0.081,
      "step": 1678
    },
    {
      "epoch": 1.9660421545667446,
      "grad_norm": 1.035873293876648,
      "learning_rate": 5e-05,
      "loss": 0.0425,
      "step": 1679
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 0.894690990447998,
      "learning_rate": 5e-05,
      "loss": 0.0917,
      "step": 1680
    },
    {
      "epoch": 1.968384074941452,
      "grad_norm": 0.795177698135376,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 1681
    },
    {
      "epoch": 1.9695550351288056,
      "grad_norm": 0.8944557309150696,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 1682
    },
    {
      "epoch": 1.970725995316159,
      "grad_norm": 1.1674286127090454,
      "learning_rate": 5e-05,
      "loss": 0.1299,
      "step": 1683
    },
    {
      "epoch": 1.9718969555035128,
      "grad_norm": 1.2111459970474243,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 1684
    },
    {
      "epoch": 1.9730679156908666,
      "grad_norm": 0.9783915877342224,
      "learning_rate": 5e-05,
      "loss": 0.1127,
      "step": 1685
    },
    {
      "epoch": 1.9742388758782201,
      "grad_norm": 0.5226426720619202,
      "learning_rate": 5e-05,
      "loss": 0.0198,
      "step": 1686
    },
    {
      "epoch": 1.9754098360655736,
      "grad_norm": 1.0519217252731323,
      "learning_rate": 5e-05,
      "loss": 0.0649,
      "step": 1687
    },
    {
      "epoch": 1.9765807962529274,
      "grad_norm": 0.9543723464012146,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 1688
    },
    {
      "epoch": 1.9777517564402811,
      "grad_norm": 1.0463471412658691,
      "learning_rate": 5e-05,
      "loss": 0.0981,
      "step": 1689
    },
    {
      "epoch": 1.9789227166276346,
      "grad_norm": 0.983798623085022,
      "learning_rate": 5e-05,
      "loss": 0.0486,
      "step": 1690
    },
    {
      "epoch": 1.9800936768149882,
      "grad_norm": 1.0807572603225708,
      "learning_rate": 5e-05,
      "loss": 0.0564,
      "step": 1691
    },
    {
      "epoch": 1.981264637002342,
      "grad_norm": 0.6723021268844604,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 1692
    },
    {
      "epoch": 1.9824355971896956,
      "grad_norm": 0.6158366799354553,
      "learning_rate": 5e-05,
      "loss": 0.0493,
      "step": 1693
    },
    {
      "epoch": 1.9836065573770492,
      "grad_norm": 1.2005082368850708,
      "learning_rate": 5e-05,
      "loss": 0.075,
      "step": 1694
    },
    {
      "epoch": 1.9847775175644027,
      "grad_norm": 1.052802324295044,
      "learning_rate": 5e-05,
      "loss": 0.1322,
      "step": 1695
    },
    {
      "epoch": 1.9859484777517564,
      "grad_norm": 0.9198846817016602,
      "learning_rate": 5e-05,
      "loss": 0.0582,
      "step": 1696
    },
    {
      "epoch": 1.9871194379391102,
      "grad_norm": 1.0301605463027954,
      "learning_rate": 5e-05,
      "loss": 0.1039,
      "step": 1697
    },
    {
      "epoch": 1.9882903981264637,
      "grad_norm": 1.237428903579712,
      "learning_rate": 5e-05,
      "loss": 0.093,
      "step": 1698
    },
    {
      "epoch": 1.9894613583138172,
      "grad_norm": 1.3778384923934937,
      "learning_rate": 5e-05,
      "loss": 0.0585,
      "step": 1699
    },
    {
      "epoch": 1.990632318501171,
      "grad_norm": 1.6898796558380127,
      "learning_rate": 5e-05,
      "loss": 0.0695,
      "step": 1700
    },
    {
      "epoch": 1.9918032786885247,
      "grad_norm": 0.9861709475517273,
      "learning_rate": 5e-05,
      "loss": 0.0569,
      "step": 1701
    },
    {
      "epoch": 1.9929742388758782,
      "grad_norm": 0.5548719167709351,
      "learning_rate": 5e-05,
      "loss": 0.0663,
      "step": 1702
    },
    {
      "epoch": 1.9941451990632317,
      "grad_norm": 0.6054394245147705,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 1703
    },
    {
      "epoch": 1.9953161592505855,
      "grad_norm": 1.0867079496383667,
      "learning_rate": 5e-05,
      "loss": 0.0656,
      "step": 1704
    },
    {
      "epoch": 1.9964871194379392,
      "grad_norm": 0.7622233033180237,
      "learning_rate": 5e-05,
      "loss": 0.0835,
      "step": 1705
    },
    {
      "epoch": 1.9976580796252927,
      "grad_norm": 0.8889811038970947,
      "learning_rate": 5e-05,
      "loss": 0.0725,
      "step": 1706
    },
    {
      "epoch": 1.9988290398126463,
      "grad_norm": 0.6030131578445435,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 1707
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.3658230304718018,
      "learning_rate": 5e-05,
      "loss": 0.1443,
      "step": 1708
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.19144593179225922,
      "eval_runtime": 101.9481,
      "eval_samples_per_second": 8.2,
      "eval_steps_per_second": 1.03,
      "step": 1708
    },
    {
      "epoch": 2.0,
      "step": 1708,
      "total_flos": 7.067651759816049e+17,
      "train_loss": 0.16220328704382028,
      "train_runtime": 5481.8767,
      "train_samples_per_second": 2.493,
      "train_steps_per_second": 0.312
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1708,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "total_flos": 7.067651759816049e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

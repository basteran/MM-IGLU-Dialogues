{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 854.0,
  "global_step": 1708,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00117096018735363,
      "grad_norm": 4.57341194152832,
      "learning_rate": 0.0,
      "loss": 1.1129,
      "step": 1
    },
    {
      "epoch": 0.00234192037470726,
      "grad_norm": 3.201429605484009,
      "learning_rate": 1.1990623328406574e-05,
      "loss": 1.0079,
      "step": 2
    },
    {
      "epoch": 0.00351288056206089,
      "grad_norm": 5.141892433166504,
      "learning_rate": 1.900468833579672e-05,
      "loss": 0.9561,
      "step": 3
    },
    {
      "epoch": 0.00468384074941452,
      "grad_norm": 5.143064498901367,
      "learning_rate": 2.3981246656813148e-05,
      "loss": 1.0431,
      "step": 4
    },
    {
      "epoch": 0.00585480093676815,
      "grad_norm": 5.502471923828125,
      "learning_rate": 2.784136518143904e-05,
      "loss": 1.1332,
      "step": 5
    },
    {
      "epoch": 0.00702576112412178,
      "grad_norm": 4.891110897064209,
      "learning_rate": 3.099531166420329e-05,
      "loss": 1.1876,
      "step": 6
    },
    {
      "epoch": 0.00819672131147541,
      "grad_norm": 3.498654842376709,
      "learning_rate": 3.366193541954093e-05,
      "loss": 0.8505,
      "step": 7
    },
    {
      "epoch": 0.00936768149882904,
      "grad_norm": 2.860396385192871,
      "learning_rate": 3.597186998521972e-05,
      "loss": 0.5949,
      "step": 8
    },
    {
      "epoch": 0.01053864168618267,
      "grad_norm": 2.0944089889526367,
      "learning_rate": 3.800937667159344e-05,
      "loss": 0.8982,
      "step": 9
    },
    {
      "epoch": 0.0117096018735363,
      "grad_norm": 2.0021562576293945,
      "learning_rate": 3.983198850984562e-05,
      "loss": 0.7015,
      "step": 10
    },
    {
      "epoch": 0.01288056206088993,
      "grad_norm": 2.0021562576293945,
      "learning_rate": 3.983198850984562e-05,
      "loss": 0.5257,
      "step": 11
    },
    {
      "epoch": 0.01405152224824356,
      "grad_norm": 1.7136534452438354,
      "learning_rate": 4.148074146945969e-05,
      "loss": 0.484,
      "step": 12
    },
    {
      "epoch": 0.01522248243559719,
      "grad_norm": 2.2287023067474365,
      "learning_rate": 4.298593499260987e-05,
      "loss": 0.6635,
      "step": 13
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 2.375896692276001,
      "learning_rate": 4.437057880970483e-05,
      "loss": 0.7566,
      "step": 14
    },
    {
      "epoch": 0.01756440281030445,
      "grad_norm": 2.594383478164673,
      "learning_rate": 4.5652558747947496e-05,
      "loss": 0.6532,
      "step": 15
    },
    {
      "epoch": 0.01873536299765808,
      "grad_norm": 6.512693881988525,
      "learning_rate": 4.684605351723575e-05,
      "loss": 0.7349,
      "step": 16
    },
    {
      "epoch": 0.01990632318501171,
      "grad_norm": 1.5777779817581177,
      "learning_rate": 4.7962493313626296e-05,
      "loss": 0.5387,
      "step": 17
    },
    {
      "epoch": 0.02107728337236534,
      "grad_norm": 2.0042881965637207,
      "learning_rate": 4.901122729829134e-05,
      "loss": 0.9318,
      "step": 18
    },
    {
      "epoch": 0.02224824355971897,
      "grad_norm": 4.010674953460693,
      "learning_rate": 5e-05,
      "loss": 0.6097,
      "step": 19
    },
    {
      "epoch": 0.0234192037470726,
      "grad_norm": 2.08505916595459,
      "learning_rate": 5e-05,
      "loss": 0.4918,
      "step": 20
    },
    {
      "epoch": 0.02459016393442623,
      "grad_norm": 1.9405896663665771,
      "learning_rate": 5e-05,
      "loss": 0.6005,
      "step": 21
    },
    {
      "epoch": 0.02576112412177986,
      "grad_norm": 2.0461537837982178,
      "learning_rate": 5e-05,
      "loss": 0.4648,
      "step": 22
    },
    {
      "epoch": 0.026932084309133488,
      "grad_norm": 2.3759543895721436,
      "learning_rate": 5e-05,
      "loss": 0.4812,
      "step": 23
    },
    {
      "epoch": 0.02810304449648712,
      "grad_norm": 5.893507480621338,
      "learning_rate": 5e-05,
      "loss": 0.6352,
      "step": 24
    },
    {
      "epoch": 0.02927400468384075,
      "grad_norm": 1.8027676343917847,
      "learning_rate": 5e-05,
      "loss": 0.3433,
      "step": 25
    },
    {
      "epoch": 0.03044496487119438,
      "grad_norm": 1.8027676343917847,
      "learning_rate": 5e-05,
      "loss": 0.5619,
      "step": 26
    },
    {
      "epoch": 0.03161592505854801,
      "grad_norm": 1.8357187509536743,
      "learning_rate": 5e-05,
      "loss": 0.5082,
      "step": 27
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 2.456012487411499,
      "learning_rate": 5e-05,
      "loss": 0.3851,
      "step": 28
    },
    {
      "epoch": 0.03395784543325527,
      "grad_norm": 2.3365638256073,
      "learning_rate": 5e-05,
      "loss": 0.4113,
      "step": 29
    },
    {
      "epoch": 0.0351288056206089,
      "grad_norm": 3.4606943130493164,
      "learning_rate": 5e-05,
      "loss": 0.9685,
      "step": 30
    },
    {
      "epoch": 0.03629976580796253,
      "grad_norm": 2.938972234725952,
      "learning_rate": 5e-05,
      "loss": 0.6077,
      "step": 31
    },
    {
      "epoch": 0.03747072599531616,
      "grad_norm": 2.9552173614501953,
      "learning_rate": 5e-05,
      "loss": 0.3971,
      "step": 32
    },
    {
      "epoch": 0.03864168618266979,
      "grad_norm": 2.641340970993042,
      "learning_rate": 5e-05,
      "loss": 0.4775,
      "step": 33
    },
    {
      "epoch": 0.03981264637002342,
      "grad_norm": 1.164323091506958,
      "learning_rate": 5e-05,
      "loss": 0.3896,
      "step": 34
    },
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 3.866093873977661,
      "learning_rate": 5e-05,
      "loss": 0.6504,
      "step": 35
    },
    {
      "epoch": 0.04215456674473068,
      "grad_norm": 5.632772445678711,
      "learning_rate": 5e-05,
      "loss": 0.3506,
      "step": 36
    },
    {
      "epoch": 0.04332552693208431,
      "grad_norm": 1.7653688192367554,
      "learning_rate": 5e-05,
      "loss": 0.5024,
      "step": 37
    },
    {
      "epoch": 0.04449648711943794,
      "grad_norm": 1.5292503833770752,
      "learning_rate": 5e-05,
      "loss": 0.4763,
      "step": 38
    },
    {
      "epoch": 0.04566744730679157,
      "grad_norm": 2.8847334384918213,
      "learning_rate": 5e-05,
      "loss": 0.4607,
      "step": 39
    },
    {
      "epoch": 0.0468384074941452,
      "grad_norm": 2.0827114582061768,
      "learning_rate": 5e-05,
      "loss": 0.4484,
      "step": 40
    },
    {
      "epoch": 0.04800936768149883,
      "grad_norm": 1.437603235244751,
      "learning_rate": 5e-05,
      "loss": 0.3319,
      "step": 41
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 1.4617819786071777,
      "learning_rate": 5e-05,
      "loss": 0.4975,
      "step": 42
    },
    {
      "epoch": 0.05035128805620609,
      "grad_norm": 1.3480902910232544,
      "learning_rate": 5e-05,
      "loss": 0.2548,
      "step": 43
    },
    {
      "epoch": 0.05152224824355972,
      "grad_norm": 1.2507061958312988,
      "learning_rate": 5e-05,
      "loss": 0.4056,
      "step": 44
    },
    {
      "epoch": 0.05269320843091335,
      "grad_norm": 1.9781994819641113,
      "learning_rate": 5e-05,
      "loss": 0.4107,
      "step": 45
    },
    {
      "epoch": 0.053864168618266976,
      "grad_norm": 0.9148968458175659,
      "learning_rate": 5e-05,
      "loss": 0.3819,
      "step": 46
    },
    {
      "epoch": 0.05503512880562061,
      "grad_norm": 2.4385385513305664,
      "learning_rate": 5e-05,
      "loss": 0.4939,
      "step": 47
    },
    {
      "epoch": 0.05620608899297424,
      "grad_norm": 2.81229567527771,
      "learning_rate": 5e-05,
      "loss": 0.3293,
      "step": 48
    },
    {
      "epoch": 0.05737704918032787,
      "grad_norm": 1.394568681716919,
      "learning_rate": 5e-05,
      "loss": 0.4048,
      "step": 49
    },
    {
      "epoch": 0.0585480093676815,
      "grad_norm": 1.4732232093811035,
      "learning_rate": 5e-05,
      "loss": 0.4227,
      "step": 50
    },
    {
      "epoch": 0.059718969555035126,
      "grad_norm": 1.504557490348816,
      "learning_rate": 5e-05,
      "loss": 0.2127,
      "step": 51
    },
    {
      "epoch": 0.06088992974238876,
      "grad_norm": 2.413745641708374,
      "learning_rate": 5e-05,
      "loss": 0.4828,
      "step": 52
    },
    {
      "epoch": 0.06206088992974239,
      "grad_norm": 1.249548316001892,
      "learning_rate": 5e-05,
      "loss": 0.4634,
      "step": 53
    },
    {
      "epoch": 0.06323185011709602,
      "grad_norm": 1.4059988260269165,
      "learning_rate": 5e-05,
      "loss": 0.3166,
      "step": 54
    },
    {
      "epoch": 0.06440281030444965,
      "grad_norm": 2.1567542552948,
      "learning_rate": 5e-05,
      "loss": 0.3399,
      "step": 55
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 0.8936358690261841,
      "learning_rate": 5e-05,
      "loss": 0.3033,
      "step": 56
    },
    {
      "epoch": 0.06674473067915691,
      "grad_norm": 1.062636375427246,
      "learning_rate": 5e-05,
      "loss": 0.4295,
      "step": 57
    },
    {
      "epoch": 0.06791569086651054,
      "grad_norm": 1.6185232400894165,
      "learning_rate": 5e-05,
      "loss": 0.3243,
      "step": 58
    },
    {
      "epoch": 0.06908665105386416,
      "grad_norm": 1.731899619102478,
      "learning_rate": 5e-05,
      "loss": 0.4049,
      "step": 59
    },
    {
      "epoch": 0.0702576112412178,
      "grad_norm": 1.345297932624817,
      "learning_rate": 5e-05,
      "loss": 0.312,
      "step": 60
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 1.727185845375061,
      "learning_rate": 5e-05,
      "loss": 0.4766,
      "step": 61
    },
    {
      "epoch": 0.07259953161592506,
      "grad_norm": 1.3845092058181763,
      "learning_rate": 5e-05,
      "loss": 0.3267,
      "step": 62
    },
    {
      "epoch": 0.07377049180327869,
      "grad_norm": 1.795027494430542,
      "learning_rate": 5e-05,
      "loss": 0.5533,
      "step": 63
    },
    {
      "epoch": 0.07494145199063232,
      "grad_norm": 0.9776955246925354,
      "learning_rate": 5e-05,
      "loss": 0.5299,
      "step": 64
    },
    {
      "epoch": 0.07611241217798595,
      "grad_norm": 1.7556663751602173,
      "learning_rate": 5e-05,
      "loss": 0.3945,
      "step": 65
    },
    {
      "epoch": 0.07728337236533958,
      "grad_norm": 1.3526474237442017,
      "learning_rate": 5e-05,
      "loss": 0.509,
      "step": 66
    },
    {
      "epoch": 0.07845433255269321,
      "grad_norm": 1.1277307271957397,
      "learning_rate": 5e-05,
      "loss": 0.4132,
      "step": 67
    },
    {
      "epoch": 0.07962529274004684,
      "grad_norm": 1.968239665031433,
      "learning_rate": 5e-05,
      "loss": 0.4309,
      "step": 68
    },
    {
      "epoch": 0.08079625292740047,
      "grad_norm": 1.3841594457626343,
      "learning_rate": 5e-05,
      "loss": 0.4474,
      "step": 69
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 1.4659171104431152,
      "learning_rate": 5e-05,
      "loss": 0.3768,
      "step": 70
    },
    {
      "epoch": 0.08313817330210772,
      "grad_norm": 0.7868735790252686,
      "learning_rate": 5e-05,
      "loss": 0.2985,
      "step": 71
    },
    {
      "epoch": 0.08430913348946135,
      "grad_norm": 1.5171234607696533,
      "learning_rate": 5e-05,
      "loss": 0.4587,
      "step": 72
    },
    {
      "epoch": 0.08548009367681499,
      "grad_norm": 1.0425655841827393,
      "learning_rate": 5e-05,
      "loss": 0.398,
      "step": 73
    },
    {
      "epoch": 0.08665105386416862,
      "grad_norm": 0.9805247187614441,
      "learning_rate": 5e-05,
      "loss": 0.25,
      "step": 74
    },
    {
      "epoch": 0.08782201405152225,
      "grad_norm": 0.9621455073356628,
      "learning_rate": 5e-05,
      "loss": 0.3365,
      "step": 75
    },
    {
      "epoch": 0.08899297423887588,
      "grad_norm": 0.9963560700416565,
      "learning_rate": 5e-05,
      "loss": 0.3895,
      "step": 76
    },
    {
      "epoch": 0.09016393442622951,
      "grad_norm": 1.105128526687622,
      "learning_rate": 5e-05,
      "loss": 0.4186,
      "step": 77
    },
    {
      "epoch": 0.09133489461358314,
      "grad_norm": 0.9319223761558533,
      "learning_rate": 5e-05,
      "loss": 0.2395,
      "step": 78
    },
    {
      "epoch": 0.09250585480093677,
      "grad_norm": 1.0266963243484497,
      "learning_rate": 5e-05,
      "loss": 0.2753,
      "step": 79
    },
    {
      "epoch": 0.0936768149882904,
      "grad_norm": 0.8906570076942444,
      "learning_rate": 5e-05,
      "loss": 0.3252,
      "step": 80
    },
    {
      "epoch": 0.09484777517564402,
      "grad_norm": 1.5343520641326904,
      "learning_rate": 5e-05,
      "loss": 0.276,
      "step": 81
    },
    {
      "epoch": 0.09601873536299765,
      "grad_norm": 0.9456745982170105,
      "learning_rate": 5e-05,
      "loss": 0.2099,
      "step": 82
    },
    {
      "epoch": 0.09718969555035128,
      "grad_norm": 0.9964691400527954,
      "learning_rate": 5e-05,
      "loss": 0.2164,
      "step": 83
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 1.0267136096954346,
      "learning_rate": 5e-05,
      "loss": 0.314,
      "step": 84
    },
    {
      "epoch": 0.09953161592505855,
      "grad_norm": 1.1431087255477905,
      "learning_rate": 5e-05,
      "loss": 0.3161,
      "step": 85
    },
    {
      "epoch": 0.10070257611241218,
      "grad_norm": 1.131880760192871,
      "learning_rate": 5e-05,
      "loss": 0.3683,
      "step": 86
    },
    {
      "epoch": 0.10187353629976581,
      "grad_norm": 0.9259104132652283,
      "learning_rate": 5e-05,
      "loss": 0.2559,
      "step": 87
    },
    {
      "epoch": 0.10304449648711944,
      "grad_norm": 0.8723392486572266,
      "learning_rate": 5e-05,
      "loss": 0.3673,
      "step": 88
    },
    {
      "epoch": 0.10421545667447307,
      "grad_norm": 0.8708447217941284,
      "learning_rate": 5e-05,
      "loss": 0.2902,
      "step": 89
    },
    {
      "epoch": 0.1053864168618267,
      "grad_norm": 0.983888566493988,
      "learning_rate": 5e-05,
      "loss": 0.2843,
      "step": 90
    },
    {
      "epoch": 0.10655737704918032,
      "grad_norm": 1.2054803371429443,
      "learning_rate": 5e-05,
      "loss": 0.2995,
      "step": 91
    },
    {
      "epoch": 0.10772833723653395,
      "grad_norm": 2.183516025543213,
      "learning_rate": 5e-05,
      "loss": 0.9139,
      "step": 92
    },
    {
      "epoch": 0.10889929742388758,
      "grad_norm": 1.3942885398864746,
      "learning_rate": 5e-05,
      "loss": 0.3002,
      "step": 93
    },
    {
      "epoch": 0.11007025761124122,
      "grad_norm": 0.8855084776878357,
      "learning_rate": 5e-05,
      "loss": 0.1473,
      "step": 94
    },
    {
      "epoch": 0.11124121779859485,
      "grad_norm": 0.9002529382705688,
      "learning_rate": 5e-05,
      "loss": 0.3505,
      "step": 95
    },
    {
      "epoch": 0.11241217798594848,
      "grad_norm": 1.7944978475570679,
      "learning_rate": 5e-05,
      "loss": 0.4315,
      "step": 96
    },
    {
      "epoch": 0.11358313817330211,
      "grad_norm": 1.2393696308135986,
      "learning_rate": 5e-05,
      "loss": 0.2121,
      "step": 97
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 1.3673532009124756,
      "learning_rate": 5e-05,
      "loss": 0.3132,
      "step": 98
    },
    {
      "epoch": 0.11592505854800937,
      "grad_norm": 1.2279514074325562,
      "learning_rate": 5e-05,
      "loss": 0.2057,
      "step": 99
    },
    {
      "epoch": 0.117096018735363,
      "grad_norm": 1.0255098342895508,
      "learning_rate": 5e-05,
      "loss": 0.3017,
      "step": 100
    },
    {
      "epoch": 0.11826697892271663,
      "grad_norm": 1.1477552652359009,
      "learning_rate": 5e-05,
      "loss": 0.4061,
      "step": 101
    },
    {
      "epoch": 0.11943793911007025,
      "grad_norm": 1.0886532068252563,
      "learning_rate": 5e-05,
      "loss": 0.3194,
      "step": 102
    },
    {
      "epoch": 0.12060889929742388,
      "grad_norm": 1.102407693862915,
      "learning_rate": 5e-05,
      "loss": 0.4435,
      "step": 103
    },
    {
      "epoch": 0.12177985948477751,
      "grad_norm": 1.0034109354019165,
      "learning_rate": 5e-05,
      "loss": 0.2634,
      "step": 104
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 1.665128469467163,
      "learning_rate": 5e-05,
      "loss": 0.4844,
      "step": 105
    },
    {
      "epoch": 0.12412177985948478,
      "grad_norm": 0.9459792375564575,
      "learning_rate": 5e-05,
      "loss": 0.2152,
      "step": 106
    },
    {
      "epoch": 0.1252927400468384,
      "grad_norm": 1.3418608903884888,
      "learning_rate": 5e-05,
      "loss": 0.4298,
      "step": 107
    },
    {
      "epoch": 0.12646370023419204,
      "grad_norm": 0.9212440252304077,
      "learning_rate": 5e-05,
      "loss": 0.4203,
      "step": 108
    },
    {
      "epoch": 0.12763466042154567,
      "grad_norm": 1.9100618362426758,
      "learning_rate": 5e-05,
      "loss": 0.3435,
      "step": 109
    },
    {
      "epoch": 0.1288056206088993,
      "grad_norm": 1.1051409244537354,
      "learning_rate": 5e-05,
      "loss": 0.2195,
      "step": 110
    },
    {
      "epoch": 0.12997658079625293,
      "grad_norm": 1.3237727880477905,
      "learning_rate": 5e-05,
      "loss": 0.2189,
      "step": 111
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.7506828904151917,
      "learning_rate": 5e-05,
      "loss": 0.1354,
      "step": 112
    },
    {
      "epoch": 0.1323185011709602,
      "grad_norm": 0.990516722202301,
      "learning_rate": 5e-05,
      "loss": 0.183,
      "step": 113
    },
    {
      "epoch": 0.13348946135831383,
      "grad_norm": 1.1416192054748535,
      "learning_rate": 5e-05,
      "loss": 0.3126,
      "step": 114
    },
    {
      "epoch": 0.13466042154566746,
      "grad_norm": 0.7530018091201782,
      "learning_rate": 5e-05,
      "loss": 0.2411,
      "step": 115
    },
    {
      "epoch": 0.1358313817330211,
      "grad_norm": 0.7504790425300598,
      "learning_rate": 5e-05,
      "loss": 0.2315,
      "step": 116
    },
    {
      "epoch": 0.13700234192037472,
      "grad_norm": 1.077426552772522,
      "learning_rate": 5e-05,
      "loss": 0.3481,
      "step": 117
    },
    {
      "epoch": 0.13817330210772832,
      "grad_norm": 0.970994770526886,
      "learning_rate": 5e-05,
      "loss": 0.2208,
      "step": 118
    },
    {
      "epoch": 0.13934426229508196,
      "grad_norm": 1.0775337219238281,
      "learning_rate": 5e-05,
      "loss": 0.396,
      "step": 119
    },
    {
      "epoch": 0.1405152224824356,
      "grad_norm": 1.4705203771591187,
      "learning_rate": 5e-05,
      "loss": 0.4724,
      "step": 120
    },
    {
      "epoch": 0.14168618266978922,
      "grad_norm": 0.946234405040741,
      "learning_rate": 5e-05,
      "loss": 0.288,
      "step": 121
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 1.2306692600250244,
      "learning_rate": 5e-05,
      "loss": 0.3512,
      "step": 122
    },
    {
      "epoch": 0.14402810304449648,
      "grad_norm": 1.2594140768051147,
      "learning_rate": 5e-05,
      "loss": 0.363,
      "step": 123
    },
    {
      "epoch": 0.1451990632318501,
      "grad_norm": 0.6834907531738281,
      "learning_rate": 5e-05,
      "loss": 0.1669,
      "step": 124
    },
    {
      "epoch": 0.14637002341920374,
      "grad_norm": 1.0865098237991333,
      "learning_rate": 5e-05,
      "loss": 0.1726,
      "step": 125
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 0.9858837723731995,
      "learning_rate": 5e-05,
      "loss": 0.3116,
      "step": 126
    },
    {
      "epoch": 0.148711943793911,
      "grad_norm": 1.5976057052612305,
      "learning_rate": 5e-05,
      "loss": 0.2766,
      "step": 127
    },
    {
      "epoch": 0.14988290398126464,
      "grad_norm": 0.9964433908462524,
      "learning_rate": 5e-05,
      "loss": 0.2233,
      "step": 128
    },
    {
      "epoch": 0.15105386416861827,
      "grad_norm": 1.0130400657653809,
      "learning_rate": 5e-05,
      "loss": 0.2924,
      "step": 129
    },
    {
      "epoch": 0.1522248243559719,
      "grad_norm": 0.8374152779579163,
      "learning_rate": 5e-05,
      "loss": 0.2142,
      "step": 130
    },
    {
      "epoch": 0.15339578454332553,
      "grad_norm": 1.8627961874008179,
      "learning_rate": 5e-05,
      "loss": 0.2363,
      "step": 131
    },
    {
      "epoch": 0.15456674473067916,
      "grad_norm": 0.9495707750320435,
      "learning_rate": 5e-05,
      "loss": 0.2211,
      "step": 132
    },
    {
      "epoch": 0.1557377049180328,
      "grad_norm": 1.1363391876220703,
      "learning_rate": 5e-05,
      "loss": 0.2663,
      "step": 133
    },
    {
      "epoch": 0.15690866510538642,
      "grad_norm": 1.8523123264312744,
      "learning_rate": 5e-05,
      "loss": 0.294,
      "step": 134
    },
    {
      "epoch": 0.15807962529274006,
      "grad_norm": 0.9635983109474182,
      "learning_rate": 5e-05,
      "loss": 0.2771,
      "step": 135
    },
    {
      "epoch": 0.1592505854800937,
      "grad_norm": 1.2837634086608887,
      "learning_rate": 5e-05,
      "loss": 0.257,
      "step": 136
    },
    {
      "epoch": 0.16042154566744732,
      "grad_norm": 0.7208678722381592,
      "learning_rate": 5e-05,
      "loss": 0.2924,
      "step": 137
    },
    {
      "epoch": 0.16159250585480095,
      "grad_norm": 0.8757469654083252,
      "learning_rate": 5e-05,
      "loss": 0.1389,
      "step": 138
    },
    {
      "epoch": 0.16276346604215455,
      "grad_norm": 1.246473789215088,
      "learning_rate": 5e-05,
      "loss": 0.1903,
      "step": 139
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 1.4772562980651855,
      "learning_rate": 5e-05,
      "loss": 0.3454,
      "step": 140
    },
    {
      "epoch": 0.16510538641686182,
      "grad_norm": 2.0078225135803223,
      "learning_rate": 5e-05,
      "loss": 0.466,
      "step": 141
    },
    {
      "epoch": 0.16627634660421545,
      "grad_norm": 0.7733511328697205,
      "learning_rate": 5e-05,
      "loss": 0.102,
      "step": 142
    },
    {
      "epoch": 0.16744730679156908,
      "grad_norm": 0.98130202293396,
      "learning_rate": 5e-05,
      "loss": 0.1763,
      "step": 143
    },
    {
      "epoch": 0.1686182669789227,
      "grad_norm": 0.8089781403541565,
      "learning_rate": 5e-05,
      "loss": 0.1606,
      "step": 144
    },
    {
      "epoch": 0.16978922716627634,
      "grad_norm": 0.9576601386070251,
      "learning_rate": 5e-05,
      "loss": 0.2063,
      "step": 145
    },
    {
      "epoch": 0.17096018735362997,
      "grad_norm": 1.0232213735580444,
      "learning_rate": 5e-05,
      "loss": 0.2442,
      "step": 146
    },
    {
      "epoch": 0.1721311475409836,
      "grad_norm": 1.2648009061813354,
      "learning_rate": 5e-05,
      "loss": 0.1398,
      "step": 147
    },
    {
      "epoch": 0.17330210772833723,
      "grad_norm": 1.3136746883392334,
      "learning_rate": 5e-05,
      "loss": 0.3401,
      "step": 148
    },
    {
      "epoch": 0.17447306791569087,
      "grad_norm": 1.0538727045059204,
      "learning_rate": 5e-05,
      "loss": 0.3166,
      "step": 149
    },
    {
      "epoch": 0.1756440281030445,
      "grad_norm": 1.000367522239685,
      "learning_rate": 5e-05,
      "loss": 0.1728,
      "step": 150
    },
    {
      "epoch": 0.17681498829039813,
      "grad_norm": 1.0589829683303833,
      "learning_rate": 5e-05,
      "loss": 0.2404,
      "step": 151
    },
    {
      "epoch": 0.17798594847775176,
      "grad_norm": 0.9919997453689575,
      "learning_rate": 5e-05,
      "loss": 0.2555,
      "step": 152
    },
    {
      "epoch": 0.1791569086651054,
      "grad_norm": 1.3029143810272217,
      "learning_rate": 5e-05,
      "loss": 0.2213,
      "step": 153
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 1.681970477104187,
      "learning_rate": 5e-05,
      "loss": 0.3951,
      "step": 154
    },
    {
      "epoch": 0.18149882903981265,
      "grad_norm": 0.7877441048622131,
      "learning_rate": 5e-05,
      "loss": 0.1299,
      "step": 155
    },
    {
      "epoch": 0.18266978922716628,
      "grad_norm": 0.9595867395401001,
      "learning_rate": 5e-05,
      "loss": 0.2688,
      "step": 156
    },
    {
      "epoch": 0.18384074941451992,
      "grad_norm": 1.201863169670105,
      "learning_rate": 5e-05,
      "loss": 0.2965,
      "step": 157
    },
    {
      "epoch": 0.18501170960187355,
      "grad_norm": 0.9266951084136963,
      "learning_rate": 5e-05,
      "loss": 0.2326,
      "step": 158
    },
    {
      "epoch": 0.18618266978922718,
      "grad_norm": 0.7232947945594788,
      "learning_rate": 5e-05,
      "loss": 0.2477,
      "step": 159
    },
    {
      "epoch": 0.1873536299765808,
      "grad_norm": 0.6637865900993347,
      "learning_rate": 5e-05,
      "loss": 0.1989,
      "step": 160
    },
    {
      "epoch": 0.1885245901639344,
      "grad_norm": 0.9040093421936035,
      "learning_rate": 5e-05,
      "loss": 0.1508,
      "step": 161
    },
    {
      "epoch": 0.18969555035128804,
      "grad_norm": 1.1620315313339233,
      "learning_rate": 5e-05,
      "loss": 0.4197,
      "step": 162
    },
    {
      "epoch": 0.19086651053864168,
      "grad_norm": 1.2110612392425537,
      "learning_rate": 5e-05,
      "loss": 0.3234,
      "step": 163
    },
    {
      "epoch": 0.1920374707259953,
      "grad_norm": 0.8113008737564087,
      "learning_rate": 5e-05,
      "loss": 0.2498,
      "step": 164
    },
    {
      "epoch": 0.19320843091334894,
      "grad_norm": 0.8136644959449768,
      "learning_rate": 5e-05,
      "loss": 0.3921,
      "step": 165
    },
    {
      "epoch": 0.19437939110070257,
      "grad_norm": 1.5449823141098022,
      "learning_rate": 5e-05,
      "loss": 0.2193,
      "step": 166
    },
    {
      "epoch": 0.1955503512880562,
      "grad_norm": 1.138514518737793,
      "learning_rate": 5e-05,
      "loss": 0.3334,
      "step": 167
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 0.9688778519630432,
      "learning_rate": 5e-05,
      "loss": 0.2683,
      "step": 168
    },
    {
      "epoch": 0.19789227166276346,
      "grad_norm": 0.9400694370269775,
      "learning_rate": 5e-05,
      "loss": 0.1624,
      "step": 169
    },
    {
      "epoch": 0.1990632318501171,
      "grad_norm": 1.1604691743850708,
      "learning_rate": 5e-05,
      "loss": 0.3175,
      "step": 170
    },
    {
      "epoch": 0.20023419203747073,
      "grad_norm": 1.0997498035430908,
      "learning_rate": 5e-05,
      "loss": 0.1807,
      "step": 171
    },
    {
      "epoch": 0.20140515222482436,
      "grad_norm": 0.7449932098388672,
      "learning_rate": 5e-05,
      "loss": 0.1979,
      "step": 172
    },
    {
      "epoch": 0.202576112412178,
      "grad_norm": 0.9396167993545532,
      "learning_rate": 5e-05,
      "loss": 0.245,
      "step": 173
    },
    {
      "epoch": 0.20374707259953162,
      "grad_norm": 1.376828908920288,
      "learning_rate": 5e-05,
      "loss": 0.3154,
      "step": 174
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 1.1673763990402222,
      "learning_rate": 5e-05,
      "loss": 0.2043,
      "step": 175
    },
    {
      "epoch": 0.20608899297423888,
      "grad_norm": 1.0965595245361328,
      "learning_rate": 5e-05,
      "loss": 0.2246,
      "step": 176
    },
    {
      "epoch": 0.20725995316159251,
      "grad_norm": 0.7784209847450256,
      "learning_rate": 5e-05,
      "loss": 0.1937,
      "step": 177
    },
    {
      "epoch": 0.20843091334894615,
      "grad_norm": 0.9792616367340088,
      "learning_rate": 5e-05,
      "loss": 0.2399,
      "step": 178
    },
    {
      "epoch": 0.20960187353629978,
      "grad_norm": 1.1811697483062744,
      "learning_rate": 5e-05,
      "loss": 0.2584,
      "step": 179
    },
    {
      "epoch": 0.2107728337236534,
      "grad_norm": 0.7376151084899902,
      "learning_rate": 5e-05,
      "loss": 0.3089,
      "step": 180
    },
    {
      "epoch": 0.21194379391100704,
      "grad_norm": 0.7926422953605652,
      "learning_rate": 5e-05,
      "loss": 0.1448,
      "step": 181
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 1.0016496181488037,
      "learning_rate": 5e-05,
      "loss": 0.2778,
      "step": 182
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 1.2218585014343262,
      "learning_rate": 5e-05,
      "loss": 0.4156,
      "step": 183
    },
    {
      "epoch": 0.2154566744730679,
      "grad_norm": 0.6968467831611633,
      "learning_rate": 5e-05,
      "loss": 0.1027,
      "step": 184
    },
    {
      "epoch": 0.21662763466042154,
      "grad_norm": 1.1796164512634277,
      "learning_rate": 5e-05,
      "loss": 0.2788,
      "step": 185
    },
    {
      "epoch": 0.21779859484777517,
      "grad_norm": 0.9675211310386658,
      "learning_rate": 5e-05,
      "loss": 0.2337,
      "step": 186
    },
    {
      "epoch": 0.2189695550351288,
      "grad_norm": 0.6222013235092163,
      "learning_rate": 5e-05,
      "loss": 0.1232,
      "step": 187
    },
    {
      "epoch": 0.22014051522248243,
      "grad_norm": 0.9134336113929749,
      "learning_rate": 5e-05,
      "loss": 0.294,
      "step": 188
    },
    {
      "epoch": 0.22131147540983606,
      "grad_norm": 1.274609923362732,
      "learning_rate": 5e-05,
      "loss": 0.3241,
      "step": 189
    },
    {
      "epoch": 0.2224824355971897,
      "grad_norm": 0.9018095135688782,
      "learning_rate": 5e-05,
      "loss": 0.2082,
      "step": 190
    },
    {
      "epoch": 0.22365339578454332,
      "grad_norm": 0.8891756534576416,
      "learning_rate": 5e-05,
      "loss": 0.2792,
      "step": 191
    },
    {
      "epoch": 0.22482435597189696,
      "grad_norm": 1.2493582963943481,
      "learning_rate": 5e-05,
      "loss": 0.2394,
      "step": 192
    },
    {
      "epoch": 0.2259953161592506,
      "grad_norm": 0.9984263181686401,
      "learning_rate": 5e-05,
      "loss": 0.3171,
      "step": 193
    },
    {
      "epoch": 0.22716627634660422,
      "grad_norm": 0.9473557472229004,
      "learning_rate": 5e-05,
      "loss": 0.2274,
      "step": 194
    },
    {
      "epoch": 0.22833723653395785,
      "grad_norm": 0.8326888084411621,
      "learning_rate": 5e-05,
      "loss": 0.2341,
      "step": 195
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 0.7808160185813904,
      "learning_rate": 5e-05,
      "loss": 0.1897,
      "step": 196
    },
    {
      "epoch": 0.2306791569086651,
      "grad_norm": 1.0248562097549438,
      "learning_rate": 5e-05,
      "loss": 0.1591,
      "step": 197
    },
    {
      "epoch": 0.23185011709601874,
      "grad_norm": 0.8167499899864197,
      "learning_rate": 5e-05,
      "loss": 0.1022,
      "step": 198
    },
    {
      "epoch": 0.23302107728337237,
      "grad_norm": 0.8919475078582764,
      "learning_rate": 5e-05,
      "loss": 0.1604,
      "step": 199
    },
    {
      "epoch": 0.234192037470726,
      "grad_norm": 1.0866445302963257,
      "learning_rate": 5e-05,
      "loss": 0.2931,
      "step": 200
    },
    {
      "epoch": 0.23536299765807964,
      "grad_norm": 0.9450720548629761,
      "learning_rate": 5e-05,
      "loss": 0.2046,
      "step": 201
    },
    {
      "epoch": 0.23653395784543327,
      "grad_norm": 0.6596251726150513,
      "learning_rate": 5e-05,
      "loss": 0.2404,
      "step": 202
    },
    {
      "epoch": 0.23770491803278687,
      "grad_norm": 1.2466917037963867,
      "learning_rate": 5e-05,
      "loss": 0.2045,
      "step": 203
    },
    {
      "epoch": 0.2388758782201405,
      "grad_norm": 0.9321942925453186,
      "learning_rate": 5e-05,
      "loss": 0.2128,
      "step": 204
    },
    {
      "epoch": 0.24004683840749413,
      "grad_norm": 1.048833966255188,
      "learning_rate": 5e-05,
      "loss": 0.1779,
      "step": 205
    },
    {
      "epoch": 0.24121779859484777,
      "grad_norm": 0.7484830617904663,
      "learning_rate": 5e-05,
      "loss": 0.1153,
      "step": 206
    },
    {
      "epoch": 0.2423887587822014,
      "grad_norm": 0.8501813411712646,
      "learning_rate": 5e-05,
      "loss": 0.2475,
      "step": 207
    },
    {
      "epoch": 0.24355971896955503,
      "grad_norm": 1.1287248134613037,
      "learning_rate": 5e-05,
      "loss": 0.2979,
      "step": 208
    },
    {
      "epoch": 0.24473067915690866,
      "grad_norm": 1.1520978212356567,
      "learning_rate": 5e-05,
      "loss": 0.3068,
      "step": 209
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 1.206432819366455,
      "learning_rate": 5e-05,
      "loss": 0.1797,
      "step": 210
    },
    {
      "epoch": 0.24707259953161592,
      "grad_norm": 0.682962954044342,
      "learning_rate": 5e-05,
      "loss": 0.1716,
      "step": 211
    },
    {
      "epoch": 0.24824355971896955,
      "grad_norm": 0.9562180042266846,
      "learning_rate": 5e-05,
      "loss": 0.2294,
      "step": 212
    },
    {
      "epoch": 0.24941451990632318,
      "grad_norm": 0.7823658585548401,
      "learning_rate": 5e-05,
      "loss": 0.1936,
      "step": 213
    },
    {
      "epoch": 0.2505854800936768,
      "grad_norm": 1.150221824645996,
      "learning_rate": 5e-05,
      "loss": 0.2073,
      "step": 214
    },
    {
      "epoch": 0.25175644028103045,
      "grad_norm": 1.4074203968048096,
      "learning_rate": 5e-05,
      "loss": 0.2453,
      "step": 215
    },
    {
      "epoch": 0.2529274004683841,
      "grad_norm": 1.8127135038375854,
      "learning_rate": 5e-05,
      "loss": 0.3882,
      "step": 216
    },
    {
      "epoch": 0.2540983606557377,
      "grad_norm": 0.9683912396430969,
      "learning_rate": 5e-05,
      "loss": 0.19,
      "step": 217
    },
    {
      "epoch": 0.25526932084309134,
      "grad_norm": 1.5371246337890625,
      "learning_rate": 5e-05,
      "loss": 0.3048,
      "step": 218
    },
    {
      "epoch": 0.25644028103044497,
      "grad_norm": 1.544954776763916,
      "learning_rate": 5e-05,
      "loss": 0.4341,
      "step": 219
    },
    {
      "epoch": 0.2576112412177986,
      "grad_norm": 1.094970941543579,
      "learning_rate": 5e-05,
      "loss": 0.2942,
      "step": 220
    },
    {
      "epoch": 0.25878220140515223,
      "grad_norm": 1.1210153102874756,
      "learning_rate": 5e-05,
      "loss": 0.4205,
      "step": 221
    },
    {
      "epoch": 0.25995316159250587,
      "grad_norm": 1.0136773586273193,
      "learning_rate": 5e-05,
      "loss": 0.23,
      "step": 222
    },
    {
      "epoch": 0.2611241217798595,
      "grad_norm": 0.9704874753952026,
      "learning_rate": 5e-05,
      "loss": 0.3065,
      "step": 223
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 1.1874419450759888,
      "learning_rate": 5e-05,
      "loss": 0.1452,
      "step": 224
    },
    {
      "epoch": 0.26346604215456676,
      "grad_norm": 0.9110910892486572,
      "learning_rate": 5e-05,
      "loss": 0.2585,
      "step": 225
    },
    {
      "epoch": 0.2646370023419204,
      "grad_norm": 1.1587944030761719,
      "learning_rate": 5e-05,
      "loss": 0.2095,
      "step": 226
    },
    {
      "epoch": 0.265807962529274,
      "grad_norm": 0.6877738237380981,
      "learning_rate": 5e-05,
      "loss": 0.1245,
      "step": 227
    },
    {
      "epoch": 0.26697892271662765,
      "grad_norm": 0.7449968457221985,
      "learning_rate": 5e-05,
      "loss": 0.3001,
      "step": 228
    },
    {
      "epoch": 0.2681498829039813,
      "grad_norm": 0.6839293241500854,
      "learning_rate": 5e-05,
      "loss": 0.2281,
      "step": 229
    },
    {
      "epoch": 0.2693208430913349,
      "grad_norm": 0.7611244320869446,
      "learning_rate": 5e-05,
      "loss": 0.2178,
      "step": 230
    },
    {
      "epoch": 0.27049180327868855,
      "grad_norm": 0.6935118436813354,
      "learning_rate": 5e-05,
      "loss": 0.1598,
      "step": 231
    },
    {
      "epoch": 0.2716627634660422,
      "grad_norm": 0.7126134634017944,
      "learning_rate": 5e-05,
      "loss": 0.2948,
      "step": 232
    },
    {
      "epoch": 0.2728337236533958,
      "grad_norm": 0.8086540102958679,
      "learning_rate": 5e-05,
      "loss": 0.3211,
      "step": 233
    },
    {
      "epoch": 0.27400468384074944,
      "grad_norm": 1.1399775743484497,
      "learning_rate": 5e-05,
      "loss": 0.3535,
      "step": 234
    },
    {
      "epoch": 0.275175644028103,
      "grad_norm": 1.0557522773742676,
      "learning_rate": 5e-05,
      "loss": 0.2531,
      "step": 235
    },
    {
      "epoch": 0.27634660421545665,
      "grad_norm": 0.8500572443008423,
      "learning_rate": 5e-05,
      "loss": 0.1955,
      "step": 236
    },
    {
      "epoch": 0.2775175644028103,
      "grad_norm": 0.915176272392273,
      "learning_rate": 5e-05,
      "loss": 0.157,
      "step": 237
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 0.7393614649772644,
      "learning_rate": 5e-05,
      "loss": 0.1051,
      "step": 238
    },
    {
      "epoch": 0.27985948477751754,
      "grad_norm": 1.0239553451538086,
      "learning_rate": 5e-05,
      "loss": 0.2517,
      "step": 239
    },
    {
      "epoch": 0.2810304449648712,
      "grad_norm": 0.760588526725769,
      "learning_rate": 5e-05,
      "loss": 0.1226,
      "step": 240
    },
    {
      "epoch": 0.2822014051522248,
      "grad_norm": 1.0622061491012573,
      "learning_rate": 5e-05,
      "loss": 0.3416,
      "step": 241
    },
    {
      "epoch": 0.28337236533957844,
      "grad_norm": 1.2398967742919922,
      "learning_rate": 5e-05,
      "loss": 0.2661,
      "step": 242
    },
    {
      "epoch": 0.28454332552693207,
      "grad_norm": 0.8569233417510986,
      "learning_rate": 5e-05,
      "loss": 0.1843,
      "step": 243
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 1.37456476688385,
      "learning_rate": 5e-05,
      "loss": 0.3172,
      "step": 244
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 0.9012590050697327,
      "learning_rate": 5e-05,
      "loss": 0.2704,
      "step": 245
    },
    {
      "epoch": 0.28805620608899296,
      "grad_norm": 0.8354252576828003,
      "learning_rate": 5e-05,
      "loss": 0.1728,
      "step": 246
    },
    {
      "epoch": 0.2892271662763466,
      "grad_norm": 1.297012448310852,
      "learning_rate": 5e-05,
      "loss": 0.2766,
      "step": 247
    },
    {
      "epoch": 0.2903981264637002,
      "grad_norm": 0.6678341031074524,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 248
    },
    {
      "epoch": 0.29156908665105385,
      "grad_norm": 0.933621883392334,
      "learning_rate": 5e-05,
      "loss": 0.2281,
      "step": 249
    },
    {
      "epoch": 0.2927400468384075,
      "grad_norm": 1.3230634927749634,
      "learning_rate": 5e-05,
      "loss": 0.2549,
      "step": 250
    },
    {
      "epoch": 0.2939110070257611,
      "grad_norm": 1.3713877201080322,
      "learning_rate": 5e-05,
      "loss": 0.2397,
      "step": 251
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 0.8243020176887512,
      "learning_rate": 5e-05,
      "loss": 0.1169,
      "step": 252
    },
    {
      "epoch": 0.2962529274004684,
      "grad_norm": 0.6657607555389404,
      "learning_rate": 5e-05,
      "loss": 0.1507,
      "step": 253
    },
    {
      "epoch": 0.297423887587822,
      "grad_norm": 0.765622079372406,
      "learning_rate": 5e-05,
      "loss": 0.2194,
      "step": 254
    },
    {
      "epoch": 0.29859484777517564,
      "grad_norm": 0.8315567374229431,
      "learning_rate": 5e-05,
      "loss": 0.2099,
      "step": 255
    },
    {
      "epoch": 0.2997658079625293,
      "grad_norm": 0.8524840474128723,
      "learning_rate": 5e-05,
      "loss": 0.2905,
      "step": 256
    },
    {
      "epoch": 0.3009367681498829,
      "grad_norm": 2.2772209644317627,
      "learning_rate": 5e-05,
      "loss": 0.3801,
      "step": 257
    },
    {
      "epoch": 0.30210772833723654,
      "grad_norm": 0.47979679703712463,
      "learning_rate": 5e-05,
      "loss": 0.1096,
      "step": 258
    },
    {
      "epoch": 0.30327868852459017,
      "grad_norm": 1.2078590393066406,
      "learning_rate": 5e-05,
      "loss": 0.1764,
      "step": 259
    },
    {
      "epoch": 0.3044496487119438,
      "grad_norm": 0.9354492425918579,
      "learning_rate": 5e-05,
      "loss": 0.1467,
      "step": 260
    },
    {
      "epoch": 0.30562060889929743,
      "grad_norm": 0.799056887626648,
      "learning_rate": 5e-05,
      "loss": 0.1262,
      "step": 261
    },
    {
      "epoch": 0.30679156908665106,
      "grad_norm": 0.5459614992141724,
      "learning_rate": 5e-05,
      "loss": 0.1003,
      "step": 262
    },
    {
      "epoch": 0.3079625292740047,
      "grad_norm": 1.0567651987075806,
      "learning_rate": 5e-05,
      "loss": 0.2288,
      "step": 263
    },
    {
      "epoch": 0.3091334894613583,
      "grad_norm": 1.2403429746627808,
      "learning_rate": 5e-05,
      "loss": 0.272,
      "step": 264
    },
    {
      "epoch": 0.31030444964871196,
      "grad_norm": 1.2207177877426147,
      "learning_rate": 5e-05,
      "loss": 0.2572,
      "step": 265
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 0.9986621737480164,
      "learning_rate": 5e-05,
      "loss": 0.1782,
      "step": 266
    },
    {
      "epoch": 0.3126463700234192,
      "grad_norm": 1.4754072427749634,
      "learning_rate": 5e-05,
      "loss": 0.2232,
      "step": 267
    },
    {
      "epoch": 0.31381733021077285,
      "grad_norm": 0.8779364824295044,
      "learning_rate": 5e-05,
      "loss": 0.1987,
      "step": 268
    },
    {
      "epoch": 0.3149882903981265,
      "grad_norm": 1.1795607805252075,
      "learning_rate": 5e-05,
      "loss": 0.3407,
      "step": 269
    },
    {
      "epoch": 0.3161592505854801,
      "grad_norm": 0.859438955783844,
      "learning_rate": 5e-05,
      "loss": 0.1105,
      "step": 270
    },
    {
      "epoch": 0.31733021077283374,
      "grad_norm": 1.025344967842102,
      "learning_rate": 5e-05,
      "loss": 0.2269,
      "step": 271
    },
    {
      "epoch": 0.3185011709601874,
      "grad_norm": 0.8202717900276184,
      "learning_rate": 5e-05,
      "loss": 0.218,
      "step": 272
    },
    {
      "epoch": 0.319672131147541,
      "grad_norm": 0.9027923941612244,
      "learning_rate": 5e-05,
      "loss": 0.1567,
      "step": 273
    },
    {
      "epoch": 0.32084309133489464,
      "grad_norm": 0.8610377311706543,
      "learning_rate": 5e-05,
      "loss": 0.1491,
      "step": 274
    },
    {
      "epoch": 0.32201405152224827,
      "grad_norm": 1.7470729351043701,
      "learning_rate": 5e-05,
      "loss": 0.1937,
      "step": 275
    },
    {
      "epoch": 0.3231850117096019,
      "grad_norm": 0.8936634659767151,
      "learning_rate": 5e-05,
      "loss": 0.1979,
      "step": 276
    },
    {
      "epoch": 0.32435597189695553,
      "grad_norm": 1.1828227043151855,
      "learning_rate": 5e-05,
      "loss": 0.346,
      "step": 277
    },
    {
      "epoch": 0.3255269320843091,
      "grad_norm": 1.239130973815918,
      "learning_rate": 5e-05,
      "loss": 0.1425,
      "step": 278
    },
    {
      "epoch": 0.32669789227166274,
      "grad_norm": 1.4430780410766602,
      "learning_rate": 5e-05,
      "loss": 0.2041,
      "step": 279
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 0.9449899792671204,
      "learning_rate": 5e-05,
      "loss": 0.1829,
      "step": 280
    },
    {
      "epoch": 0.32903981264637,
      "grad_norm": 1.283890724182129,
      "learning_rate": 5e-05,
      "loss": 0.1423,
      "step": 281
    },
    {
      "epoch": 0.33021077283372363,
      "grad_norm": 0.9231147766113281,
      "learning_rate": 5e-05,
      "loss": 0.2246,
      "step": 282
    },
    {
      "epoch": 0.33138173302107726,
      "grad_norm": 0.8577829003334045,
      "learning_rate": 5e-05,
      "loss": 0.26,
      "step": 283
    },
    {
      "epoch": 0.3325526932084309,
      "grad_norm": 0.6862982511520386,
      "learning_rate": 5e-05,
      "loss": 0.1143,
      "step": 284
    },
    {
      "epoch": 0.3337236533957845,
      "grad_norm": 0.8107389807701111,
      "learning_rate": 5e-05,
      "loss": 0.1032,
      "step": 285
    },
    {
      "epoch": 0.33489461358313816,
      "grad_norm": 1.5395567417144775,
      "learning_rate": 5e-05,
      "loss": 0.2646,
      "step": 286
    },
    {
      "epoch": 0.3360655737704918,
      "grad_norm": 0.8109204173088074,
      "learning_rate": 5e-05,
      "loss": 0.1194,
      "step": 287
    },
    {
      "epoch": 0.3372365339578454,
      "grad_norm": 0.9203944802284241,
      "learning_rate": 5e-05,
      "loss": 0.1987,
      "step": 288
    },
    {
      "epoch": 0.33840749414519905,
      "grad_norm": 1.8398029804229736,
      "learning_rate": 5e-05,
      "loss": 0.409,
      "step": 289
    },
    {
      "epoch": 0.3395784543325527,
      "grad_norm": 1.7188653945922852,
      "learning_rate": 5e-05,
      "loss": 0.2235,
      "step": 290
    },
    {
      "epoch": 0.3407494145199063,
      "grad_norm": 1.224403738975525,
      "learning_rate": 5e-05,
      "loss": 0.2037,
      "step": 291
    },
    {
      "epoch": 0.34192037470725994,
      "grad_norm": 1.4987932443618774,
      "learning_rate": 5e-05,
      "loss": 0.4123,
      "step": 292
    },
    {
      "epoch": 0.3430913348946136,
      "grad_norm": 1.1789172887802124,
      "learning_rate": 5e-05,
      "loss": 0.2841,
      "step": 293
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 1.1942415237426758,
      "learning_rate": 5e-05,
      "loss": 0.2931,
      "step": 294
    },
    {
      "epoch": 0.34543325526932084,
      "grad_norm": 0.9301571249961853,
      "learning_rate": 5e-05,
      "loss": 0.3203,
      "step": 295
    },
    {
      "epoch": 0.34660421545667447,
      "grad_norm": 1.027997612953186,
      "learning_rate": 5e-05,
      "loss": 0.2353,
      "step": 296
    },
    {
      "epoch": 0.3477751756440281,
      "grad_norm": 0.9813650250434875,
      "learning_rate": 5e-05,
      "loss": 0.1613,
      "step": 297
    },
    {
      "epoch": 0.34894613583138173,
      "grad_norm": 1.0435956716537476,
      "learning_rate": 5e-05,
      "loss": 0.2033,
      "step": 298
    },
    {
      "epoch": 0.35011709601873536,
      "grad_norm": 0.9507393836975098,
      "learning_rate": 5e-05,
      "loss": 0.2369,
      "step": 299
    },
    {
      "epoch": 0.351288056206089,
      "grad_norm": 1.4567828178405762,
      "learning_rate": 5e-05,
      "loss": 0.3378,
      "step": 300
    },
    {
      "epoch": 0.3524590163934426,
      "grad_norm": 0.7720144987106323,
      "learning_rate": 5e-05,
      "loss": 0.1859,
      "step": 301
    },
    {
      "epoch": 0.35362997658079626,
      "grad_norm": 1.4743218421936035,
      "learning_rate": 5e-05,
      "loss": 0.123,
      "step": 302
    },
    {
      "epoch": 0.3548009367681499,
      "grad_norm": 0.869155764579773,
      "learning_rate": 5e-05,
      "loss": 0.1377,
      "step": 303
    },
    {
      "epoch": 0.3559718969555035,
      "grad_norm": 0.9531005620956421,
      "learning_rate": 5e-05,
      "loss": 0.2915,
      "step": 304
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 1.1777163743972778,
      "learning_rate": 5e-05,
      "loss": 0.2074,
      "step": 305
    },
    {
      "epoch": 0.3583138173302108,
      "grad_norm": 0.9528010487556458,
      "learning_rate": 5e-05,
      "loss": 0.2142,
      "step": 306
    },
    {
      "epoch": 0.3594847775175644,
      "grad_norm": 1.1649235486984253,
      "learning_rate": 5e-05,
      "loss": 0.2377,
      "step": 307
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 0.9549082517623901,
      "learning_rate": 5e-05,
      "loss": 0.3055,
      "step": 308
    },
    {
      "epoch": 0.3618266978922717,
      "grad_norm": 0.9701464176177979,
      "learning_rate": 5e-05,
      "loss": 0.2728,
      "step": 309
    },
    {
      "epoch": 0.3629976580796253,
      "grad_norm": 1.1790883541107178,
      "learning_rate": 5e-05,
      "loss": 0.1494,
      "step": 310
    },
    {
      "epoch": 0.36416861826697894,
      "grad_norm": 1.29237961769104,
      "learning_rate": 5e-05,
      "loss": 0.1954,
      "step": 311
    },
    {
      "epoch": 0.36533957845433257,
      "grad_norm": 0.9188796877861023,
      "learning_rate": 5e-05,
      "loss": 0.2634,
      "step": 312
    },
    {
      "epoch": 0.3665105386416862,
      "grad_norm": 0.9025296568870544,
      "learning_rate": 5e-05,
      "loss": 0.1912,
      "step": 313
    },
    {
      "epoch": 0.36768149882903983,
      "grad_norm": 1.7011607885360718,
      "learning_rate": 5e-05,
      "loss": 0.2775,
      "step": 314
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 1.1529324054718018,
      "learning_rate": 5e-05,
      "loss": 0.2172,
      "step": 315
    },
    {
      "epoch": 0.3700234192037471,
      "grad_norm": 1.229529619216919,
      "learning_rate": 5e-05,
      "loss": 0.1363,
      "step": 316
    },
    {
      "epoch": 0.3711943793911007,
      "grad_norm": 0.8254060745239258,
      "learning_rate": 5e-05,
      "loss": 0.1386,
      "step": 317
    },
    {
      "epoch": 0.37236533957845436,
      "grad_norm": 1.4025663137435913,
      "learning_rate": 5e-05,
      "loss": 0.1983,
      "step": 318
    },
    {
      "epoch": 0.373536299765808,
      "grad_norm": 1.114466667175293,
      "learning_rate": 5e-05,
      "loss": 0.1363,
      "step": 319
    },
    {
      "epoch": 0.3747072599531616,
      "grad_norm": 2.041804075241089,
      "learning_rate": 5e-05,
      "loss": 0.4173,
      "step": 320
    },
    {
      "epoch": 0.3758782201405152,
      "grad_norm": 1.2074681520462036,
      "learning_rate": 5e-05,
      "loss": 0.2291,
      "step": 321
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 0.9786560535430908,
      "learning_rate": 5e-05,
      "loss": 0.2381,
      "step": 322
    },
    {
      "epoch": 0.37822014051522246,
      "grad_norm": 1.0459152460098267,
      "learning_rate": 5e-05,
      "loss": 0.1936,
      "step": 323
    },
    {
      "epoch": 0.3793911007025761,
      "grad_norm": 1.1411386728286743,
      "learning_rate": 5e-05,
      "loss": 0.3436,
      "step": 324
    },
    {
      "epoch": 0.3805620608899297,
      "grad_norm": 1.1492305994033813,
      "learning_rate": 5e-05,
      "loss": 0.1959,
      "step": 325
    },
    {
      "epoch": 0.38173302107728335,
      "grad_norm": 0.8301316499710083,
      "learning_rate": 5e-05,
      "loss": 0.1534,
      "step": 326
    },
    {
      "epoch": 0.382903981264637,
      "grad_norm": 1.0834158658981323,
      "learning_rate": 5e-05,
      "loss": 0.2846,
      "step": 327
    },
    {
      "epoch": 0.3840749414519906,
      "grad_norm": 0.9292325377464294,
      "learning_rate": 5e-05,
      "loss": 0.3479,
      "step": 328
    },
    {
      "epoch": 0.38524590163934425,
      "grad_norm": 0.6251810789108276,
      "learning_rate": 5e-05,
      "loss": 0.1725,
      "step": 329
    },
    {
      "epoch": 0.3864168618266979,
      "grad_norm": 0.9972607493400574,
      "learning_rate": 5e-05,
      "loss": 0.2203,
      "step": 330
    },
    {
      "epoch": 0.3875878220140515,
      "grad_norm": 0.7853621244430542,
      "learning_rate": 5e-05,
      "loss": 0.2553,
      "step": 331
    },
    {
      "epoch": 0.38875878220140514,
      "grad_norm": 0.6279919743537903,
      "learning_rate": 5e-05,
      "loss": 0.1021,
      "step": 332
    },
    {
      "epoch": 0.38992974238875877,
      "grad_norm": 1.0032414197921753,
      "learning_rate": 5e-05,
      "loss": 0.2164,
      "step": 333
    },
    {
      "epoch": 0.3911007025761124,
      "grad_norm": 0.8329780101776123,
      "learning_rate": 5e-05,
      "loss": 0.3033,
      "step": 334
    },
    {
      "epoch": 0.39227166276346603,
      "grad_norm": 0.6353895664215088,
      "learning_rate": 5e-05,
      "loss": 0.1864,
      "step": 335
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.4992120563983917,
      "learning_rate": 5e-05,
      "loss": 0.0965,
      "step": 336
    },
    {
      "epoch": 0.3946135831381733,
      "grad_norm": 1.0370068550109863,
      "learning_rate": 5e-05,
      "loss": 0.1909,
      "step": 337
    },
    {
      "epoch": 0.3957845433255269,
      "grad_norm": 1.6742137670516968,
      "learning_rate": 5e-05,
      "loss": 0.2628,
      "step": 338
    },
    {
      "epoch": 0.39695550351288056,
      "grad_norm": 1.6607729196548462,
      "learning_rate": 5e-05,
      "loss": 0.2998,
      "step": 339
    },
    {
      "epoch": 0.3981264637002342,
      "grad_norm": 0.8551661968231201,
      "learning_rate": 5e-05,
      "loss": 0.22,
      "step": 340
    },
    {
      "epoch": 0.3992974238875878,
      "grad_norm": 0.8314205408096313,
      "learning_rate": 5e-05,
      "loss": 0.2662,
      "step": 341
    },
    {
      "epoch": 0.40046838407494145,
      "grad_norm": 0.7891259789466858,
      "learning_rate": 5e-05,
      "loss": 0.1686,
      "step": 342
    },
    {
      "epoch": 0.4016393442622951,
      "grad_norm": 1.0930724143981934,
      "learning_rate": 5e-05,
      "loss": 0.2054,
      "step": 343
    },
    {
      "epoch": 0.4028103044496487,
      "grad_norm": 0.7989704608917236,
      "learning_rate": 5e-05,
      "loss": 0.1673,
      "step": 344
    },
    {
      "epoch": 0.40398126463700235,
      "grad_norm": 0.8446422815322876,
      "learning_rate": 5e-05,
      "loss": 0.2201,
      "step": 345
    },
    {
      "epoch": 0.405152224824356,
      "grad_norm": 1.027624249458313,
      "learning_rate": 5e-05,
      "loss": 0.1966,
      "step": 346
    },
    {
      "epoch": 0.4063231850117096,
      "grad_norm": 1.0720783472061157,
      "learning_rate": 5e-05,
      "loss": 0.2192,
      "step": 347
    },
    {
      "epoch": 0.40749414519906324,
      "grad_norm": 1.1158053874969482,
      "learning_rate": 5e-05,
      "loss": 0.3292,
      "step": 348
    },
    {
      "epoch": 0.40866510538641687,
      "grad_norm": 0.8369395732879639,
      "learning_rate": 5e-05,
      "loss": 0.127,
      "step": 349
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 1.3913376331329346,
      "learning_rate": 5e-05,
      "loss": 0.1848,
      "step": 350
    },
    {
      "epoch": 0.41100702576112413,
      "grad_norm": 1.0946873426437378,
      "learning_rate": 5e-05,
      "loss": 0.2113,
      "step": 351
    },
    {
      "epoch": 0.41217798594847777,
      "grad_norm": 0.8676327466964722,
      "learning_rate": 5e-05,
      "loss": 0.2231,
      "step": 352
    },
    {
      "epoch": 0.4133489461358314,
      "grad_norm": 0.9383844137191772,
      "learning_rate": 5e-05,
      "loss": 0.1788,
      "step": 353
    },
    {
      "epoch": 0.41451990632318503,
      "grad_norm": 0.5302308797836304,
      "learning_rate": 5e-05,
      "loss": 0.0942,
      "step": 354
    },
    {
      "epoch": 0.41569086651053866,
      "grad_norm": 0.9199606776237488,
      "learning_rate": 5e-05,
      "loss": 0.1876,
      "step": 355
    },
    {
      "epoch": 0.4168618266978923,
      "grad_norm": 0.8317784667015076,
      "learning_rate": 5e-05,
      "loss": 0.0984,
      "step": 356
    },
    {
      "epoch": 0.4180327868852459,
      "grad_norm": 1.0014327764511108,
      "learning_rate": 5e-05,
      "loss": 0.208,
      "step": 357
    },
    {
      "epoch": 0.41920374707259955,
      "grad_norm": 1.6457393169403076,
      "learning_rate": 5e-05,
      "loss": 0.1927,
      "step": 358
    },
    {
      "epoch": 0.4203747072599532,
      "grad_norm": 1.684771180152893,
      "learning_rate": 5e-05,
      "loss": 0.2477,
      "step": 359
    },
    {
      "epoch": 0.4215456674473068,
      "grad_norm": 0.9560356736183167,
      "learning_rate": 5e-05,
      "loss": 0.1712,
      "step": 360
    },
    {
      "epoch": 0.42271662763466045,
      "grad_norm": 1.2261898517608643,
      "learning_rate": 5e-05,
      "loss": 0.2062,
      "step": 361
    },
    {
      "epoch": 0.4238875878220141,
      "grad_norm": 1.08522629737854,
      "learning_rate": 5e-05,
      "loss": 0.2106,
      "step": 362
    },
    {
      "epoch": 0.42505854800936765,
      "grad_norm": 1.5955184698104858,
      "learning_rate": 5e-05,
      "loss": 0.1954,
      "step": 363
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 0.7961490750312805,
      "learning_rate": 5e-05,
      "loss": 0.1413,
      "step": 364
    },
    {
      "epoch": 0.4274004683840749,
      "grad_norm": 1.0188733339309692,
      "learning_rate": 5e-05,
      "loss": 0.2911,
      "step": 365
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.1751905679702759,
      "learning_rate": 5e-05,
      "loss": 0.2359,
      "step": 366
    },
    {
      "epoch": 0.4297423887587822,
      "grad_norm": 1.0745501518249512,
      "learning_rate": 5e-05,
      "loss": 0.3181,
      "step": 367
    },
    {
      "epoch": 0.4309133489461358,
      "grad_norm": 0.8931562304496765,
      "learning_rate": 5e-05,
      "loss": 0.2387,
      "step": 368
    },
    {
      "epoch": 0.43208430913348944,
      "grad_norm": 1.028438687324524,
      "learning_rate": 5e-05,
      "loss": 0.2813,
      "step": 369
    },
    {
      "epoch": 0.4332552693208431,
      "grad_norm": 1.3186322450637817,
      "learning_rate": 5e-05,
      "loss": 0.1635,
      "step": 370
    },
    {
      "epoch": 0.4344262295081967,
      "grad_norm": 0.7981401681900024,
      "learning_rate": 5e-05,
      "loss": 0.1731,
      "step": 371
    },
    {
      "epoch": 0.43559718969555034,
      "grad_norm": 0.9915591478347778,
      "learning_rate": 5e-05,
      "loss": 0.2188,
      "step": 372
    },
    {
      "epoch": 0.43676814988290397,
      "grad_norm": 0.7374874949455261,
      "learning_rate": 5e-05,
      "loss": 0.2311,
      "step": 373
    },
    {
      "epoch": 0.4379391100702576,
      "grad_norm": 0.851717472076416,
      "learning_rate": 5e-05,
      "loss": 0.1187,
      "step": 374
    },
    {
      "epoch": 0.43911007025761123,
      "grad_norm": 0.9216967821121216,
      "learning_rate": 5e-05,
      "loss": 0.1808,
      "step": 375
    },
    {
      "epoch": 0.44028103044496486,
      "grad_norm": 1.0247857570648193,
      "learning_rate": 5e-05,
      "loss": 0.2077,
      "step": 376
    },
    {
      "epoch": 0.4414519906323185,
      "grad_norm": 1.181145429611206,
      "learning_rate": 5e-05,
      "loss": 0.2275,
      "step": 377
    },
    {
      "epoch": 0.4426229508196721,
      "grad_norm": 0.7041964530944824,
      "learning_rate": 5e-05,
      "loss": 0.1361,
      "step": 378
    },
    {
      "epoch": 0.44379391100702575,
      "grad_norm": 0.7790698409080505,
      "learning_rate": 5e-05,
      "loss": 0.1976,
      "step": 379
    },
    {
      "epoch": 0.4449648711943794,
      "grad_norm": 0.6598180532455444,
      "learning_rate": 5e-05,
      "loss": 0.1543,
      "step": 380
    },
    {
      "epoch": 0.446135831381733,
      "grad_norm": 0.5716289281845093,
      "learning_rate": 5e-05,
      "loss": 0.1058,
      "step": 381
    },
    {
      "epoch": 0.44730679156908665,
      "grad_norm": 0.9356935620307922,
      "learning_rate": 5e-05,
      "loss": 0.1848,
      "step": 382
    },
    {
      "epoch": 0.4484777517564403,
      "grad_norm": 0.8493645787239075,
      "learning_rate": 5e-05,
      "loss": 0.1124,
      "step": 383
    },
    {
      "epoch": 0.4496487119437939,
      "grad_norm": 0.9079912900924683,
      "learning_rate": 5e-05,
      "loss": 0.1473,
      "step": 384
    },
    {
      "epoch": 0.45081967213114754,
      "grad_norm": 1.0445328950881958,
      "learning_rate": 5e-05,
      "loss": 0.2144,
      "step": 385
    },
    {
      "epoch": 0.4519906323185012,
      "grad_norm": 0.8556708693504333,
      "learning_rate": 5e-05,
      "loss": 0.1652,
      "step": 386
    },
    {
      "epoch": 0.4531615925058548,
      "grad_norm": 1.051552653312683,
      "learning_rate": 5e-05,
      "loss": 0.2019,
      "step": 387
    },
    {
      "epoch": 0.45433255269320844,
      "grad_norm": 1.0717633962631226,
      "learning_rate": 5e-05,
      "loss": 0.1371,
      "step": 388
    },
    {
      "epoch": 0.45550351288056207,
      "grad_norm": 0.6724720597267151,
      "learning_rate": 5e-05,
      "loss": 0.1121,
      "step": 389
    },
    {
      "epoch": 0.4566744730679157,
      "grad_norm": 1.5941948890686035,
      "learning_rate": 5e-05,
      "loss": 0.2938,
      "step": 390
    },
    {
      "epoch": 0.45784543325526933,
      "grad_norm": 0.96883225440979,
      "learning_rate": 5e-05,
      "loss": 0.1271,
      "step": 391
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 1.1081492900848389,
      "learning_rate": 5e-05,
      "loss": 0.3494,
      "step": 392
    },
    {
      "epoch": 0.4601873536299766,
      "grad_norm": 0.7158045172691345,
      "learning_rate": 5e-05,
      "loss": 0.1684,
      "step": 393
    },
    {
      "epoch": 0.4613583138173302,
      "grad_norm": 0.8752233386039734,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 394
    },
    {
      "epoch": 0.46252927400468385,
      "grad_norm": 1.5977296829223633,
      "learning_rate": 5e-05,
      "loss": 0.2185,
      "step": 395
    },
    {
      "epoch": 0.4637002341920375,
      "grad_norm": 0.8263867497444153,
      "learning_rate": 5e-05,
      "loss": 0.2099,
      "step": 396
    },
    {
      "epoch": 0.4648711943793911,
      "grad_norm": 0.7378787398338318,
      "learning_rate": 5e-05,
      "loss": 0.2444,
      "step": 397
    },
    {
      "epoch": 0.46604215456674475,
      "grad_norm": 0.8967553377151489,
      "learning_rate": 5e-05,
      "loss": 0.0877,
      "step": 398
    },
    {
      "epoch": 0.4672131147540984,
      "grad_norm": 0.7947469353675842,
      "learning_rate": 5e-05,
      "loss": 0.1658,
      "step": 399
    },
    {
      "epoch": 0.468384074941452,
      "grad_norm": 0.5040697455406189,
      "learning_rate": 5e-05,
      "loss": 0.0509,
      "step": 400
    },
    {
      "epoch": 0.46955503512880564,
      "grad_norm": 1.953522801399231,
      "learning_rate": 5e-05,
      "loss": 0.3502,
      "step": 401
    },
    {
      "epoch": 0.4707259953161593,
      "grad_norm": 0.732383131980896,
      "learning_rate": 5e-05,
      "loss": 0.1131,
      "step": 402
    },
    {
      "epoch": 0.4718969555035129,
      "grad_norm": 0.6332831978797913,
      "learning_rate": 5e-05,
      "loss": 0.0764,
      "step": 403
    },
    {
      "epoch": 0.47306791569086654,
      "grad_norm": 0.980751633644104,
      "learning_rate": 5e-05,
      "loss": 0.1458,
      "step": 404
    },
    {
      "epoch": 0.47423887587822017,
      "grad_norm": 0.8861015439033508,
      "learning_rate": 5e-05,
      "loss": 0.2472,
      "step": 405
    },
    {
      "epoch": 0.47540983606557374,
      "grad_norm": 1.2339850664138794,
      "learning_rate": 5e-05,
      "loss": 0.2221,
      "step": 406
    },
    {
      "epoch": 0.4765807962529274,
      "grad_norm": 1.2128920555114746,
      "learning_rate": 5e-05,
      "loss": 0.1959,
      "step": 407
    },
    {
      "epoch": 0.477751756440281,
      "grad_norm": 1.3294399976730347,
      "learning_rate": 5e-05,
      "loss": 0.3116,
      "step": 408
    },
    {
      "epoch": 0.47892271662763464,
      "grad_norm": 0.7644985318183899,
      "learning_rate": 5e-05,
      "loss": 0.1732,
      "step": 409
    },
    {
      "epoch": 0.48009367681498827,
      "grad_norm": 1.8628321886062622,
      "learning_rate": 5e-05,
      "loss": 0.288,
      "step": 410
    },
    {
      "epoch": 0.4812646370023419,
      "grad_norm": 0.7763954997062683,
      "learning_rate": 5e-05,
      "loss": 0.1865,
      "step": 411
    },
    {
      "epoch": 0.48243559718969553,
      "grad_norm": 0.6474214196205139,
      "learning_rate": 5e-05,
      "loss": 0.1744,
      "step": 412
    },
    {
      "epoch": 0.48360655737704916,
      "grad_norm": 1.1205135583877563,
      "learning_rate": 5e-05,
      "loss": 0.2359,
      "step": 413
    },
    {
      "epoch": 0.4847775175644028,
      "grad_norm": 0.7113279700279236,
      "learning_rate": 5e-05,
      "loss": 0.1222,
      "step": 414
    },
    {
      "epoch": 0.4859484777517564,
      "grad_norm": 1.0443090200424194,
      "learning_rate": 5e-05,
      "loss": 0.1978,
      "step": 415
    },
    {
      "epoch": 0.48711943793911006,
      "grad_norm": 1.1319421529769897,
      "learning_rate": 5e-05,
      "loss": 0.2553,
      "step": 416
    },
    {
      "epoch": 0.4882903981264637,
      "grad_norm": 0.8958824872970581,
      "learning_rate": 5e-05,
      "loss": 0.1774,
      "step": 417
    },
    {
      "epoch": 0.4894613583138173,
      "grad_norm": 8.335951805114746,
      "learning_rate": 5e-05,
      "loss": 0.1743,
      "step": 418
    },
    {
      "epoch": 0.49063231850117095,
      "grad_norm": 1.1525382995605469,
      "learning_rate": 5e-05,
      "loss": 0.6055,
      "step": 419
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 0.893369197845459,
      "learning_rate": 5e-05,
      "loss": 0.2346,
      "step": 420
    },
    {
      "epoch": 0.4929742388758782,
      "grad_norm": 1.3757282495498657,
      "learning_rate": 5e-05,
      "loss": 0.1967,
      "step": 421
    },
    {
      "epoch": 0.49414519906323184,
      "grad_norm": 0.9983652830123901,
      "learning_rate": 5e-05,
      "loss": 0.2635,
      "step": 422
    },
    {
      "epoch": 0.4953161592505855,
      "grad_norm": 0.5471099615097046,
      "learning_rate": 5e-05,
      "loss": 0.1094,
      "step": 423
    },
    {
      "epoch": 0.4964871194379391,
      "grad_norm": 0.679563581943512,
      "learning_rate": 5e-05,
      "loss": 0.1832,
      "step": 424
    },
    {
      "epoch": 0.49765807962529274,
      "grad_norm": 0.569052517414093,
      "learning_rate": 5e-05,
      "loss": 0.1925,
      "step": 425
    },
    {
      "epoch": 0.49882903981264637,
      "grad_norm": 0.8793511986732483,
      "learning_rate": 5e-05,
      "loss": 0.1713,
      "step": 426
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.6933028697967529,
      "learning_rate": 5e-05,
      "loss": 0.1116,
      "step": 427
    },
    {
      "epoch": 0.5011709601873536,
      "grad_norm": 0.7620845437049866,
      "learning_rate": 5e-05,
      "loss": 0.1758,
      "step": 428
    },
    {
      "epoch": 0.5023419203747073,
      "grad_norm": 1.0803807973861694,
      "learning_rate": 5e-05,
      "loss": 0.2637,
      "step": 429
    },
    {
      "epoch": 0.5035128805620609,
      "grad_norm": 1.0311940908432007,
      "learning_rate": 5e-05,
      "loss": 0.3459,
      "step": 430
    },
    {
      "epoch": 0.5046838407494145,
      "grad_norm": 0.7104464769363403,
      "learning_rate": 5e-05,
      "loss": 0.1683,
      "step": 431
    },
    {
      "epoch": 0.5058548009367682,
      "grad_norm": 0.7146840691566467,
      "learning_rate": 5e-05,
      "loss": 0.1258,
      "step": 432
    },
    {
      "epoch": 0.5070257611241218,
      "grad_norm": 0.982945442199707,
      "learning_rate": 5e-05,
      "loss": 0.1799,
      "step": 433
    },
    {
      "epoch": 0.5081967213114754,
      "grad_norm": 0.7972589731216431,
      "learning_rate": 5e-05,
      "loss": 0.1908,
      "step": 434
    },
    {
      "epoch": 0.509367681498829,
      "grad_norm": 0.4919991195201874,
      "learning_rate": 5e-05,
      "loss": 0.0602,
      "step": 435
    },
    {
      "epoch": 0.5105386416861827,
      "grad_norm": 0.8439709544181824,
      "learning_rate": 5e-05,
      "loss": 0.221,
      "step": 436
    },
    {
      "epoch": 0.5117096018735363,
      "grad_norm": 0.9151207804679871,
      "learning_rate": 5e-05,
      "loss": 0.1725,
      "step": 437
    },
    {
      "epoch": 0.5128805620608899,
      "grad_norm": 1.2324066162109375,
      "learning_rate": 5e-05,
      "loss": 0.2269,
      "step": 438
    },
    {
      "epoch": 0.5140515222482436,
      "grad_norm": 0.6386104226112366,
      "learning_rate": 5e-05,
      "loss": 0.1867,
      "step": 439
    },
    {
      "epoch": 0.5152224824355972,
      "grad_norm": 0.9033518433570862,
      "learning_rate": 5e-05,
      "loss": 0.1375,
      "step": 440
    },
    {
      "epoch": 0.5163934426229508,
      "grad_norm": 0.8104183077812195,
      "learning_rate": 5e-05,
      "loss": 0.2247,
      "step": 441
    },
    {
      "epoch": 0.5175644028103045,
      "grad_norm": 0.9047492742538452,
      "learning_rate": 5e-05,
      "loss": 0.4122,
      "step": 442
    },
    {
      "epoch": 0.5187353629976581,
      "grad_norm": 0.7552227973937988,
      "learning_rate": 5e-05,
      "loss": 0.205,
      "step": 443
    },
    {
      "epoch": 0.5199063231850117,
      "grad_norm": 1.019049048423767,
      "learning_rate": 5e-05,
      "loss": 0.1423,
      "step": 444
    },
    {
      "epoch": 0.5210772833723654,
      "grad_norm": 0.8283849358558655,
      "learning_rate": 5e-05,
      "loss": 0.1147,
      "step": 445
    },
    {
      "epoch": 0.522248243559719,
      "grad_norm": 0.9272650480270386,
      "learning_rate": 5e-05,
      "loss": 0.1442,
      "step": 446
    },
    {
      "epoch": 0.5234192037470726,
      "grad_norm": 0.6588469743728638,
      "learning_rate": 5e-05,
      "loss": 0.1506,
      "step": 447
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 0.7712093591690063,
      "learning_rate": 5e-05,
      "loss": 0.1025,
      "step": 448
    },
    {
      "epoch": 0.5257611241217799,
      "grad_norm": 1.405097484588623,
      "learning_rate": 5e-05,
      "loss": 0.2443,
      "step": 449
    },
    {
      "epoch": 0.5269320843091335,
      "grad_norm": 1.1541838645935059,
      "learning_rate": 5e-05,
      "loss": 0.1665,
      "step": 450
    },
    {
      "epoch": 0.5281030444964872,
      "grad_norm": 0.9795811772346497,
      "learning_rate": 5e-05,
      "loss": 0.2109,
      "step": 451
    },
    {
      "epoch": 0.5292740046838408,
      "grad_norm": 0.8595281839370728,
      "learning_rate": 5e-05,
      "loss": 0.0793,
      "step": 452
    },
    {
      "epoch": 0.5304449648711944,
      "grad_norm": 1.1438820362091064,
      "learning_rate": 5e-05,
      "loss": 0.1346,
      "step": 453
    },
    {
      "epoch": 0.531615925058548,
      "grad_norm": 0.8277682662010193,
      "learning_rate": 5e-05,
      "loss": 0.2868,
      "step": 454
    },
    {
      "epoch": 0.5327868852459017,
      "grad_norm": 0.8850882053375244,
      "learning_rate": 5e-05,
      "loss": 0.1365,
      "step": 455
    },
    {
      "epoch": 0.5339578454332553,
      "grad_norm": 0.9024569392204285,
      "learning_rate": 5e-05,
      "loss": 0.1873,
      "step": 456
    },
    {
      "epoch": 0.5351288056206089,
      "grad_norm": 0.7573788166046143,
      "learning_rate": 5e-05,
      "loss": 0.1031,
      "step": 457
    },
    {
      "epoch": 0.5362997658079626,
      "grad_norm": 0.8266049027442932,
      "learning_rate": 5e-05,
      "loss": 0.1175,
      "step": 458
    },
    {
      "epoch": 0.5374707259953162,
      "grad_norm": 1.3378729820251465,
      "learning_rate": 5e-05,
      "loss": 0.2818,
      "step": 459
    },
    {
      "epoch": 0.5386416861826698,
      "grad_norm": 0.5271730422973633,
      "learning_rate": 5e-05,
      "loss": 0.0932,
      "step": 460
    },
    {
      "epoch": 0.5398126463700235,
      "grad_norm": 0.7121453285217285,
      "learning_rate": 5e-05,
      "loss": 0.1418,
      "step": 461
    },
    {
      "epoch": 0.5409836065573771,
      "grad_norm": 0.6908478140830994,
      "learning_rate": 5e-05,
      "loss": 0.1249,
      "step": 462
    },
    {
      "epoch": 0.5421545667447307,
      "grad_norm": 0.5785465240478516,
      "learning_rate": 5e-05,
      "loss": 0.1152,
      "step": 463
    },
    {
      "epoch": 0.5433255269320844,
      "grad_norm": 1.462785243988037,
      "learning_rate": 5e-05,
      "loss": 0.1729,
      "step": 464
    },
    {
      "epoch": 0.544496487119438,
      "grad_norm": 0.6198853850364685,
      "learning_rate": 5e-05,
      "loss": 0.0682,
      "step": 465
    },
    {
      "epoch": 0.5456674473067916,
      "grad_norm": 1.082151174545288,
      "learning_rate": 5e-05,
      "loss": 0.2713,
      "step": 466
    },
    {
      "epoch": 0.5468384074941453,
      "grad_norm": 0.8141269683837891,
      "learning_rate": 5e-05,
      "loss": 0.1391,
      "step": 467
    },
    {
      "epoch": 0.5480093676814989,
      "grad_norm": 0.9419107437133789,
      "learning_rate": 5e-05,
      "loss": 0.1292,
      "step": 468
    },
    {
      "epoch": 0.5491803278688525,
      "grad_norm": 0.5872287154197693,
      "learning_rate": 5e-05,
      "loss": 0.0704,
      "step": 469
    },
    {
      "epoch": 0.550351288056206,
      "grad_norm": 1.2868874073028564,
      "learning_rate": 5e-05,
      "loss": 0.2915,
      "step": 470
    },
    {
      "epoch": 0.5515222482435597,
      "grad_norm": 0.8169856667518616,
      "learning_rate": 5e-05,
      "loss": 0.192,
      "step": 471
    },
    {
      "epoch": 0.5526932084309133,
      "grad_norm": 0.8843258023262024,
      "learning_rate": 5e-05,
      "loss": 0.1919,
      "step": 472
    },
    {
      "epoch": 0.5538641686182669,
      "grad_norm": 0.6892499923706055,
      "learning_rate": 5e-05,
      "loss": 0.088,
      "step": 473
    },
    {
      "epoch": 0.5550351288056206,
      "grad_norm": 0.7767154574394226,
      "learning_rate": 5e-05,
      "loss": 0.2752,
      "step": 474
    },
    {
      "epoch": 0.5562060889929742,
      "grad_norm": 1.24142587184906,
      "learning_rate": 5e-05,
      "loss": 0.1869,
      "step": 475
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 1.0573650598526,
      "learning_rate": 5e-05,
      "loss": 0.2237,
      "step": 476
    },
    {
      "epoch": 0.5585480093676815,
      "grad_norm": 0.6417459845542908,
      "learning_rate": 5e-05,
      "loss": 0.1945,
      "step": 477
    },
    {
      "epoch": 0.5597189695550351,
      "grad_norm": 1.172951579093933,
      "learning_rate": 5e-05,
      "loss": 0.2927,
      "step": 478
    },
    {
      "epoch": 0.5608899297423887,
      "grad_norm": 0.8266577124595642,
      "learning_rate": 5e-05,
      "loss": 0.1293,
      "step": 479
    },
    {
      "epoch": 0.5620608899297423,
      "grad_norm": 0.6546378135681152,
      "learning_rate": 5e-05,
      "loss": 0.1755,
      "step": 480
    },
    {
      "epoch": 0.563231850117096,
      "grad_norm": 0.4693901240825653,
      "learning_rate": 5e-05,
      "loss": 0.185,
      "step": 481
    },
    {
      "epoch": 0.5644028103044496,
      "grad_norm": 1.344138503074646,
      "learning_rate": 5e-05,
      "loss": 0.2369,
      "step": 482
    },
    {
      "epoch": 0.5655737704918032,
      "grad_norm": 0.9353551268577576,
      "learning_rate": 5e-05,
      "loss": 0.2541,
      "step": 483
    },
    {
      "epoch": 0.5667447306791569,
      "grad_norm": 1.3202239274978638,
      "learning_rate": 5e-05,
      "loss": 0.24,
      "step": 484
    },
    {
      "epoch": 0.5679156908665105,
      "grad_norm": 0.6230173110961914,
      "learning_rate": 5e-05,
      "loss": 0.1412,
      "step": 485
    },
    {
      "epoch": 0.5690866510538641,
      "grad_norm": 0.8797799944877625,
      "learning_rate": 5e-05,
      "loss": 0.1222,
      "step": 486
    },
    {
      "epoch": 0.5702576112412178,
      "grad_norm": 0.5647742748260498,
      "learning_rate": 5e-05,
      "loss": 0.0977,
      "step": 487
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.7585904598236084,
      "learning_rate": 5e-05,
      "loss": 0.2399,
      "step": 488
    },
    {
      "epoch": 0.572599531615925,
      "grad_norm": 0.979888916015625,
      "learning_rate": 5e-05,
      "loss": 0.1613,
      "step": 489
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 0.9519590735435486,
      "learning_rate": 5e-05,
      "loss": 0.1999,
      "step": 490
    },
    {
      "epoch": 0.5749414519906323,
      "grad_norm": 0.9524645209312439,
      "learning_rate": 5e-05,
      "loss": 0.2357,
      "step": 491
    },
    {
      "epoch": 0.5761124121779859,
      "grad_norm": 1.0444679260253906,
      "learning_rate": 5e-05,
      "loss": 0.2088,
      "step": 492
    },
    {
      "epoch": 0.5772833723653396,
      "grad_norm": 0.8905912637710571,
      "learning_rate": 5e-05,
      "loss": 0.2789,
      "step": 493
    },
    {
      "epoch": 0.5784543325526932,
      "grad_norm": 0.8034136295318604,
      "learning_rate": 5e-05,
      "loss": 0.1754,
      "step": 494
    },
    {
      "epoch": 0.5796252927400468,
      "grad_norm": 0.8109515309333801,
      "learning_rate": 5e-05,
      "loss": 0.1288,
      "step": 495
    },
    {
      "epoch": 0.5807962529274004,
      "grad_norm": 0.8797544836997986,
      "learning_rate": 5e-05,
      "loss": 0.135,
      "step": 496
    },
    {
      "epoch": 0.5819672131147541,
      "grad_norm": 0.8929939270019531,
      "learning_rate": 5e-05,
      "loss": 0.1317,
      "step": 497
    },
    {
      "epoch": 0.5831381733021077,
      "grad_norm": 1.0177133083343506,
      "learning_rate": 5e-05,
      "loss": 0.0775,
      "step": 498
    },
    {
      "epoch": 0.5843091334894613,
      "grad_norm": 0.8635395169258118,
      "learning_rate": 5e-05,
      "loss": 0.251,
      "step": 499
    },
    {
      "epoch": 0.585480093676815,
      "grad_norm": 1.0383201837539673,
      "learning_rate": 5e-05,
      "loss": 0.1922,
      "step": 500
    },
    {
      "epoch": 0.5866510538641686,
      "grad_norm": 1.357316255569458,
      "learning_rate": 5e-05,
      "loss": 0.207,
      "step": 501
    },
    {
      "epoch": 0.5878220140515222,
      "grad_norm": 0.8810313940048218,
      "learning_rate": 5e-05,
      "loss": 0.1806,
      "step": 502
    },
    {
      "epoch": 0.5889929742388759,
      "grad_norm": 0.6746959090232849,
      "learning_rate": 5e-05,
      "loss": 0.1379,
      "step": 503
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 0.8446153402328491,
      "learning_rate": 5e-05,
      "loss": 0.0939,
      "step": 504
    },
    {
      "epoch": 0.5913348946135831,
      "grad_norm": 0.6590154767036438,
      "learning_rate": 5e-05,
      "loss": 0.1799,
      "step": 505
    },
    {
      "epoch": 0.5925058548009368,
      "grad_norm": 0.6256260275840759,
      "learning_rate": 5e-05,
      "loss": 0.1308,
      "step": 506
    },
    {
      "epoch": 0.5936768149882904,
      "grad_norm": 0.6059020757675171,
      "learning_rate": 5e-05,
      "loss": 0.1001,
      "step": 507
    },
    {
      "epoch": 0.594847775175644,
      "grad_norm": 0.8900473713874817,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 508
    },
    {
      "epoch": 0.5960187353629977,
      "grad_norm": 0.8622647523880005,
      "learning_rate": 5e-05,
      "loss": 0.2269,
      "step": 509
    },
    {
      "epoch": 0.5971896955503513,
      "grad_norm": 0.904223620891571,
      "learning_rate": 5e-05,
      "loss": 0.1112,
      "step": 510
    },
    {
      "epoch": 0.5983606557377049,
      "grad_norm": 1.3617008924484253,
      "learning_rate": 5e-05,
      "loss": 0.2038,
      "step": 511
    },
    {
      "epoch": 0.5995316159250585,
      "grad_norm": 1.3960012197494507,
      "learning_rate": 5e-05,
      "loss": 0.3677,
      "step": 512
    },
    {
      "epoch": 0.6007025761124122,
      "grad_norm": 0.7807445526123047,
      "learning_rate": 5e-05,
      "loss": 0.1081,
      "step": 513
    },
    {
      "epoch": 0.6018735362997658,
      "grad_norm": 0.868705153465271,
      "learning_rate": 5e-05,
      "loss": 0.108,
      "step": 514
    },
    {
      "epoch": 0.6030444964871194,
      "grad_norm": 1.3419123888015747,
      "learning_rate": 5e-05,
      "loss": 0.3052,
      "step": 515
    },
    {
      "epoch": 0.6042154566744731,
      "grad_norm": 0.7593042850494385,
      "learning_rate": 5e-05,
      "loss": 0.2013,
      "step": 516
    },
    {
      "epoch": 0.6053864168618267,
      "grad_norm": 1.3194591999053955,
      "learning_rate": 5e-05,
      "loss": 0.2248,
      "step": 517
    },
    {
      "epoch": 0.6065573770491803,
      "grad_norm": 1.5961135625839233,
      "learning_rate": 5e-05,
      "loss": 0.1542,
      "step": 518
    },
    {
      "epoch": 0.607728337236534,
      "grad_norm": 0.7781845331192017,
      "learning_rate": 5e-05,
      "loss": 0.1451,
      "step": 519
    },
    {
      "epoch": 0.6088992974238876,
      "grad_norm": 0.943681538105011,
      "learning_rate": 5e-05,
      "loss": 0.151,
      "step": 520
    },
    {
      "epoch": 0.6100702576112412,
      "grad_norm": 0.9974660873413086,
      "learning_rate": 5e-05,
      "loss": 0.1266,
      "step": 521
    },
    {
      "epoch": 0.6112412177985949,
      "grad_norm": 0.9477709531784058,
      "learning_rate": 5e-05,
      "loss": 0.1834,
      "step": 522
    },
    {
      "epoch": 0.6124121779859485,
      "grad_norm": 0.8391426801681519,
      "learning_rate": 5e-05,
      "loss": 0.1984,
      "step": 523
    },
    {
      "epoch": 0.6135831381733021,
      "grad_norm": 0.9878169894218445,
      "learning_rate": 5e-05,
      "loss": 0.2718,
      "step": 524
    },
    {
      "epoch": 0.6147540983606558,
      "grad_norm": 0.8978649973869324,
      "learning_rate": 5e-05,
      "loss": 0.1909,
      "step": 525
    },
    {
      "epoch": 0.6159250585480094,
      "grad_norm": 0.7021591663360596,
      "learning_rate": 5e-05,
      "loss": 0.135,
      "step": 526
    },
    {
      "epoch": 0.617096018735363,
      "grad_norm": 0.9866658449172974,
      "learning_rate": 5e-05,
      "loss": 0.2261,
      "step": 527
    },
    {
      "epoch": 0.6182669789227166,
      "grad_norm": 0.9210829138755798,
      "learning_rate": 5e-05,
      "loss": 0.2251,
      "step": 528
    },
    {
      "epoch": 0.6194379391100703,
      "grad_norm": 0.8234099745750427,
      "learning_rate": 5e-05,
      "loss": 0.0988,
      "step": 529
    },
    {
      "epoch": 0.6206088992974239,
      "grad_norm": 0.6197835206985474,
      "learning_rate": 5e-05,
      "loss": 0.1081,
      "step": 530
    },
    {
      "epoch": 0.6217798594847775,
      "grad_norm": 0.7356049418449402,
      "learning_rate": 5e-05,
      "loss": 0.1605,
      "step": 531
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 0.7115222811698914,
      "learning_rate": 5e-05,
      "loss": 0.1531,
      "step": 532
    },
    {
      "epoch": 0.6241217798594848,
      "grad_norm": 1.0968250036239624,
      "learning_rate": 5e-05,
      "loss": 0.1548,
      "step": 533
    },
    {
      "epoch": 0.6252927400468384,
      "grad_norm": 0.5439021587371826,
      "learning_rate": 5e-05,
      "loss": 0.1047,
      "step": 534
    },
    {
      "epoch": 0.6264637002341921,
      "grad_norm": 0.7889012098312378,
      "learning_rate": 5e-05,
      "loss": 0.2529,
      "step": 535
    },
    {
      "epoch": 0.6276346604215457,
      "grad_norm": 1.1100521087646484,
      "learning_rate": 5e-05,
      "loss": 0.2397,
      "step": 536
    },
    {
      "epoch": 0.6288056206088993,
      "grad_norm": 0.5657084584236145,
      "learning_rate": 5e-05,
      "loss": 0.1317,
      "step": 537
    },
    {
      "epoch": 0.629976580796253,
      "grad_norm": 1.1949461698532104,
      "learning_rate": 5e-05,
      "loss": 0.1353,
      "step": 538
    },
    {
      "epoch": 0.6311475409836066,
      "grad_norm": 0.7745693922042847,
      "learning_rate": 5e-05,
      "loss": 0.1982,
      "step": 539
    },
    {
      "epoch": 0.6323185011709602,
      "grad_norm": 0.9588724374771118,
      "learning_rate": 5e-05,
      "loss": 0.1629,
      "step": 540
    },
    {
      "epoch": 0.6334894613583139,
      "grad_norm": 0.9131604433059692,
      "learning_rate": 5e-05,
      "loss": 0.11,
      "step": 541
    },
    {
      "epoch": 0.6346604215456675,
      "grad_norm": 0.5361155271530151,
      "learning_rate": 5e-05,
      "loss": 0.0919,
      "step": 542
    },
    {
      "epoch": 0.6358313817330211,
      "grad_norm": 0.9347140192985535,
      "learning_rate": 5e-05,
      "loss": 0.4079,
      "step": 543
    },
    {
      "epoch": 0.6370023419203747,
      "grad_norm": 0.8966814875602722,
      "learning_rate": 5e-05,
      "loss": 0.0963,
      "step": 544
    },
    {
      "epoch": 0.6381733021077284,
      "grad_norm": 0.7460395693778992,
      "learning_rate": 5e-05,
      "loss": 0.1429,
      "step": 545
    },
    {
      "epoch": 0.639344262295082,
      "grad_norm": 0.940119743347168,
      "learning_rate": 5e-05,
      "loss": 0.1837,
      "step": 546
    },
    {
      "epoch": 0.6405152224824356,
      "grad_norm": 0.5062552690505981,
      "learning_rate": 5e-05,
      "loss": 0.0875,
      "step": 547
    },
    {
      "epoch": 0.6416861826697893,
      "grad_norm": 1.0055242776870728,
      "learning_rate": 5e-05,
      "loss": 0.2118,
      "step": 548
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 1.0089287757873535,
      "learning_rate": 5e-05,
      "loss": 0.2982,
      "step": 549
    },
    {
      "epoch": 0.6440281030444965,
      "grad_norm": 1.0779331922531128,
      "learning_rate": 5e-05,
      "loss": 0.3154,
      "step": 550
    },
    {
      "epoch": 0.6451990632318502,
      "grad_norm": 0.5533347725868225,
      "learning_rate": 5e-05,
      "loss": 0.0712,
      "step": 551
    },
    {
      "epoch": 0.6463700234192038,
      "grad_norm": 0.8784337639808655,
      "learning_rate": 5e-05,
      "loss": 0.1246,
      "step": 552
    },
    {
      "epoch": 0.6475409836065574,
      "grad_norm": 0.713638961315155,
      "learning_rate": 5e-05,
      "loss": 0.122,
      "step": 553
    },
    {
      "epoch": 0.6487119437939111,
      "grad_norm": 1.232643961906433,
      "learning_rate": 5e-05,
      "loss": 0.1969,
      "step": 554
    },
    {
      "epoch": 0.6498829039812647,
      "grad_norm": 0.8782873153686523,
      "learning_rate": 5e-05,
      "loss": 0.1426,
      "step": 555
    },
    {
      "epoch": 0.6510538641686182,
      "grad_norm": 1.5775519609451294,
      "learning_rate": 5e-05,
      "loss": 0.3631,
      "step": 556
    },
    {
      "epoch": 0.6522248243559718,
      "grad_norm": 0.8117465376853943,
      "learning_rate": 5e-05,
      "loss": 0.1647,
      "step": 557
    },
    {
      "epoch": 0.6533957845433255,
      "grad_norm": 1.636362910270691,
      "learning_rate": 5e-05,
      "loss": 0.2094,
      "step": 558
    },
    {
      "epoch": 0.6545667447306791,
      "grad_norm": 0.6076356768608093,
      "learning_rate": 5e-05,
      "loss": 0.1053,
      "step": 559
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 0.9041610956192017,
      "learning_rate": 5e-05,
      "loss": 0.1822,
      "step": 560
    },
    {
      "epoch": 0.6569086651053864,
      "grad_norm": 1.0199798345565796,
      "learning_rate": 5e-05,
      "loss": 0.2576,
      "step": 561
    },
    {
      "epoch": 0.65807962529274,
      "grad_norm": 0.9351782202720642,
      "learning_rate": 5e-05,
      "loss": 0.1923,
      "step": 562
    },
    {
      "epoch": 0.6592505854800936,
      "grad_norm": 0.727895200252533,
      "learning_rate": 5e-05,
      "loss": 0.1507,
      "step": 563
    },
    {
      "epoch": 0.6604215456674473,
      "grad_norm": 0.5424399971961975,
      "learning_rate": 5e-05,
      "loss": 0.1162,
      "step": 564
    },
    {
      "epoch": 0.6615925058548009,
      "grad_norm": 0.7502294182777405,
      "learning_rate": 5e-05,
      "loss": 0.0851,
      "step": 565
    },
    {
      "epoch": 0.6627634660421545,
      "grad_norm": 0.6950207352638245,
      "learning_rate": 5e-05,
      "loss": 0.1344,
      "step": 566
    },
    {
      "epoch": 0.6639344262295082,
      "grad_norm": 1.2665796279907227,
      "learning_rate": 5e-05,
      "loss": 0.1414,
      "step": 567
    },
    {
      "epoch": 0.6651053864168618,
      "grad_norm": 0.7571395039558411,
      "learning_rate": 5e-05,
      "loss": 0.1334,
      "step": 568
    },
    {
      "epoch": 0.6662763466042154,
      "grad_norm": 0.8061158657073975,
      "learning_rate": 5e-05,
      "loss": 0.0681,
      "step": 569
    },
    {
      "epoch": 0.667447306791569,
      "grad_norm": 1.1505298614501953,
      "learning_rate": 5e-05,
      "loss": 0.3117,
      "step": 570
    },
    {
      "epoch": 0.6686182669789227,
      "grad_norm": 1.242689609527588,
      "learning_rate": 5e-05,
      "loss": 0.2118,
      "step": 571
    },
    {
      "epoch": 0.6697892271662763,
      "grad_norm": 1.4907985925674438,
      "learning_rate": 5e-05,
      "loss": 0.2129,
      "step": 572
    },
    {
      "epoch": 0.6709601873536299,
      "grad_norm": 1.201464056968689,
      "learning_rate": 5e-05,
      "loss": 0.1345,
      "step": 573
    },
    {
      "epoch": 0.6721311475409836,
      "grad_norm": 1.9048044681549072,
      "learning_rate": 5e-05,
      "loss": 0.3573,
      "step": 574
    },
    {
      "epoch": 0.6733021077283372,
      "grad_norm": 0.9210594892501831,
      "learning_rate": 5e-05,
      "loss": 0.1353,
      "step": 575
    },
    {
      "epoch": 0.6744730679156908,
      "grad_norm": 0.7404020428657532,
      "learning_rate": 5e-05,
      "loss": 0.1641,
      "step": 576
    },
    {
      "epoch": 0.6756440281030445,
      "grad_norm": 1.1351152658462524,
      "learning_rate": 5e-05,
      "loss": 0.2414,
      "step": 577
    },
    {
      "epoch": 0.6768149882903981,
      "grad_norm": 0.8698041439056396,
      "learning_rate": 5e-05,
      "loss": 0.1698,
      "step": 578
    },
    {
      "epoch": 0.6779859484777517,
      "grad_norm": 0.6393489241600037,
      "learning_rate": 5e-05,
      "loss": 0.1875,
      "step": 579
    },
    {
      "epoch": 0.6791569086651054,
      "grad_norm": 0.9234943985939026,
      "learning_rate": 5e-05,
      "loss": 0.2247,
      "step": 580
    },
    {
      "epoch": 0.680327868852459,
      "grad_norm": 0.9722076654434204,
      "learning_rate": 5e-05,
      "loss": 0.0871,
      "step": 581
    },
    {
      "epoch": 0.6814988290398126,
      "grad_norm": 0.8923214673995972,
      "learning_rate": 5e-05,
      "loss": 0.1919,
      "step": 582
    },
    {
      "epoch": 0.6826697892271663,
      "grad_norm": 0.9485388398170471,
      "learning_rate": 5e-05,
      "loss": 0.0976,
      "step": 583
    },
    {
      "epoch": 0.6838407494145199,
      "grad_norm": 1.3136069774627686,
      "learning_rate": 5e-05,
      "loss": 0.2984,
      "step": 584
    },
    {
      "epoch": 0.6850117096018735,
      "grad_norm": 1.2082051038742065,
      "learning_rate": 5e-05,
      "loss": 0.1693,
      "step": 585
    },
    {
      "epoch": 0.6861826697892272,
      "grad_norm": 1.0657942295074463,
      "learning_rate": 5e-05,
      "loss": 0.1748,
      "step": 586
    },
    {
      "epoch": 0.6873536299765808,
      "grad_norm": 0.9792893528938293,
      "learning_rate": 5e-05,
      "loss": 0.1602,
      "step": 587
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 1.0112123489379883,
      "learning_rate": 5e-05,
      "loss": 0.0829,
      "step": 588
    },
    {
      "epoch": 0.689695550351288,
      "grad_norm": 0.586618185043335,
      "learning_rate": 5e-05,
      "loss": 0.1591,
      "step": 589
    },
    {
      "epoch": 0.6908665105386417,
      "grad_norm": 0.7532021403312683,
      "learning_rate": 5e-05,
      "loss": 0.0759,
      "step": 590
    },
    {
      "epoch": 0.6920374707259953,
      "grad_norm": 0.9033877849578857,
      "learning_rate": 5e-05,
      "loss": 0.2031,
      "step": 591
    },
    {
      "epoch": 0.6932084309133489,
      "grad_norm": 0.7347846627235413,
      "learning_rate": 5e-05,
      "loss": 0.136,
      "step": 592
    },
    {
      "epoch": 0.6943793911007026,
      "grad_norm": 0.576227605342865,
      "learning_rate": 5e-05,
      "loss": 0.1231,
      "step": 593
    },
    {
      "epoch": 0.6955503512880562,
      "grad_norm": 0.4848816990852356,
      "learning_rate": 5e-05,
      "loss": 0.1545,
      "step": 594
    },
    {
      "epoch": 0.6967213114754098,
      "grad_norm": 0.6078945994377136,
      "learning_rate": 5e-05,
      "loss": 0.095,
      "step": 595
    },
    {
      "epoch": 0.6978922716627635,
      "grad_norm": 1.2559460401535034,
      "learning_rate": 5e-05,
      "loss": 0.3207,
      "step": 596
    },
    {
      "epoch": 0.6990632318501171,
      "grad_norm": 0.6214315891265869,
      "learning_rate": 5e-05,
      "loss": 0.1468,
      "step": 597
    },
    {
      "epoch": 0.7002341920374707,
      "grad_norm": 1.003318428993225,
      "learning_rate": 5e-05,
      "loss": 0.0509,
      "step": 598
    },
    {
      "epoch": 0.7014051522248244,
      "grad_norm": 0.6666979789733887,
      "learning_rate": 5e-05,
      "loss": 0.1998,
      "step": 599
    },
    {
      "epoch": 0.702576112412178,
      "grad_norm": 1.5630545616149902,
      "learning_rate": 5e-05,
      "loss": 0.2218,
      "step": 600
    },
    {
      "epoch": 0.7037470725995316,
      "grad_norm": 0.9219649434089661,
      "learning_rate": 5e-05,
      "loss": 0.1762,
      "step": 601
    },
    {
      "epoch": 0.7049180327868853,
      "grad_norm": 0.8378086090087891,
      "learning_rate": 5e-05,
      "loss": 0.2286,
      "step": 602
    },
    {
      "epoch": 0.7060889929742389,
      "grad_norm": 0.9358880519866943,
      "learning_rate": 5e-05,
      "loss": 0.084,
      "step": 603
    },
    {
      "epoch": 0.7072599531615925,
      "grad_norm": 1.0307811498641968,
      "learning_rate": 5e-05,
      "loss": 0.192,
      "step": 604
    },
    {
      "epoch": 0.7084309133489461,
      "grad_norm": 0.8861804604530334,
      "learning_rate": 5e-05,
      "loss": 0.1015,
      "step": 605
    },
    {
      "epoch": 0.7096018735362998,
      "grad_norm": 1.0628374814987183,
      "learning_rate": 5e-05,
      "loss": 0.1691,
      "step": 606
    },
    {
      "epoch": 0.7107728337236534,
      "grad_norm": 0.7438756227493286,
      "learning_rate": 5e-05,
      "loss": 0.1255,
      "step": 607
    },
    {
      "epoch": 0.711943793911007,
      "grad_norm": 0.7371640205383301,
      "learning_rate": 5e-05,
      "loss": 0.1385,
      "step": 608
    },
    {
      "epoch": 0.7131147540983607,
      "grad_norm": 0.8912538290023804,
      "learning_rate": 5e-05,
      "loss": 0.08,
      "step": 609
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.1407160758972168,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 610
    },
    {
      "epoch": 0.7154566744730679,
      "grad_norm": 1.9877041578292847,
      "learning_rate": 5e-05,
      "loss": 0.3776,
      "step": 611
    },
    {
      "epoch": 0.7166276346604216,
      "grad_norm": 0.9028395414352417,
      "learning_rate": 5e-05,
      "loss": 0.1783,
      "step": 612
    },
    {
      "epoch": 0.7177985948477752,
      "grad_norm": 1.3240025043487549,
      "learning_rate": 5e-05,
      "loss": 0.1879,
      "step": 613
    },
    {
      "epoch": 0.7189695550351288,
      "grad_norm": 0.7219727635383606,
      "learning_rate": 5e-05,
      "loss": 0.0611,
      "step": 614
    },
    {
      "epoch": 0.7201405152224825,
      "grad_norm": 1.0348434448242188,
      "learning_rate": 5e-05,
      "loss": 0.1702,
      "step": 615
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 0.7272930145263672,
      "learning_rate": 5e-05,
      "loss": 0.1977,
      "step": 616
    },
    {
      "epoch": 0.7224824355971897,
      "grad_norm": 1.0008891820907593,
      "learning_rate": 5e-05,
      "loss": 0.1574,
      "step": 617
    },
    {
      "epoch": 0.7236533957845434,
      "grad_norm": 1.0001882314682007,
      "learning_rate": 5e-05,
      "loss": 0.153,
      "step": 618
    },
    {
      "epoch": 0.724824355971897,
      "grad_norm": 0.6567885279655457,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 619
    },
    {
      "epoch": 0.7259953161592506,
      "grad_norm": 0.769646942615509,
      "learning_rate": 5e-05,
      "loss": 0.1054,
      "step": 620
    },
    {
      "epoch": 0.7271662763466042,
      "grad_norm": 1.1967213153839111,
      "learning_rate": 5e-05,
      "loss": 0.1278,
      "step": 621
    },
    {
      "epoch": 0.7283372365339579,
      "grad_norm": 1.7226191759109497,
      "learning_rate": 5e-05,
      "loss": 0.2421,
      "step": 622
    },
    {
      "epoch": 0.7295081967213115,
      "grad_norm": 0.7754063010215759,
      "learning_rate": 5e-05,
      "loss": 0.1192,
      "step": 623
    },
    {
      "epoch": 0.7306791569086651,
      "grad_norm": 1.1296298503875732,
      "learning_rate": 5e-05,
      "loss": 0.2538,
      "step": 624
    },
    {
      "epoch": 0.7318501170960188,
      "grad_norm": 0.7849509716033936,
      "learning_rate": 5e-05,
      "loss": 0.1504,
      "step": 625
    },
    {
      "epoch": 0.7330210772833724,
      "grad_norm": 1.1280443668365479,
      "learning_rate": 5e-05,
      "loss": 0.1908,
      "step": 626
    },
    {
      "epoch": 0.734192037470726,
      "grad_norm": 1.424654483795166,
      "learning_rate": 5e-05,
      "loss": 0.2134,
      "step": 627
    },
    {
      "epoch": 0.7353629976580797,
      "grad_norm": 0.6482892632484436,
      "learning_rate": 5e-05,
      "loss": 0.1422,
      "step": 628
    },
    {
      "epoch": 0.7365339578454333,
      "grad_norm": 0.7104600667953491,
      "learning_rate": 5e-05,
      "loss": 0.1208,
      "step": 629
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 0.9005659818649292,
      "learning_rate": 5e-05,
      "loss": 0.1258,
      "step": 630
    },
    {
      "epoch": 0.7388758782201406,
      "grad_norm": 1.0040563344955444,
      "learning_rate": 5e-05,
      "loss": 0.2474,
      "step": 631
    },
    {
      "epoch": 0.7400468384074942,
      "grad_norm": 0.853834867477417,
      "learning_rate": 5e-05,
      "loss": 0.1935,
      "step": 632
    },
    {
      "epoch": 0.7412177985948478,
      "grad_norm": 0.7903426885604858,
      "learning_rate": 5e-05,
      "loss": 0.1789,
      "step": 633
    },
    {
      "epoch": 0.7423887587822015,
      "grad_norm": 0.8193129301071167,
      "learning_rate": 5e-05,
      "loss": 0.2035,
      "step": 634
    },
    {
      "epoch": 0.7435597189695551,
      "grad_norm": 1.3451457023620605,
      "learning_rate": 5e-05,
      "loss": 0.2881,
      "step": 635
    },
    {
      "epoch": 0.7447306791569087,
      "grad_norm": 0.9097744822502136,
      "learning_rate": 5e-05,
      "loss": 0.1718,
      "step": 636
    },
    {
      "epoch": 0.7459016393442623,
      "grad_norm": 0.873253345489502,
      "learning_rate": 5e-05,
      "loss": 0.2695,
      "step": 637
    },
    {
      "epoch": 0.747072599531616,
      "grad_norm": 0.8731308579444885,
      "learning_rate": 5e-05,
      "loss": 0.1933,
      "step": 638
    },
    {
      "epoch": 0.7482435597189696,
      "grad_norm": 0.9836427569389343,
      "learning_rate": 5e-05,
      "loss": 0.2533,
      "step": 639
    },
    {
      "epoch": 0.7494145199063232,
      "grad_norm": 0.924052357673645,
      "learning_rate": 5e-05,
      "loss": 0.1675,
      "step": 640
    },
    {
      "epoch": 0.7505854800936768,
      "grad_norm": 0.8491424918174744,
      "learning_rate": 5e-05,
      "loss": 0.1495,
      "step": 641
    },
    {
      "epoch": 0.7517564402810304,
      "grad_norm": 0.7065046429634094,
      "learning_rate": 5e-05,
      "loss": 0.1438,
      "step": 642
    },
    {
      "epoch": 0.752927400468384,
      "grad_norm": 0.612852156162262,
      "learning_rate": 5e-05,
      "loss": 0.0912,
      "step": 643
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 0.7391743063926697,
      "learning_rate": 5e-05,
      "loss": 0.1578,
      "step": 644
    },
    {
      "epoch": 0.7552693208430913,
      "grad_norm": 0.6544528603553772,
      "learning_rate": 5e-05,
      "loss": 0.1147,
      "step": 645
    },
    {
      "epoch": 0.7564402810304449,
      "grad_norm": 0.9500585198402405,
      "learning_rate": 5e-05,
      "loss": 0.0909,
      "step": 646
    },
    {
      "epoch": 0.7576112412177985,
      "grad_norm": 1.5143948793411255,
      "learning_rate": 5e-05,
      "loss": 0.2246,
      "step": 647
    },
    {
      "epoch": 0.7587822014051522,
      "grad_norm": 0.7884593605995178,
      "learning_rate": 5e-05,
      "loss": 0.1849,
      "step": 648
    },
    {
      "epoch": 0.7599531615925058,
      "grad_norm": 0.8435537815093994,
      "learning_rate": 5e-05,
      "loss": 0.1628,
      "step": 649
    },
    {
      "epoch": 0.7611241217798594,
      "grad_norm": 0.844500720500946,
      "learning_rate": 5e-05,
      "loss": 0.2437,
      "step": 650
    },
    {
      "epoch": 0.7622950819672131,
      "grad_norm": 0.7526556253433228,
      "learning_rate": 5e-05,
      "loss": 0.1704,
      "step": 651
    },
    {
      "epoch": 0.7634660421545667,
      "grad_norm": 1.4348502159118652,
      "learning_rate": 5e-05,
      "loss": 0.1055,
      "step": 652
    },
    {
      "epoch": 0.7646370023419203,
      "grad_norm": 0.6096806526184082,
      "learning_rate": 5e-05,
      "loss": 0.0699,
      "step": 653
    },
    {
      "epoch": 0.765807962529274,
      "grad_norm": 0.9042558670043945,
      "learning_rate": 5e-05,
      "loss": 0.2067,
      "step": 654
    },
    {
      "epoch": 0.7669789227166276,
      "grad_norm": 1.1446332931518555,
      "learning_rate": 5e-05,
      "loss": 0.2967,
      "step": 655
    },
    {
      "epoch": 0.7681498829039812,
      "grad_norm": 0.9869493246078491,
      "learning_rate": 5e-05,
      "loss": 0.093,
      "step": 656
    },
    {
      "epoch": 0.7693208430913349,
      "grad_norm": 0.8247200846672058,
      "learning_rate": 5e-05,
      "loss": 0.1263,
      "step": 657
    },
    {
      "epoch": 0.7704918032786885,
      "grad_norm": 1.1866343021392822,
      "learning_rate": 5e-05,
      "loss": 0.2201,
      "step": 658
    },
    {
      "epoch": 0.7716627634660421,
      "grad_norm": 0.6696946620941162,
      "learning_rate": 5e-05,
      "loss": 0.1788,
      "step": 659
    },
    {
      "epoch": 0.7728337236533958,
      "grad_norm": 0.49724048376083374,
      "learning_rate": 5e-05,
      "loss": 0.0852,
      "step": 660
    },
    {
      "epoch": 0.7740046838407494,
      "grad_norm": 0.8055618405342102,
      "learning_rate": 5e-05,
      "loss": 0.18,
      "step": 661
    },
    {
      "epoch": 0.775175644028103,
      "grad_norm": 0.735441267490387,
      "learning_rate": 5e-05,
      "loss": 0.2183,
      "step": 662
    },
    {
      "epoch": 0.7763466042154566,
      "grad_norm": 1.3882668018341064,
      "learning_rate": 5e-05,
      "loss": 0.2025,
      "step": 663
    },
    {
      "epoch": 0.7775175644028103,
      "grad_norm": 0.8654156923294067,
      "learning_rate": 5e-05,
      "loss": 0.0828,
      "step": 664
    },
    {
      "epoch": 0.7786885245901639,
      "grad_norm": 1.0568290948867798,
      "learning_rate": 5e-05,
      "loss": 0.243,
      "step": 665
    },
    {
      "epoch": 0.7798594847775175,
      "grad_norm": 1.2986196279525757,
      "learning_rate": 5e-05,
      "loss": 0.2621,
      "step": 666
    },
    {
      "epoch": 0.7810304449648712,
      "grad_norm": 0.6983702778816223,
      "learning_rate": 5e-05,
      "loss": 0.1305,
      "step": 667
    },
    {
      "epoch": 0.7822014051522248,
      "grad_norm": 1.0129828453063965,
      "learning_rate": 5e-05,
      "loss": 0.2173,
      "step": 668
    },
    {
      "epoch": 0.7833723653395784,
      "grad_norm": 1.025280237197876,
      "learning_rate": 5e-05,
      "loss": 0.3568,
      "step": 669
    },
    {
      "epoch": 0.7845433255269321,
      "grad_norm": 0.8790856599807739,
      "learning_rate": 5e-05,
      "loss": 0.1397,
      "step": 670
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 0.6403105854988098,
      "learning_rate": 5e-05,
      "loss": 0.0882,
      "step": 671
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 0.7766119837760925,
      "learning_rate": 5e-05,
      "loss": 0.1854,
      "step": 672
    },
    {
      "epoch": 0.788056206088993,
      "grad_norm": 0.5818421244621277,
      "learning_rate": 5e-05,
      "loss": 0.1277,
      "step": 673
    },
    {
      "epoch": 0.7892271662763466,
      "grad_norm": 1.0216426849365234,
      "learning_rate": 5e-05,
      "loss": 0.2396,
      "step": 674
    },
    {
      "epoch": 0.7903981264637002,
      "grad_norm": 0.8263012766838074,
      "learning_rate": 5e-05,
      "loss": 0.1752,
      "step": 675
    },
    {
      "epoch": 0.7915690866510539,
      "grad_norm": 1.6060905456542969,
      "learning_rate": 5e-05,
      "loss": 0.117,
      "step": 676
    },
    {
      "epoch": 0.7927400468384075,
      "grad_norm": 0.879615306854248,
      "learning_rate": 5e-05,
      "loss": 0.2624,
      "step": 677
    },
    {
      "epoch": 0.7939110070257611,
      "grad_norm": 1.0216054916381836,
      "learning_rate": 5e-05,
      "loss": 0.1049,
      "step": 678
    },
    {
      "epoch": 0.7950819672131147,
      "grad_norm": 1.9842736721038818,
      "learning_rate": 5e-05,
      "loss": 0.2738,
      "step": 679
    },
    {
      "epoch": 0.7962529274004684,
      "grad_norm": 0.8737586736679077,
      "learning_rate": 5e-05,
      "loss": 0.1784,
      "step": 680
    },
    {
      "epoch": 0.797423887587822,
      "grad_norm": 0.6279183030128479,
      "learning_rate": 5e-05,
      "loss": 0.1227,
      "step": 681
    },
    {
      "epoch": 0.7985948477751756,
      "grad_norm": 1.1246403455734253,
      "learning_rate": 5e-05,
      "loss": 0.113,
      "step": 682
    },
    {
      "epoch": 0.7997658079625293,
      "grad_norm": 0.7037845253944397,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 683
    },
    {
      "epoch": 0.8009367681498829,
      "grad_norm": 0.9184229373931885,
      "learning_rate": 5e-05,
      "loss": 0.194,
      "step": 684
    },
    {
      "epoch": 0.8021077283372365,
      "grad_norm": 0.970826268196106,
      "learning_rate": 5e-05,
      "loss": 0.1357,
      "step": 685
    },
    {
      "epoch": 0.8032786885245902,
      "grad_norm": 0.938351035118103,
      "learning_rate": 5e-05,
      "loss": 0.2285,
      "step": 686
    },
    {
      "epoch": 0.8044496487119438,
      "grad_norm": 1.0612776279449463,
      "learning_rate": 5e-05,
      "loss": 0.2786,
      "step": 687
    },
    {
      "epoch": 0.8056206088992974,
      "grad_norm": 0.712609589099884,
      "learning_rate": 5e-05,
      "loss": 0.1269,
      "step": 688
    },
    {
      "epoch": 0.8067915690866511,
      "grad_norm": 1.114572286605835,
      "learning_rate": 5e-05,
      "loss": 0.1809,
      "step": 689
    },
    {
      "epoch": 0.8079625292740047,
      "grad_norm": 0.8736885190010071,
      "learning_rate": 5e-05,
      "loss": 0.1543,
      "step": 690
    },
    {
      "epoch": 0.8091334894613583,
      "grad_norm": 0.9848564267158508,
      "learning_rate": 5e-05,
      "loss": 0.1314,
      "step": 691
    },
    {
      "epoch": 0.810304449648712,
      "grad_norm": 1.1189693212509155,
      "learning_rate": 5e-05,
      "loss": 0.1722,
      "step": 692
    },
    {
      "epoch": 0.8114754098360656,
      "grad_norm": 0.6680352091789246,
      "learning_rate": 5e-05,
      "loss": 0.1365,
      "step": 693
    },
    {
      "epoch": 0.8126463700234192,
      "grad_norm": 0.8788827061653137,
      "learning_rate": 5e-05,
      "loss": 0.1218,
      "step": 694
    },
    {
      "epoch": 0.8138173302107728,
      "grad_norm": 1.2068684101104736,
      "learning_rate": 5e-05,
      "loss": 0.2609,
      "step": 695
    },
    {
      "epoch": 0.8149882903981265,
      "grad_norm": 1.3974781036376953,
      "learning_rate": 5e-05,
      "loss": 0.1293,
      "step": 696
    },
    {
      "epoch": 0.8161592505854801,
      "grad_norm": 0.7264589071273804,
      "learning_rate": 5e-05,
      "loss": 0.1182,
      "step": 697
    },
    {
      "epoch": 0.8173302107728337,
      "grad_norm": 1.1744163036346436,
      "learning_rate": 5e-05,
      "loss": 0.1413,
      "step": 698
    },
    {
      "epoch": 0.8185011709601874,
      "grad_norm": 1.3611676692962646,
      "learning_rate": 5e-05,
      "loss": 0.1683,
      "step": 699
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 0.9499055743217468,
      "learning_rate": 5e-05,
      "loss": 0.1446,
      "step": 700
    },
    {
      "epoch": 0.8208430913348946,
      "grad_norm": 1.555822491645813,
      "learning_rate": 5e-05,
      "loss": 0.2705,
      "step": 701
    },
    {
      "epoch": 0.8220140515222483,
      "grad_norm": 0.8600680828094482,
      "learning_rate": 5e-05,
      "loss": 0.1337,
      "step": 702
    },
    {
      "epoch": 0.8231850117096019,
      "grad_norm": 0.78726726770401,
      "learning_rate": 5e-05,
      "loss": 0.1466,
      "step": 703
    },
    {
      "epoch": 0.8243559718969555,
      "grad_norm": 0.7314836978912354,
      "learning_rate": 5e-05,
      "loss": 0.0839,
      "step": 704
    },
    {
      "epoch": 0.8255269320843092,
      "grad_norm": 1.8219388723373413,
      "learning_rate": 5e-05,
      "loss": 0.2871,
      "step": 705
    },
    {
      "epoch": 0.8266978922716628,
      "grad_norm": 0.858414351940155,
      "learning_rate": 5e-05,
      "loss": 0.207,
      "step": 706
    },
    {
      "epoch": 0.8278688524590164,
      "grad_norm": 1.431883454322815,
      "learning_rate": 5e-05,
      "loss": 0.1332,
      "step": 707
    },
    {
      "epoch": 0.8290398126463701,
      "grad_norm": 1.0361522436141968,
      "learning_rate": 5e-05,
      "loss": 0.1094,
      "step": 708
    },
    {
      "epoch": 0.8302107728337237,
      "grad_norm": 0.7467823028564453,
      "learning_rate": 5e-05,
      "loss": 0.1476,
      "step": 709
    },
    {
      "epoch": 0.8313817330210773,
      "grad_norm": 1.1464723348617554,
      "learning_rate": 5e-05,
      "loss": 0.1357,
      "step": 710
    },
    {
      "epoch": 0.832552693208431,
      "grad_norm": 0.7047457098960876,
      "learning_rate": 5e-05,
      "loss": 0.1744,
      "step": 711
    },
    {
      "epoch": 0.8337236533957846,
      "grad_norm": 0.829155445098877,
      "learning_rate": 5e-05,
      "loss": 0.1662,
      "step": 712
    },
    {
      "epoch": 0.8348946135831382,
      "grad_norm": 0.9245274662971497,
      "learning_rate": 5e-05,
      "loss": 0.1239,
      "step": 713
    },
    {
      "epoch": 0.8360655737704918,
      "grad_norm": 1.049976110458374,
      "learning_rate": 5e-05,
      "loss": 0.1382,
      "step": 714
    },
    {
      "epoch": 0.8372365339578455,
      "grad_norm": 1.0422770977020264,
      "learning_rate": 5e-05,
      "loss": 0.1763,
      "step": 715
    },
    {
      "epoch": 0.8384074941451991,
      "grad_norm": 0.8838465213775635,
      "learning_rate": 5e-05,
      "loss": 0.1106,
      "step": 716
    },
    {
      "epoch": 0.8395784543325527,
      "grad_norm": 1.224457859992981,
      "learning_rate": 5e-05,
      "loss": 0.1748,
      "step": 717
    },
    {
      "epoch": 0.8407494145199064,
      "grad_norm": 0.7461263537406921,
      "learning_rate": 5e-05,
      "loss": 0.1225,
      "step": 718
    },
    {
      "epoch": 0.84192037470726,
      "grad_norm": 0.8512495756149292,
      "learning_rate": 5e-05,
      "loss": 0.108,
      "step": 719
    },
    {
      "epoch": 0.8430913348946136,
      "grad_norm": 1.212647795677185,
      "learning_rate": 5e-05,
      "loss": 0.184,
      "step": 720
    },
    {
      "epoch": 0.8442622950819673,
      "grad_norm": 0.7723935842514038,
      "learning_rate": 5e-05,
      "loss": 0.0967,
      "step": 721
    },
    {
      "epoch": 0.8454332552693209,
      "grad_norm": 1.1757005453109741,
      "learning_rate": 5e-05,
      "loss": 0.1874,
      "step": 722
    },
    {
      "epoch": 0.8466042154566745,
      "grad_norm": 1.1598459482192993,
      "learning_rate": 5e-05,
      "loss": 0.1072,
      "step": 723
    },
    {
      "epoch": 0.8477751756440282,
      "grad_norm": 1.131317138671875,
      "learning_rate": 5e-05,
      "loss": 0.2341,
      "step": 724
    },
    {
      "epoch": 0.8489461358313818,
      "grad_norm": 0.7896918654441833,
      "learning_rate": 5e-05,
      "loss": 0.1001,
      "step": 725
    },
    {
      "epoch": 0.8501170960187353,
      "grad_norm": 1.0524171590805054,
      "learning_rate": 5e-05,
      "loss": 0.1253,
      "step": 726
    },
    {
      "epoch": 0.8512880562060889,
      "grad_norm": 0.8277357220649719,
      "learning_rate": 5e-05,
      "loss": 0.2328,
      "step": 727
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 0.6579596996307373,
      "learning_rate": 5e-05,
      "loss": 0.1265,
      "step": 728
    },
    {
      "epoch": 0.8536299765807962,
      "grad_norm": 0.8081661462783813,
      "learning_rate": 5e-05,
      "loss": 0.2766,
      "step": 729
    },
    {
      "epoch": 0.8548009367681498,
      "grad_norm": 0.7135605812072754,
      "learning_rate": 5e-05,
      "loss": 0.1276,
      "step": 730
    },
    {
      "epoch": 0.8559718969555035,
      "grad_norm": 0.81215900182724,
      "learning_rate": 5e-05,
      "loss": 0.1255,
      "step": 731
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.9921958446502686,
      "learning_rate": 5e-05,
      "loss": 0.162,
      "step": 732
    },
    {
      "epoch": 0.8583138173302107,
      "grad_norm": 0.6906371712684631,
      "learning_rate": 5e-05,
      "loss": 0.2137,
      "step": 733
    },
    {
      "epoch": 0.8594847775175644,
      "grad_norm": 1.296022891998291,
      "learning_rate": 5e-05,
      "loss": 0.2039,
      "step": 734
    },
    {
      "epoch": 0.860655737704918,
      "grad_norm": 0.7657315135002136,
      "learning_rate": 5e-05,
      "loss": 0.1696,
      "step": 735
    },
    {
      "epoch": 0.8618266978922716,
      "grad_norm": 1.0106769800186157,
      "learning_rate": 5e-05,
      "loss": 0.0725,
      "step": 736
    },
    {
      "epoch": 0.8629976580796253,
      "grad_norm": 0.5375091433525085,
      "learning_rate": 5e-05,
      "loss": 0.0784,
      "step": 737
    },
    {
      "epoch": 0.8641686182669789,
      "grad_norm": 0.6747841238975525,
      "learning_rate": 5e-05,
      "loss": 0.1741,
      "step": 738
    },
    {
      "epoch": 0.8653395784543325,
      "grad_norm": 1.3321484327316284,
      "learning_rate": 5e-05,
      "loss": 0.201,
      "step": 739
    },
    {
      "epoch": 0.8665105386416861,
      "grad_norm": 0.8017879128456116,
      "learning_rate": 5e-05,
      "loss": 0.1138,
      "step": 740
    },
    {
      "epoch": 0.8676814988290398,
      "grad_norm": 1.0600186586380005,
      "learning_rate": 5e-05,
      "loss": 0.1421,
      "step": 741
    },
    {
      "epoch": 0.8688524590163934,
      "grad_norm": 0.5936041474342346,
      "learning_rate": 5e-05,
      "loss": 0.1041,
      "step": 742
    },
    {
      "epoch": 0.870023419203747,
      "grad_norm": 1.1691577434539795,
      "learning_rate": 5e-05,
      "loss": 0.1134,
      "step": 743
    },
    {
      "epoch": 0.8711943793911007,
      "grad_norm": 0.5118800401687622,
      "learning_rate": 5e-05,
      "loss": 0.0838,
      "step": 744
    },
    {
      "epoch": 0.8723653395784543,
      "grad_norm": 0.665703296661377,
      "learning_rate": 5e-05,
      "loss": 0.1016,
      "step": 745
    },
    {
      "epoch": 0.8735362997658079,
      "grad_norm": 0.8731909394264221,
      "learning_rate": 5e-05,
      "loss": 0.1374,
      "step": 746
    },
    {
      "epoch": 0.8747072599531616,
      "grad_norm": 0.9629630446434021,
      "learning_rate": 5e-05,
      "loss": 0.1022,
      "step": 747
    },
    {
      "epoch": 0.8758782201405152,
      "grad_norm": 0.7915377020835876,
      "learning_rate": 5e-05,
      "loss": 0.1235,
      "step": 748
    },
    {
      "epoch": 0.8770491803278688,
      "grad_norm": 1.8968100547790527,
      "learning_rate": 5e-05,
      "loss": 0.2527,
      "step": 749
    },
    {
      "epoch": 0.8782201405152225,
      "grad_norm": 0.8985297679901123,
      "learning_rate": 5e-05,
      "loss": 0.1705,
      "step": 750
    },
    {
      "epoch": 0.8793911007025761,
      "grad_norm": 0.8709537982940674,
      "learning_rate": 5e-05,
      "loss": 0.2563,
      "step": 751
    },
    {
      "epoch": 0.8805620608899297,
      "grad_norm": 0.9341668486595154,
      "learning_rate": 5e-05,
      "loss": 0.1155,
      "step": 752
    },
    {
      "epoch": 0.8817330210772834,
      "grad_norm": 0.6422825455665588,
      "learning_rate": 5e-05,
      "loss": 0.0874,
      "step": 753
    },
    {
      "epoch": 0.882903981264637,
      "grad_norm": 0.9396832585334778,
      "learning_rate": 5e-05,
      "loss": 0.1434,
      "step": 754
    },
    {
      "epoch": 0.8840749414519906,
      "grad_norm": 1.435874104499817,
      "learning_rate": 5e-05,
      "loss": 0.2221,
      "step": 755
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 0.6153630614280701,
      "learning_rate": 5e-05,
      "loss": 0.1315,
      "step": 756
    },
    {
      "epoch": 0.8864168618266979,
      "grad_norm": 0.7705073952674866,
      "learning_rate": 5e-05,
      "loss": 0.1417,
      "step": 757
    },
    {
      "epoch": 0.8875878220140515,
      "grad_norm": 1.207606554031372,
      "learning_rate": 5e-05,
      "loss": 0.1396,
      "step": 758
    },
    {
      "epoch": 0.8887587822014051,
      "grad_norm": 0.6375484466552734,
      "learning_rate": 5e-05,
      "loss": 0.1227,
      "step": 759
    },
    {
      "epoch": 0.8899297423887588,
      "grad_norm": 1.035513997077942,
      "learning_rate": 5e-05,
      "loss": 0.2319,
      "step": 760
    },
    {
      "epoch": 0.8911007025761124,
      "grad_norm": 2.000821113586426,
      "learning_rate": 5e-05,
      "loss": 0.1121,
      "step": 761
    },
    {
      "epoch": 0.892271662763466,
      "grad_norm": 0.9666226506233215,
      "learning_rate": 5e-05,
      "loss": 0.1351,
      "step": 762
    },
    {
      "epoch": 0.8934426229508197,
      "grad_norm": 1.1197479963302612,
      "learning_rate": 5e-05,
      "loss": 0.2371,
      "step": 763
    },
    {
      "epoch": 0.8946135831381733,
      "grad_norm": 1.6719293594360352,
      "learning_rate": 5e-05,
      "loss": 0.2305,
      "step": 764
    },
    {
      "epoch": 0.8957845433255269,
      "grad_norm": 0.8361876606941223,
      "learning_rate": 5e-05,
      "loss": 0.1851,
      "step": 765
    },
    {
      "epoch": 0.8969555035128806,
      "grad_norm": 0.9023621678352356,
      "learning_rate": 5e-05,
      "loss": 0.1979,
      "step": 766
    },
    {
      "epoch": 0.8981264637002342,
      "grad_norm": 1.636098861694336,
      "learning_rate": 5e-05,
      "loss": 0.253,
      "step": 767
    },
    {
      "epoch": 0.8992974238875878,
      "grad_norm": 1.675679326057434,
      "learning_rate": 5e-05,
      "loss": 0.2801,
      "step": 768
    },
    {
      "epoch": 0.9004683840749415,
      "grad_norm": 2.326265811920166,
      "learning_rate": 5e-05,
      "loss": 0.2001,
      "step": 769
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 0.9213147759437561,
      "learning_rate": 5e-05,
      "loss": 0.1734,
      "step": 770
    },
    {
      "epoch": 0.9028103044496487,
      "grad_norm": 0.6494961977005005,
      "learning_rate": 5e-05,
      "loss": 0.0691,
      "step": 771
    },
    {
      "epoch": 0.9039812646370023,
      "grad_norm": 0.9555619359016418,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 772
    },
    {
      "epoch": 0.905152224824356,
      "grad_norm": 0.9375364780426025,
      "learning_rate": 5e-05,
      "loss": 0.3203,
      "step": 773
    },
    {
      "epoch": 0.9063231850117096,
      "grad_norm": 0.908539891242981,
      "learning_rate": 5e-05,
      "loss": 0.1316,
      "step": 774
    },
    {
      "epoch": 0.9074941451990632,
      "grad_norm": 0.7463377118110657,
      "learning_rate": 5e-05,
      "loss": 0.1709,
      "step": 775
    },
    {
      "epoch": 0.9086651053864169,
      "grad_norm": 0.9261021614074707,
      "learning_rate": 5e-05,
      "loss": 0.1967,
      "step": 776
    },
    {
      "epoch": 0.9098360655737705,
      "grad_norm": 0.7774578332901001,
      "learning_rate": 5e-05,
      "loss": 0.1147,
      "step": 777
    },
    {
      "epoch": 0.9110070257611241,
      "grad_norm": 0.8867047429084778,
      "learning_rate": 5e-05,
      "loss": 0.1291,
      "step": 778
    },
    {
      "epoch": 0.9121779859484778,
      "grad_norm": 1.0477503538131714,
      "learning_rate": 5e-05,
      "loss": 0.2325,
      "step": 779
    },
    {
      "epoch": 0.9133489461358314,
      "grad_norm": 0.70510333776474,
      "learning_rate": 5e-05,
      "loss": 0.1438,
      "step": 780
    },
    {
      "epoch": 0.914519906323185,
      "grad_norm": 0.6745645403862,
      "learning_rate": 5e-05,
      "loss": 0.1102,
      "step": 781
    },
    {
      "epoch": 0.9156908665105387,
      "grad_norm": 1.125931978225708,
      "learning_rate": 5e-05,
      "loss": 0.1918,
      "step": 782
    },
    {
      "epoch": 0.9168618266978923,
      "grad_norm": 1.0188099145889282,
      "learning_rate": 5e-05,
      "loss": 0.1856,
      "step": 783
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 1.4204185009002686,
      "learning_rate": 5e-05,
      "loss": 0.2831,
      "step": 784
    },
    {
      "epoch": 0.9192037470725996,
      "grad_norm": 0.6433759927749634,
      "learning_rate": 5e-05,
      "loss": 0.1092,
      "step": 785
    },
    {
      "epoch": 0.9203747072599532,
      "grad_norm": 0.7168794870376587,
      "learning_rate": 5e-05,
      "loss": 0.0889,
      "step": 786
    },
    {
      "epoch": 0.9215456674473068,
      "grad_norm": 0.5688297152519226,
      "learning_rate": 5e-05,
      "loss": 0.1245,
      "step": 787
    },
    {
      "epoch": 0.9227166276346604,
      "grad_norm": 0.8923586010932922,
      "learning_rate": 5e-05,
      "loss": 0.1234,
      "step": 788
    },
    {
      "epoch": 0.9238875878220141,
      "grad_norm": 0.8318002223968506,
      "learning_rate": 5e-05,
      "loss": 0.1461,
      "step": 789
    },
    {
      "epoch": 0.9250585480093677,
      "grad_norm": 1.2440662384033203,
      "learning_rate": 5e-05,
      "loss": 0.1314,
      "step": 790
    },
    {
      "epoch": 0.9262295081967213,
      "grad_norm": 0.8844910264015198,
      "learning_rate": 5e-05,
      "loss": 0.2005,
      "step": 791
    },
    {
      "epoch": 0.927400468384075,
      "grad_norm": 1.09750235080719,
      "learning_rate": 5e-05,
      "loss": 0.1975,
      "step": 792
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 1.141693115234375,
      "learning_rate": 5e-05,
      "loss": 0.1309,
      "step": 793
    },
    {
      "epoch": 0.9297423887587822,
      "grad_norm": 1.3598610162734985,
      "learning_rate": 5e-05,
      "loss": 0.1634,
      "step": 794
    },
    {
      "epoch": 0.9309133489461359,
      "grad_norm": 0.6043835878372192,
      "learning_rate": 5e-05,
      "loss": 0.0606,
      "step": 795
    },
    {
      "epoch": 0.9320843091334895,
      "grad_norm": 0.6228625774383545,
      "learning_rate": 5e-05,
      "loss": 0.0739,
      "step": 796
    },
    {
      "epoch": 0.9332552693208431,
      "grad_norm": 0.8068194389343262,
      "learning_rate": 5e-05,
      "loss": 0.1265,
      "step": 797
    },
    {
      "epoch": 0.9344262295081968,
      "grad_norm": 0.6624938249588013,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 798
    },
    {
      "epoch": 0.9355971896955504,
      "grad_norm": 0.6747577786445618,
      "learning_rate": 5e-05,
      "loss": 0.0702,
      "step": 799
    },
    {
      "epoch": 0.936768149882904,
      "grad_norm": 0.6066030859947205,
      "learning_rate": 5e-05,
      "loss": 0.0568,
      "step": 800
    },
    {
      "epoch": 0.9379391100702577,
      "grad_norm": 0.6753064393997192,
      "learning_rate": 5e-05,
      "loss": 0.0962,
      "step": 801
    },
    {
      "epoch": 0.9391100702576113,
      "grad_norm": 1.3467929363250732,
      "learning_rate": 5e-05,
      "loss": 0.1753,
      "step": 802
    },
    {
      "epoch": 0.9402810304449649,
      "grad_norm": 0.9639670848846436,
      "learning_rate": 5e-05,
      "loss": 0.2148,
      "step": 803
    },
    {
      "epoch": 0.9414519906323185,
      "grad_norm": 0.724403977394104,
      "learning_rate": 5e-05,
      "loss": 0.1094,
      "step": 804
    },
    {
      "epoch": 0.9426229508196722,
      "grad_norm": 1.031076192855835,
      "learning_rate": 5e-05,
      "loss": 0.1348,
      "step": 805
    },
    {
      "epoch": 0.9437939110070258,
      "grad_norm": 1.46358323097229,
      "learning_rate": 5e-05,
      "loss": 0.2705,
      "step": 806
    },
    {
      "epoch": 0.9449648711943794,
      "grad_norm": 1.0371812582015991,
      "learning_rate": 5e-05,
      "loss": 0.1105,
      "step": 807
    },
    {
      "epoch": 0.9461358313817331,
      "grad_norm": 1.1391572952270508,
      "learning_rate": 5e-05,
      "loss": 0.1594,
      "step": 808
    },
    {
      "epoch": 0.9473067915690867,
      "grad_norm": 1.1106159687042236,
      "learning_rate": 5e-05,
      "loss": 0.1282,
      "step": 809
    },
    {
      "epoch": 0.9484777517564403,
      "grad_norm": 0.5611034631729126,
      "learning_rate": 5e-05,
      "loss": 0.0853,
      "step": 810
    },
    {
      "epoch": 0.949648711943794,
      "grad_norm": 1.0544344186782837,
      "learning_rate": 5e-05,
      "loss": 0.1934,
      "step": 811
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 0.786737322807312,
      "learning_rate": 5e-05,
      "loss": 0.0817,
      "step": 812
    },
    {
      "epoch": 0.9519906323185011,
      "grad_norm": 1.0951716899871826,
      "learning_rate": 5e-05,
      "loss": 0.1397,
      "step": 813
    },
    {
      "epoch": 0.9531615925058547,
      "grad_norm": 1.2383010387420654,
      "learning_rate": 5e-05,
      "loss": 0.1983,
      "step": 814
    },
    {
      "epoch": 0.9543325526932084,
      "grad_norm": 0.9906324744224548,
      "learning_rate": 5e-05,
      "loss": 0.1329,
      "step": 815
    },
    {
      "epoch": 0.955503512880562,
      "grad_norm": 1.1274199485778809,
      "learning_rate": 5e-05,
      "loss": 0.1485,
      "step": 816
    },
    {
      "epoch": 0.9566744730679156,
      "grad_norm": 0.6582203507423401,
      "learning_rate": 5e-05,
      "loss": 0.0575,
      "step": 817
    },
    {
      "epoch": 0.9578454332552693,
      "grad_norm": 0.9612882733345032,
      "learning_rate": 5e-05,
      "loss": 0.1599,
      "step": 818
    },
    {
      "epoch": 0.9590163934426229,
      "grad_norm": 1.3844660520553589,
      "learning_rate": 5e-05,
      "loss": 0.2049,
      "step": 819
    },
    {
      "epoch": 0.9601873536299765,
      "grad_norm": 0.9908501505851746,
      "learning_rate": 5e-05,
      "loss": 0.2085,
      "step": 820
    },
    {
      "epoch": 0.9613583138173302,
      "grad_norm": 1.2106636762619019,
      "learning_rate": 5e-05,
      "loss": 0.163,
      "step": 821
    },
    {
      "epoch": 0.9625292740046838,
      "grad_norm": 0.813835084438324,
      "learning_rate": 5e-05,
      "loss": 0.2262,
      "step": 822
    },
    {
      "epoch": 0.9637002341920374,
      "grad_norm": 0.9729620814323425,
      "learning_rate": 5e-05,
      "loss": 0.1522,
      "step": 823
    },
    {
      "epoch": 0.9648711943793911,
      "grad_norm": 0.7134082913398743,
      "learning_rate": 5e-05,
      "loss": 0.0813,
      "step": 824
    },
    {
      "epoch": 0.9660421545667447,
      "grad_norm": 0.9377176761627197,
      "learning_rate": 5e-05,
      "loss": 0.2002,
      "step": 825
    },
    {
      "epoch": 0.9672131147540983,
      "grad_norm": 0.8956932425498962,
      "learning_rate": 5e-05,
      "loss": 0.1664,
      "step": 826
    },
    {
      "epoch": 0.968384074941452,
      "grad_norm": 0.6832823157310486,
      "learning_rate": 5e-05,
      "loss": 0.1627,
      "step": 827
    },
    {
      "epoch": 0.9695550351288056,
      "grad_norm": 1.278572678565979,
      "learning_rate": 5e-05,
      "loss": 0.2274,
      "step": 828
    },
    {
      "epoch": 0.9707259953161592,
      "grad_norm": 0.7036656141281128,
      "learning_rate": 5e-05,
      "loss": 0.2095,
      "step": 829
    },
    {
      "epoch": 0.9718969555035128,
      "grad_norm": 0.8799889087677002,
      "learning_rate": 5e-05,
      "loss": 0.1449,
      "step": 830
    },
    {
      "epoch": 0.9730679156908665,
      "grad_norm": 1.1372276544570923,
      "learning_rate": 5e-05,
      "loss": 0.216,
      "step": 831
    },
    {
      "epoch": 0.9742388758782201,
      "grad_norm": 1.8946595191955566,
      "learning_rate": 5e-05,
      "loss": 0.2272,
      "step": 832
    },
    {
      "epoch": 0.9754098360655737,
      "grad_norm": 1.057363510131836,
      "learning_rate": 5e-05,
      "loss": 0.2256,
      "step": 833
    },
    {
      "epoch": 0.9765807962529274,
      "grad_norm": 0.933983564376831,
      "learning_rate": 5e-05,
      "loss": 0.1941,
      "step": 834
    },
    {
      "epoch": 0.977751756440281,
      "grad_norm": 0.7881125211715698,
      "learning_rate": 5e-05,
      "loss": 0.2232,
      "step": 835
    },
    {
      "epoch": 0.9789227166276346,
      "grad_norm": 0.6996781826019287,
      "learning_rate": 5e-05,
      "loss": 0.1177,
      "step": 836
    },
    {
      "epoch": 0.9800936768149883,
      "grad_norm": 0.601989209651947,
      "learning_rate": 5e-05,
      "loss": 0.0726,
      "step": 837
    },
    {
      "epoch": 0.9812646370023419,
      "grad_norm": 1.2323076725006104,
      "learning_rate": 5e-05,
      "loss": 0.148,
      "step": 838
    },
    {
      "epoch": 0.9824355971896955,
      "grad_norm": 0.6833469867706299,
      "learning_rate": 5e-05,
      "loss": 0.1533,
      "step": 839
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 0.7528191804885864,
      "learning_rate": 5e-05,
      "loss": 0.1837,
      "step": 840
    },
    {
      "epoch": 0.9847775175644028,
      "grad_norm": 0.7078287601470947,
      "learning_rate": 5e-05,
      "loss": 0.1699,
      "step": 841
    },
    {
      "epoch": 0.9859484777517564,
      "grad_norm": 1.1291455030441284,
      "learning_rate": 5e-05,
      "loss": 0.2509,
      "step": 842
    },
    {
      "epoch": 0.9871194379391101,
      "grad_norm": 0.6813889145851135,
      "learning_rate": 5e-05,
      "loss": 0.0695,
      "step": 843
    },
    {
      "epoch": 0.9882903981264637,
      "grad_norm": 0.5850104093551636,
      "learning_rate": 5e-05,
      "loss": 0.0592,
      "step": 844
    },
    {
      "epoch": 0.9894613583138173,
      "grad_norm": 0.95611971616745,
      "learning_rate": 5e-05,
      "loss": 0.1587,
      "step": 845
    },
    {
      "epoch": 0.990632318501171,
      "grad_norm": 0.6057126522064209,
      "learning_rate": 5e-05,
      "loss": 0.0994,
      "step": 846
    },
    {
      "epoch": 0.9918032786885246,
      "grad_norm": 0.5675015449523926,
      "learning_rate": 5e-05,
      "loss": 0.0884,
      "step": 847
    },
    {
      "epoch": 0.9929742388758782,
      "grad_norm": 0.7180892825126648,
      "learning_rate": 5e-05,
      "loss": 0.2253,
      "step": 848
    },
    {
      "epoch": 0.9941451990632318,
      "grad_norm": 0.6174193620681763,
      "learning_rate": 5e-05,
      "loss": 0.106,
      "step": 849
    },
    {
      "epoch": 0.9953161592505855,
      "grad_norm": 0.802741289138794,
      "learning_rate": 5e-05,
      "loss": 0.1674,
      "step": 850
    },
    {
      "epoch": 0.9964871194379391,
      "grad_norm": 1.3539320230484009,
      "learning_rate": 5e-05,
      "loss": 0.4109,
      "step": 851
    },
    {
      "epoch": 0.9976580796252927,
      "grad_norm": 1.0195598602294922,
      "learning_rate": 5e-05,
      "loss": 0.1996,
      "step": 852
    },
    {
      "epoch": 0.9988290398126464,
      "grad_norm": 0.7897893786430359,
      "learning_rate": 5e-05,
      "loss": 0.167,
      "step": 853
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7450163960456848,
      "learning_rate": 5e-05,
      "loss": 0.1421,
      "step": 854
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.17316210269927979,
      "eval_runtime": 46.3073,
      "eval_samples_per_second": 18.053,
      "eval_steps_per_second": 2.267,
      "step": 854
    },
    {
      "epoch": 1.0011709601873535,
      "grad_norm": 1.128117322921753,
      "learning_rate": 5e-05,
      "loss": 0.2138,
      "step": 855
    },
    {
      "epoch": 1.0023419203747073,
      "grad_norm": 0.7026335000991821,
      "learning_rate": 5e-05,
      "loss": 0.1578,
      "step": 856
    },
    {
      "epoch": 1.0035128805620608,
      "grad_norm": 1.235532522201538,
      "learning_rate": 5e-05,
      "loss": 0.1406,
      "step": 857
    },
    {
      "epoch": 1.0046838407494145,
      "grad_norm": 0.8263527750968933,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 858
    },
    {
      "epoch": 1.005854800936768,
      "grad_norm": 0.7802268862724304,
      "learning_rate": 5e-05,
      "loss": 0.1375,
      "step": 859
    },
    {
      "epoch": 1.0070257611241218,
      "grad_norm": 0.9842966794967651,
      "learning_rate": 5e-05,
      "loss": 0.1229,
      "step": 860
    },
    {
      "epoch": 1.0081967213114753,
      "grad_norm": 1.0392683744430542,
      "learning_rate": 5e-05,
      "loss": 0.3081,
      "step": 861
    },
    {
      "epoch": 1.009367681498829,
      "grad_norm": 0.7185857892036438,
      "learning_rate": 5e-05,
      "loss": 0.089,
      "step": 862
    },
    {
      "epoch": 1.0105386416861826,
      "grad_norm": 0.5741665959358215,
      "learning_rate": 5e-05,
      "loss": 0.0779,
      "step": 863
    },
    {
      "epoch": 1.0117096018735363,
      "grad_norm": 0.8186947703361511,
      "learning_rate": 5e-05,
      "loss": 0.1333,
      "step": 864
    },
    {
      "epoch": 1.0128805620608898,
      "grad_norm": 0.8716706037521362,
      "learning_rate": 5e-05,
      "loss": 0.1759,
      "step": 865
    },
    {
      "epoch": 1.0140515222482436,
      "grad_norm": 0.6826083064079285,
      "learning_rate": 5e-05,
      "loss": 0.1588,
      "step": 866
    },
    {
      "epoch": 1.015222482435597,
      "grad_norm": 0.8217223882675171,
      "learning_rate": 5e-05,
      "loss": 0.1446,
      "step": 867
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 0.8051192760467529,
      "learning_rate": 5e-05,
      "loss": 0.0902,
      "step": 868
    },
    {
      "epoch": 1.0175644028103044,
      "grad_norm": 0.7797974348068237,
      "learning_rate": 5e-05,
      "loss": 0.0955,
      "step": 869
    },
    {
      "epoch": 1.018735362997658,
      "grad_norm": 0.6702274680137634,
      "learning_rate": 5e-05,
      "loss": 0.119,
      "step": 870
    },
    {
      "epoch": 1.0199063231850116,
      "grad_norm": 0.8820955753326416,
      "learning_rate": 5e-05,
      "loss": 0.0978,
      "step": 871
    },
    {
      "epoch": 1.0210772833723654,
      "grad_norm": 0.8459568023681641,
      "learning_rate": 5e-05,
      "loss": 0.1174,
      "step": 872
    },
    {
      "epoch": 1.0222482435597189,
      "grad_norm": 0.6110239624977112,
      "learning_rate": 5e-05,
      "loss": 0.1541,
      "step": 873
    },
    {
      "epoch": 1.0234192037470726,
      "grad_norm": 1.1429730653762817,
      "learning_rate": 5e-05,
      "loss": 0.1126,
      "step": 874
    },
    {
      "epoch": 1.0245901639344261,
      "grad_norm": 0.7912877798080444,
      "learning_rate": 5e-05,
      "loss": 0.1292,
      "step": 875
    },
    {
      "epoch": 1.0257611241217799,
      "grad_norm": 0.9373376369476318,
      "learning_rate": 5e-05,
      "loss": 0.1411,
      "step": 876
    },
    {
      "epoch": 1.0269320843091334,
      "grad_norm": 0.6693671941757202,
      "learning_rate": 5e-05,
      "loss": 0.0989,
      "step": 877
    },
    {
      "epoch": 1.0281030444964872,
      "grad_norm": 1.277024269104004,
      "learning_rate": 5e-05,
      "loss": 0.1827,
      "step": 878
    },
    {
      "epoch": 1.0292740046838407,
      "grad_norm": 1.0279786586761475,
      "learning_rate": 5e-05,
      "loss": 0.0866,
      "step": 879
    },
    {
      "epoch": 1.0304449648711944,
      "grad_norm": 1.0189989805221558,
      "learning_rate": 5e-05,
      "loss": 0.1125,
      "step": 880
    },
    {
      "epoch": 1.031615925058548,
      "grad_norm": 1.1731953620910645,
      "learning_rate": 5e-05,
      "loss": 0.1242,
      "step": 881
    },
    {
      "epoch": 1.0327868852459017,
      "grad_norm": 1.4236639738082886,
      "learning_rate": 5e-05,
      "loss": 0.0888,
      "step": 882
    },
    {
      "epoch": 1.0339578454332552,
      "grad_norm": 1.077789306640625,
      "learning_rate": 5e-05,
      "loss": 0.1248,
      "step": 883
    },
    {
      "epoch": 1.035128805620609,
      "grad_norm": 0.8675088882446289,
      "learning_rate": 5e-05,
      "loss": 0.1415,
      "step": 884
    },
    {
      "epoch": 1.0362997658079625,
      "grad_norm": 0.9369794726371765,
      "learning_rate": 5e-05,
      "loss": 0.1098,
      "step": 885
    },
    {
      "epoch": 1.0374707259953162,
      "grad_norm": 1.263148307800293,
      "learning_rate": 5e-05,
      "loss": 0.182,
      "step": 886
    },
    {
      "epoch": 1.0386416861826697,
      "grad_norm": 1.0129152536392212,
      "learning_rate": 5e-05,
      "loss": 0.1624,
      "step": 887
    },
    {
      "epoch": 1.0398126463700235,
      "grad_norm": 1.1333383321762085,
      "learning_rate": 5e-05,
      "loss": 0.2203,
      "step": 888
    },
    {
      "epoch": 1.040983606557377,
      "grad_norm": 0.8460853695869446,
      "learning_rate": 5e-05,
      "loss": 0.1396,
      "step": 889
    },
    {
      "epoch": 1.0421545667447307,
      "grad_norm": 0.6545953154563904,
      "learning_rate": 5e-05,
      "loss": 0.1954,
      "step": 890
    },
    {
      "epoch": 1.0433255269320842,
      "grad_norm": 0.9220132827758789,
      "learning_rate": 5e-05,
      "loss": 0.168,
      "step": 891
    },
    {
      "epoch": 1.044496487119438,
      "grad_norm": 0.8860206007957458,
      "learning_rate": 5e-05,
      "loss": 0.1538,
      "step": 892
    },
    {
      "epoch": 1.0456674473067915,
      "grad_norm": 0.6897925734519958,
      "learning_rate": 5e-05,
      "loss": 0.0632,
      "step": 893
    },
    {
      "epoch": 1.0468384074941453,
      "grad_norm": 0.6982322931289673,
      "learning_rate": 5e-05,
      "loss": 0.0513,
      "step": 894
    },
    {
      "epoch": 1.0480093676814988,
      "grad_norm": 1.1076570749282837,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 895
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 0.6467732191085815,
      "learning_rate": 5e-05,
      "loss": 0.1277,
      "step": 896
    },
    {
      "epoch": 1.050351288056206,
      "grad_norm": 1.1596083641052246,
      "learning_rate": 5e-05,
      "loss": 0.134,
      "step": 897
    },
    {
      "epoch": 1.0515222482435598,
      "grad_norm": 0.689193844795227,
      "learning_rate": 5e-05,
      "loss": 0.1397,
      "step": 898
    },
    {
      "epoch": 1.0526932084309133,
      "grad_norm": 0.9354106783866882,
      "learning_rate": 5e-05,
      "loss": 0.1089,
      "step": 899
    },
    {
      "epoch": 1.053864168618267,
      "grad_norm": 0.6439288854598999,
      "learning_rate": 5e-05,
      "loss": 0.0982,
      "step": 900
    },
    {
      "epoch": 1.0550351288056206,
      "grad_norm": 0.8457044363021851,
      "learning_rate": 5e-05,
      "loss": 0.0879,
      "step": 901
    },
    {
      "epoch": 1.0562060889929743,
      "grad_norm": 0.9132851362228394,
      "learning_rate": 5e-05,
      "loss": 0.108,
      "step": 902
    },
    {
      "epoch": 1.0573770491803278,
      "grad_norm": 1.17438542842865,
      "learning_rate": 5e-05,
      "loss": 0.0697,
      "step": 903
    },
    {
      "epoch": 1.0585480093676816,
      "grad_norm": 0.8471859097480774,
      "learning_rate": 5e-05,
      "loss": 0.1165,
      "step": 904
    },
    {
      "epoch": 1.059718969555035,
      "grad_norm": 1.1301120519638062,
      "learning_rate": 5e-05,
      "loss": 0.1407,
      "step": 905
    },
    {
      "epoch": 1.0608899297423888,
      "grad_norm": 0.8499418497085571,
      "learning_rate": 5e-05,
      "loss": 0.0916,
      "step": 906
    },
    {
      "epoch": 1.0620608899297423,
      "grad_norm": 1.059639811515808,
      "learning_rate": 5e-05,
      "loss": 0.1186,
      "step": 907
    },
    {
      "epoch": 1.063231850117096,
      "grad_norm": 0.5744646191596985,
      "learning_rate": 5e-05,
      "loss": 0.0652,
      "step": 908
    },
    {
      "epoch": 1.0644028103044496,
      "grad_norm": 1.26839280128479,
      "learning_rate": 5e-05,
      "loss": 0.1325,
      "step": 909
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 0.8359230756759644,
      "learning_rate": 5e-05,
      "loss": 0.0837,
      "step": 910
    },
    {
      "epoch": 1.0667447306791569,
      "grad_norm": 0.7033975720405579,
      "learning_rate": 5e-05,
      "loss": 0.1016,
      "step": 911
    },
    {
      "epoch": 1.0679156908665106,
      "grad_norm": 0.8577737212181091,
      "learning_rate": 5e-05,
      "loss": 0.1613,
      "step": 912
    },
    {
      "epoch": 1.0690866510538641,
      "grad_norm": 1.3410638570785522,
      "learning_rate": 5e-05,
      "loss": 0.1224,
      "step": 913
    },
    {
      "epoch": 1.0702576112412179,
      "grad_norm": 1.5255939960479736,
      "learning_rate": 5e-05,
      "loss": 0.197,
      "step": 914
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.6225078701972961,
      "learning_rate": 5e-05,
      "loss": 0.0451,
      "step": 915
    },
    {
      "epoch": 1.0725995316159251,
      "grad_norm": 1.1486701965332031,
      "learning_rate": 5e-05,
      "loss": 0.1749,
      "step": 916
    },
    {
      "epoch": 1.0737704918032787,
      "grad_norm": 0.7244523167610168,
      "learning_rate": 5e-05,
      "loss": 0.0479,
      "step": 917
    },
    {
      "epoch": 1.0749414519906324,
      "grad_norm": 0.8533693552017212,
      "learning_rate": 5e-05,
      "loss": 0.0806,
      "step": 918
    },
    {
      "epoch": 1.076112412177986,
      "grad_norm": 0.5709671378135681,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 919
    },
    {
      "epoch": 1.0772833723653397,
      "grad_norm": 0.691592276096344,
      "learning_rate": 5e-05,
      "loss": 0.0907,
      "step": 920
    },
    {
      "epoch": 1.0784543325526932,
      "grad_norm": 0.6557824015617371,
      "learning_rate": 5e-05,
      "loss": 0.0692,
      "step": 921
    },
    {
      "epoch": 1.079625292740047,
      "grad_norm": 0.7158998250961304,
      "learning_rate": 5e-05,
      "loss": 0.0918,
      "step": 922
    },
    {
      "epoch": 1.0807962529274004,
      "grad_norm": 0.9406479001045227,
      "learning_rate": 5e-05,
      "loss": 0.1522,
      "step": 923
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 1.0895464420318604,
      "learning_rate": 5e-05,
      "loss": 0.1452,
      "step": 924
    },
    {
      "epoch": 1.0831381733021077,
      "grad_norm": 1.16372811794281,
      "learning_rate": 5e-05,
      "loss": 0.1321,
      "step": 925
    },
    {
      "epoch": 1.0843091334894615,
      "grad_norm": 0.4725216031074524,
      "learning_rate": 5e-05,
      "loss": 0.098,
      "step": 926
    },
    {
      "epoch": 1.085480093676815,
      "grad_norm": 2.1545746326446533,
      "learning_rate": 5e-05,
      "loss": 0.2274,
      "step": 927
    },
    {
      "epoch": 1.0866510538641687,
      "grad_norm": 0.7268596887588501,
      "learning_rate": 5e-05,
      "loss": 0.1645,
      "step": 928
    },
    {
      "epoch": 1.0878220140515222,
      "grad_norm": 0.9428548216819763,
      "learning_rate": 5e-05,
      "loss": 0.2035,
      "step": 929
    },
    {
      "epoch": 1.088992974238876,
      "grad_norm": 0.7595848441123962,
      "learning_rate": 5e-05,
      "loss": 0.0963,
      "step": 930
    },
    {
      "epoch": 1.0901639344262295,
      "grad_norm": 0.906461238861084,
      "learning_rate": 5e-05,
      "loss": 0.0922,
      "step": 931
    },
    {
      "epoch": 1.0913348946135832,
      "grad_norm": 1.7766337394714355,
      "learning_rate": 5e-05,
      "loss": 0.1062,
      "step": 932
    },
    {
      "epoch": 1.0925058548009368,
      "grad_norm": 1.0605342388153076,
      "learning_rate": 5e-05,
      "loss": 0.2183,
      "step": 933
    },
    {
      "epoch": 1.0936768149882905,
      "grad_norm": 0.8596060872077942,
      "learning_rate": 5e-05,
      "loss": 0.0785,
      "step": 934
    },
    {
      "epoch": 1.094847775175644,
      "grad_norm": 0.7948146462440491,
      "learning_rate": 5e-05,
      "loss": 0.0941,
      "step": 935
    },
    {
      "epoch": 1.0960187353629975,
      "grad_norm": 0.9098392128944397,
      "learning_rate": 5e-05,
      "loss": 0.0804,
      "step": 936
    },
    {
      "epoch": 1.0971896955503513,
      "grad_norm": 1.1207096576690674,
      "learning_rate": 5e-05,
      "loss": 0.0896,
      "step": 937
    },
    {
      "epoch": 1.098360655737705,
      "grad_norm": 0.6295706033706665,
      "learning_rate": 5e-05,
      "loss": 0.0685,
      "step": 938
    },
    {
      "epoch": 1.0995316159250585,
      "grad_norm": 1.0530858039855957,
      "learning_rate": 5e-05,
      "loss": 0.1112,
      "step": 939
    },
    {
      "epoch": 1.100702576112412,
      "grad_norm": 0.7175400853157043,
      "learning_rate": 5e-05,
      "loss": 0.1143,
      "step": 940
    },
    {
      "epoch": 1.1018735362997658,
      "grad_norm": 0.4197864532470703,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 941
    },
    {
      "epoch": 1.1030444964871196,
      "grad_norm": 0.8565190434455872,
      "learning_rate": 5e-05,
      "loss": 0.0815,
      "step": 942
    },
    {
      "epoch": 1.104215456674473,
      "grad_norm": 0.5804254412651062,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 943
    },
    {
      "epoch": 1.1053864168618266,
      "grad_norm": 0.8288275003433228,
      "learning_rate": 5e-05,
      "loss": 0.1067,
      "step": 944
    },
    {
      "epoch": 1.1065573770491803,
      "grad_norm": 1.2965153455734253,
      "learning_rate": 5e-05,
      "loss": 0.189,
      "step": 945
    },
    {
      "epoch": 1.1077283372365339,
      "grad_norm": 0.768805742263794,
      "learning_rate": 5e-05,
      "loss": 0.0832,
      "step": 946
    },
    {
      "epoch": 1.1088992974238876,
      "grad_norm": 0.9674071073532104,
      "learning_rate": 5e-05,
      "loss": 0.191,
      "step": 947
    },
    {
      "epoch": 1.1100702576112411,
      "grad_norm": 0.6313222050666809,
      "learning_rate": 5e-05,
      "loss": 0.0404,
      "step": 948
    },
    {
      "epoch": 1.1112412177985949,
      "grad_norm": 0.9697228074073792,
      "learning_rate": 5e-05,
      "loss": 0.0825,
      "step": 949
    },
    {
      "epoch": 1.1124121779859484,
      "grad_norm": 1.1345882415771484,
      "learning_rate": 5e-05,
      "loss": 0.2417,
      "step": 950
    },
    {
      "epoch": 1.1135831381733021,
      "grad_norm": 0.8437686562538147,
      "learning_rate": 5e-05,
      "loss": 0.1141,
      "step": 951
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 1.2498785257339478,
      "learning_rate": 5e-05,
      "loss": 0.1533,
      "step": 952
    },
    {
      "epoch": 1.1159250585480094,
      "grad_norm": 0.7974075078964233,
      "learning_rate": 5e-05,
      "loss": 0.1385,
      "step": 953
    },
    {
      "epoch": 1.117096018735363,
      "grad_norm": 0.8889461755752563,
      "learning_rate": 5e-05,
      "loss": 0.1033,
      "step": 954
    },
    {
      "epoch": 1.1182669789227166,
      "grad_norm": 0.9303468465805054,
      "learning_rate": 5e-05,
      "loss": 0.2449,
      "step": 955
    },
    {
      "epoch": 1.1194379391100702,
      "grad_norm": 1.0233296155929565,
      "learning_rate": 5e-05,
      "loss": 0.0856,
      "step": 956
    },
    {
      "epoch": 1.120608899297424,
      "grad_norm": 0.8936985731124878,
      "learning_rate": 5e-05,
      "loss": 0.1539,
      "step": 957
    },
    {
      "epoch": 1.1217798594847774,
      "grad_norm": 1.2723499536514282,
      "learning_rate": 5e-05,
      "loss": 0.225,
      "step": 958
    },
    {
      "epoch": 1.1229508196721312,
      "grad_norm": 0.9391960501670837,
      "learning_rate": 5e-05,
      "loss": 0.0854,
      "step": 959
    },
    {
      "epoch": 1.1241217798594847,
      "grad_norm": 1.317021369934082,
      "learning_rate": 5e-05,
      "loss": 0.1695,
      "step": 960
    },
    {
      "epoch": 1.1252927400468384,
      "grad_norm": 0.5735152959823608,
      "learning_rate": 5e-05,
      "loss": 0.2005,
      "step": 961
    },
    {
      "epoch": 1.126463700234192,
      "grad_norm": 0.6742730736732483,
      "learning_rate": 5e-05,
      "loss": 0.0976,
      "step": 962
    },
    {
      "epoch": 1.1276346604215457,
      "grad_norm": 1.6449161767959595,
      "learning_rate": 5e-05,
      "loss": 0.2253,
      "step": 963
    },
    {
      "epoch": 1.1288056206088992,
      "grad_norm": 1.2470335960388184,
      "learning_rate": 5e-05,
      "loss": 0.1323,
      "step": 964
    },
    {
      "epoch": 1.129976580796253,
      "grad_norm": 0.7975921630859375,
      "learning_rate": 5e-05,
      "loss": 0.0997,
      "step": 965
    },
    {
      "epoch": 1.1311475409836065,
      "grad_norm": 1.1137052774429321,
      "learning_rate": 5e-05,
      "loss": 0.1431,
      "step": 966
    },
    {
      "epoch": 1.1323185011709602,
      "grad_norm": 0.5526547431945801,
      "learning_rate": 5e-05,
      "loss": 0.0745,
      "step": 967
    },
    {
      "epoch": 1.1334894613583137,
      "grad_norm": 0.566832423210144,
      "learning_rate": 5e-05,
      "loss": 0.0629,
      "step": 968
    },
    {
      "epoch": 1.1346604215456675,
      "grad_norm": 0.7793398499488831,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 969
    },
    {
      "epoch": 1.135831381733021,
      "grad_norm": 0.7256536483764648,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 970
    },
    {
      "epoch": 1.1370023419203747,
      "grad_norm": 0.748052716255188,
      "learning_rate": 5e-05,
      "loss": 0.1011,
      "step": 971
    },
    {
      "epoch": 1.1381733021077283,
      "grad_norm": 1.0504944324493408,
      "learning_rate": 5e-05,
      "loss": 0.2569,
      "step": 972
    },
    {
      "epoch": 1.139344262295082,
      "grad_norm": 0.8730632066726685,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 973
    },
    {
      "epoch": 1.1405152224824355,
      "grad_norm": 0.7691459059715271,
      "learning_rate": 5e-05,
      "loss": 0.0863,
      "step": 974
    },
    {
      "epoch": 1.1416861826697893,
      "grad_norm": 0.9391533136367798,
      "learning_rate": 5e-05,
      "loss": 0.1244,
      "step": 975
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.9951766729354858,
      "learning_rate": 5e-05,
      "loss": 0.1161,
      "step": 976
    },
    {
      "epoch": 1.1440281030444965,
      "grad_norm": 0.8243236541748047,
      "learning_rate": 5e-05,
      "loss": 0.1122,
      "step": 977
    },
    {
      "epoch": 1.14519906323185,
      "grad_norm": 1.0178241729736328,
      "learning_rate": 5e-05,
      "loss": 0.2058,
      "step": 978
    },
    {
      "epoch": 1.1463700234192038,
      "grad_norm": 0.8767431378364563,
      "learning_rate": 5e-05,
      "loss": 0.1091,
      "step": 979
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 0.8825131058692932,
      "learning_rate": 5e-05,
      "loss": 0.0798,
      "step": 980
    },
    {
      "epoch": 1.148711943793911,
      "grad_norm": 1.2981586456298828,
      "learning_rate": 5e-05,
      "loss": 0.1675,
      "step": 981
    },
    {
      "epoch": 1.1498829039812646,
      "grad_norm": 0.7539429068565369,
      "learning_rate": 5e-05,
      "loss": 0.057,
      "step": 982
    },
    {
      "epoch": 1.1510538641686183,
      "grad_norm": 0.9947644472122192,
      "learning_rate": 5e-05,
      "loss": 0.0819,
      "step": 983
    },
    {
      "epoch": 1.1522248243559718,
      "grad_norm": 1.4149476289749146,
      "learning_rate": 5e-05,
      "loss": 0.1365,
      "step": 984
    },
    {
      "epoch": 1.1533957845433256,
      "grad_norm": 1.1793148517608643,
      "learning_rate": 5e-05,
      "loss": 0.1473,
      "step": 985
    },
    {
      "epoch": 1.154566744730679,
      "grad_norm": 1.1361634731292725,
      "learning_rate": 5e-05,
      "loss": 0.174,
      "step": 986
    },
    {
      "epoch": 1.1557377049180328,
      "grad_norm": 1.1108289957046509,
      "learning_rate": 5e-05,
      "loss": 0.0925,
      "step": 987
    },
    {
      "epoch": 1.1569086651053864,
      "grad_norm": 1.1388323307037354,
      "learning_rate": 5e-05,
      "loss": 0.174,
      "step": 988
    },
    {
      "epoch": 1.1580796252927401,
      "grad_norm": 1.1667089462280273,
      "learning_rate": 5e-05,
      "loss": 0.2042,
      "step": 989
    },
    {
      "epoch": 1.1592505854800936,
      "grad_norm": 0.7564126253128052,
      "learning_rate": 5e-05,
      "loss": 0.1101,
      "step": 990
    },
    {
      "epoch": 1.1604215456674474,
      "grad_norm": 1.165325403213501,
      "learning_rate": 5e-05,
      "loss": 0.1086,
      "step": 991
    },
    {
      "epoch": 1.161592505854801,
      "grad_norm": 0.8289740085601807,
      "learning_rate": 5e-05,
      "loss": 0.1382,
      "step": 992
    },
    {
      "epoch": 1.1627634660421546,
      "grad_norm": 0.6996687054634094,
      "learning_rate": 5e-05,
      "loss": 0.0538,
      "step": 993
    },
    {
      "epoch": 1.1639344262295082,
      "grad_norm": 1.1166328191757202,
      "learning_rate": 5e-05,
      "loss": 0.1541,
      "step": 994
    },
    {
      "epoch": 1.165105386416862,
      "grad_norm": 1.0542356967926025,
      "learning_rate": 5e-05,
      "loss": 0.199,
      "step": 995
    },
    {
      "epoch": 1.1662763466042154,
      "grad_norm": 1.6774532794952393,
      "learning_rate": 5e-05,
      "loss": 0.1881,
      "step": 996
    },
    {
      "epoch": 1.1674473067915692,
      "grad_norm": 0.9065428376197815,
      "learning_rate": 5e-05,
      "loss": 0.1773,
      "step": 997
    },
    {
      "epoch": 1.1686182669789227,
      "grad_norm": 1.0015218257904053,
      "learning_rate": 5e-05,
      "loss": 0.1766,
      "step": 998
    },
    {
      "epoch": 1.1697892271662764,
      "grad_norm": 0.8065192699432373,
      "learning_rate": 5e-05,
      "loss": 0.1232,
      "step": 999
    },
    {
      "epoch": 1.17096018735363,
      "grad_norm": 1.0744842290878296,
      "learning_rate": 5e-05,
      "loss": 0.0852,
      "step": 1000
    },
    {
      "epoch": 1.1721311475409837,
      "grad_norm": 0.6935502886772156,
      "learning_rate": 5e-05,
      "loss": 0.1094,
      "step": 1001
    },
    {
      "epoch": 1.1733021077283372,
      "grad_norm": 0.7801164984703064,
      "learning_rate": 5e-05,
      "loss": 0.053,
      "step": 1002
    },
    {
      "epoch": 1.174473067915691,
      "grad_norm": 0.7523021101951599,
      "learning_rate": 5e-05,
      "loss": 0.0938,
      "step": 1003
    },
    {
      "epoch": 1.1756440281030445,
      "grad_norm": 2.542055606842041,
      "learning_rate": 5e-05,
      "loss": 0.1503,
      "step": 1004
    },
    {
      "epoch": 1.1768149882903982,
      "grad_norm": 0.9689861536026001,
      "learning_rate": 5e-05,
      "loss": 0.1416,
      "step": 1005
    },
    {
      "epoch": 1.1779859484777517,
      "grad_norm": 0.6258859634399414,
      "learning_rate": 5e-05,
      "loss": 0.0543,
      "step": 1006
    },
    {
      "epoch": 1.1791569086651055,
      "grad_norm": 1.2572075128555298,
      "learning_rate": 5e-05,
      "loss": 0.1311,
      "step": 1007
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 0.8734439611434937,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 1008
    },
    {
      "epoch": 1.1814988290398127,
      "grad_norm": 0.6221426725387573,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 1009
    },
    {
      "epoch": 1.1826697892271663,
      "grad_norm": 1.6213934421539307,
      "learning_rate": 5e-05,
      "loss": 0.1204,
      "step": 1010
    },
    {
      "epoch": 1.18384074941452,
      "grad_norm": 1.0224539041519165,
      "learning_rate": 5e-05,
      "loss": 0.12,
      "step": 1011
    },
    {
      "epoch": 1.1850117096018735,
      "grad_norm": 1.1200324296951294,
      "learning_rate": 5e-05,
      "loss": 0.1437,
      "step": 1012
    },
    {
      "epoch": 1.1861826697892273,
      "grad_norm": 1.28755784034729,
      "learning_rate": 5e-05,
      "loss": 0.1224,
      "step": 1013
    },
    {
      "epoch": 1.1873536299765808,
      "grad_norm": 1.2541859149932861,
      "learning_rate": 5e-05,
      "loss": 0.1438,
      "step": 1014
    },
    {
      "epoch": 1.1885245901639343,
      "grad_norm": 0.9591256976127625,
      "learning_rate": 5e-05,
      "loss": 0.177,
      "step": 1015
    },
    {
      "epoch": 1.189695550351288,
      "grad_norm": 1.2152706384658813,
      "learning_rate": 5e-05,
      "loss": 0.1755,
      "step": 1016
    },
    {
      "epoch": 1.1908665105386418,
      "grad_norm": 0.7630430459976196,
      "learning_rate": 5e-05,
      "loss": 0.0823,
      "step": 1017
    },
    {
      "epoch": 1.1920374707259953,
      "grad_norm": 0.6316036581993103,
      "learning_rate": 5e-05,
      "loss": 0.0517,
      "step": 1018
    },
    {
      "epoch": 1.1932084309133488,
      "grad_norm": 0.7799339890480042,
      "learning_rate": 5e-05,
      "loss": 0.1278,
      "step": 1019
    },
    {
      "epoch": 1.1943793911007026,
      "grad_norm": 1.4047579765319824,
      "learning_rate": 5e-05,
      "loss": 0.1797,
      "step": 1020
    },
    {
      "epoch": 1.1955503512880563,
      "grad_norm": 1.4684966802597046,
      "learning_rate": 5e-05,
      "loss": 0.185,
      "step": 1021
    },
    {
      "epoch": 1.1967213114754098,
      "grad_norm": 1.22498619556427,
      "learning_rate": 5e-05,
      "loss": 0.1858,
      "step": 1022
    },
    {
      "epoch": 1.1978922716627634,
      "grad_norm": 2.576467752456665,
      "learning_rate": 5e-05,
      "loss": 0.1172,
      "step": 1023
    },
    {
      "epoch": 1.199063231850117,
      "grad_norm": 0.9141151905059814,
      "learning_rate": 5e-05,
      "loss": 0.1106,
      "step": 1024
    },
    {
      "epoch": 1.2002341920374708,
      "grad_norm": 1.2211873531341553,
      "learning_rate": 5e-05,
      "loss": 0.1558,
      "step": 1025
    },
    {
      "epoch": 1.2014051522248244,
      "grad_norm": 1.013333797454834,
      "learning_rate": 5e-05,
      "loss": 0.1647,
      "step": 1026
    },
    {
      "epoch": 1.2025761124121779,
      "grad_norm": 2.0338706970214844,
      "learning_rate": 5e-05,
      "loss": 0.2228,
      "step": 1027
    },
    {
      "epoch": 1.2037470725995316,
      "grad_norm": 0.7298120856285095,
      "learning_rate": 5e-05,
      "loss": 0.0756,
      "step": 1028
    },
    {
      "epoch": 1.2049180327868854,
      "grad_norm": 0.7580761909484863,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 1029
    },
    {
      "epoch": 1.2060889929742389,
      "grad_norm": 0.6897748708724976,
      "learning_rate": 5e-05,
      "loss": 0.11,
      "step": 1030
    },
    {
      "epoch": 1.2072599531615924,
      "grad_norm": 0.7214400172233582,
      "learning_rate": 5e-05,
      "loss": 0.0667,
      "step": 1031
    },
    {
      "epoch": 1.2084309133489461,
      "grad_norm": 0.8028423190116882,
      "learning_rate": 5e-05,
      "loss": 0.0761,
      "step": 1032
    },
    {
      "epoch": 1.2096018735362999,
      "grad_norm": 0.6065487265586853,
      "learning_rate": 5e-05,
      "loss": 0.0754,
      "step": 1033
    },
    {
      "epoch": 1.2107728337236534,
      "grad_norm": 1.040053367614746,
      "learning_rate": 5e-05,
      "loss": 0.1444,
      "step": 1034
    },
    {
      "epoch": 1.211943793911007,
      "grad_norm": 0.9055126309394836,
      "learning_rate": 5e-05,
      "loss": 0.2015,
      "step": 1035
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 0.7368438243865967,
      "learning_rate": 5e-05,
      "loss": 0.1133,
      "step": 1036
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 0.9093409180641174,
      "learning_rate": 5e-05,
      "loss": 0.1437,
      "step": 1037
    },
    {
      "epoch": 1.215456674473068,
      "grad_norm": 0.7840180397033691,
      "learning_rate": 5e-05,
      "loss": 0.1445,
      "step": 1038
    },
    {
      "epoch": 1.2166276346604215,
      "grad_norm": 1.2211271524429321,
      "learning_rate": 5e-05,
      "loss": 0.2724,
      "step": 1039
    },
    {
      "epoch": 1.2177985948477752,
      "grad_norm": 0.7574119567871094,
      "learning_rate": 5e-05,
      "loss": 0.0927,
      "step": 1040
    },
    {
      "epoch": 1.2189695550351287,
      "grad_norm": 1.1718709468841553,
      "learning_rate": 5e-05,
      "loss": 0.1532,
      "step": 1041
    },
    {
      "epoch": 1.2201405152224825,
      "grad_norm": 0.8438392877578735,
      "learning_rate": 5e-05,
      "loss": 0.1037,
      "step": 1042
    },
    {
      "epoch": 1.221311475409836,
      "grad_norm": 0.8547176122665405,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 1043
    },
    {
      "epoch": 1.2224824355971897,
      "grad_norm": 1.447428584098816,
      "learning_rate": 5e-05,
      "loss": 0.1778,
      "step": 1044
    },
    {
      "epoch": 1.2236533957845432,
      "grad_norm": 0.6326680183410645,
      "learning_rate": 5e-05,
      "loss": 0.1328,
      "step": 1045
    },
    {
      "epoch": 1.224824355971897,
      "grad_norm": 1.1070683002471924,
      "learning_rate": 5e-05,
      "loss": 0.1202,
      "step": 1046
    },
    {
      "epoch": 1.2259953161592505,
      "grad_norm": 0.5666257739067078,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 1047
    },
    {
      "epoch": 1.2271662763466042,
      "grad_norm": 1.0235779285430908,
      "learning_rate": 5e-05,
      "loss": 0.1069,
      "step": 1048
    },
    {
      "epoch": 1.2283372365339578,
      "grad_norm": 1.0679868459701538,
      "learning_rate": 5e-05,
      "loss": 0.097,
      "step": 1049
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 0.7068184018135071,
      "learning_rate": 5e-05,
      "loss": 0.0587,
      "step": 1050
    },
    {
      "epoch": 1.230679156908665,
      "grad_norm": 1.243909239768982,
      "learning_rate": 5e-05,
      "loss": 0.1895,
      "step": 1051
    },
    {
      "epoch": 1.2318501170960188,
      "grad_norm": 0.5788844227790833,
      "learning_rate": 5e-05,
      "loss": 0.0546,
      "step": 1052
    },
    {
      "epoch": 1.2330210772833723,
      "grad_norm": 0.7607214450836182,
      "learning_rate": 5e-05,
      "loss": 0.1048,
      "step": 1053
    },
    {
      "epoch": 1.234192037470726,
      "grad_norm": 0.5355818271636963,
      "learning_rate": 5e-05,
      "loss": 0.0415,
      "step": 1054
    },
    {
      "epoch": 1.2353629976580796,
      "grad_norm": 1.7152183055877686,
      "learning_rate": 5e-05,
      "loss": 0.1127,
      "step": 1055
    },
    {
      "epoch": 1.2365339578454333,
      "grad_norm": 0.9584295749664307,
      "learning_rate": 5e-05,
      "loss": 0.1698,
      "step": 1056
    },
    {
      "epoch": 1.2377049180327868,
      "grad_norm": 0.9432288408279419,
      "learning_rate": 5e-05,
      "loss": 0.0517,
      "step": 1057
    },
    {
      "epoch": 1.2388758782201406,
      "grad_norm": 0.8368309140205383,
      "learning_rate": 5e-05,
      "loss": 0.0616,
      "step": 1058
    },
    {
      "epoch": 1.240046838407494,
      "grad_norm": 1.4332034587860107,
      "learning_rate": 5e-05,
      "loss": 0.1333,
      "step": 1059
    },
    {
      "epoch": 1.2412177985948478,
      "grad_norm": 0.5213127136230469,
      "learning_rate": 5e-05,
      "loss": 0.0939,
      "step": 1060
    },
    {
      "epoch": 1.2423887587822013,
      "grad_norm": 0.7869560122489929,
      "learning_rate": 5e-05,
      "loss": 0.0643,
      "step": 1061
    },
    {
      "epoch": 1.243559718969555,
      "grad_norm": 1.1184712648391724,
      "learning_rate": 5e-05,
      "loss": 0.1263,
      "step": 1062
    },
    {
      "epoch": 1.2447306791569086,
      "grad_norm": 0.8794554471969604,
      "learning_rate": 5e-05,
      "loss": 0.1058,
      "step": 1063
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 1.7569608688354492,
      "learning_rate": 5e-05,
      "loss": 0.2536,
      "step": 1064
    },
    {
      "epoch": 1.2470725995316159,
      "grad_norm": 0.8907802104949951,
      "learning_rate": 5e-05,
      "loss": 0.0723,
      "step": 1065
    },
    {
      "epoch": 1.2482435597189696,
      "grad_norm": 1.1756141185760498,
      "learning_rate": 5e-05,
      "loss": 0.1851,
      "step": 1066
    },
    {
      "epoch": 1.2494145199063231,
      "grad_norm": 0.8169171214103699,
      "learning_rate": 5e-05,
      "loss": 0.0871,
      "step": 1067
    },
    {
      "epoch": 1.2505854800936769,
      "grad_norm": 0.9032279253005981,
      "learning_rate": 5e-05,
      "loss": 0.0984,
      "step": 1068
    },
    {
      "epoch": 1.2517564402810304,
      "grad_norm": 0.5822732448577881,
      "learning_rate": 5e-05,
      "loss": 0.0637,
      "step": 1069
    },
    {
      "epoch": 1.2529274004683841,
      "grad_norm": 0.7333047389984131,
      "learning_rate": 5e-05,
      "loss": 0.0688,
      "step": 1070
    },
    {
      "epoch": 1.2540983606557377,
      "grad_norm": 0.5367558598518372,
      "learning_rate": 5e-05,
      "loss": 0.0434,
      "step": 1071
    },
    {
      "epoch": 1.2552693208430914,
      "grad_norm": 0.7898735404014587,
      "learning_rate": 5e-05,
      "loss": 0.0574,
      "step": 1072
    },
    {
      "epoch": 1.256440281030445,
      "grad_norm": 0.702244222164154,
      "learning_rate": 5e-05,
      "loss": 0.0585,
      "step": 1073
    },
    {
      "epoch": 1.2576112412177987,
      "grad_norm": 1.0130928754806519,
      "learning_rate": 5e-05,
      "loss": 0.0903,
      "step": 1074
    },
    {
      "epoch": 1.2587822014051522,
      "grad_norm": 1.0545507669448853,
      "learning_rate": 5e-05,
      "loss": 0.1339,
      "step": 1075
    },
    {
      "epoch": 1.259953161592506,
      "grad_norm": 0.7806156873703003,
      "learning_rate": 5e-05,
      "loss": 0.0616,
      "step": 1076
    },
    {
      "epoch": 1.2611241217798594,
      "grad_norm": 0.8551329374313354,
      "learning_rate": 5e-05,
      "loss": 0.1473,
      "step": 1077
    },
    {
      "epoch": 1.2622950819672132,
      "grad_norm": 1.05069899559021,
      "learning_rate": 5e-05,
      "loss": 0.1418,
      "step": 1078
    },
    {
      "epoch": 1.2634660421545667,
      "grad_norm": 0.6273868680000305,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 1079
    },
    {
      "epoch": 1.2646370023419204,
      "grad_norm": 0.8405355215072632,
      "learning_rate": 5e-05,
      "loss": 0.0906,
      "step": 1080
    },
    {
      "epoch": 1.265807962529274,
      "grad_norm": 1.0486609935760498,
      "learning_rate": 5e-05,
      "loss": 0.0668,
      "step": 1081
    },
    {
      "epoch": 1.2669789227166277,
      "grad_norm": 1.604906678199768,
      "learning_rate": 5e-05,
      "loss": 0.1996,
      "step": 1082
    },
    {
      "epoch": 1.2681498829039812,
      "grad_norm": 1.3731026649475098,
      "learning_rate": 5e-05,
      "loss": 0.151,
      "step": 1083
    },
    {
      "epoch": 1.269320843091335,
      "grad_norm": 1.2114790678024292,
      "learning_rate": 5e-05,
      "loss": 0.2861,
      "step": 1084
    },
    {
      "epoch": 1.2704918032786885,
      "grad_norm": 0.7515633702278137,
      "learning_rate": 5e-05,
      "loss": 0.0865,
      "step": 1085
    },
    {
      "epoch": 1.2716627634660422,
      "grad_norm": 1.0384855270385742,
      "learning_rate": 5e-05,
      "loss": 0.1319,
      "step": 1086
    },
    {
      "epoch": 1.2728337236533958,
      "grad_norm": 0.6887341737747192,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 1087
    },
    {
      "epoch": 1.2740046838407495,
      "grad_norm": 0.8160718679428101,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 1088
    },
    {
      "epoch": 1.275175644028103,
      "grad_norm": 0.7525578141212463,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 1089
    },
    {
      "epoch": 1.2763466042154565,
      "grad_norm": 0.5467427372932434,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1090
    },
    {
      "epoch": 1.2775175644028103,
      "grad_norm": 1.0769561529159546,
      "learning_rate": 5e-05,
      "loss": 0.0793,
      "step": 1091
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 1.2625232934951782,
      "learning_rate": 5e-05,
      "loss": 0.1255,
      "step": 1092
    },
    {
      "epoch": 1.2798594847775175,
      "grad_norm": 1.005048155784607,
      "learning_rate": 5e-05,
      "loss": 0.1466,
      "step": 1093
    },
    {
      "epoch": 1.281030444964871,
      "grad_norm": 1.4379595518112183,
      "learning_rate": 5e-05,
      "loss": 0.2135,
      "step": 1094
    },
    {
      "epoch": 1.2822014051522248,
      "grad_norm": 1.0736223459243774,
      "learning_rate": 5e-05,
      "loss": 0.0941,
      "step": 1095
    },
    {
      "epoch": 1.2833723653395785,
      "grad_norm": 0.8282961249351501,
      "learning_rate": 5e-05,
      "loss": 0.0532,
      "step": 1096
    },
    {
      "epoch": 1.284543325526932,
      "grad_norm": 1.369010090827942,
      "learning_rate": 5e-05,
      "loss": 0.0971,
      "step": 1097
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.1204290390014648,
      "learning_rate": 5e-05,
      "loss": 0.1737,
      "step": 1098
    },
    {
      "epoch": 1.2868852459016393,
      "grad_norm": 0.6855546236038208,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 1099
    },
    {
      "epoch": 1.288056206088993,
      "grad_norm": 1.113790512084961,
      "learning_rate": 5e-05,
      "loss": 0.1016,
      "step": 1100
    },
    {
      "epoch": 1.2892271662763466,
      "grad_norm": 1.1619110107421875,
      "learning_rate": 5e-05,
      "loss": 0.0719,
      "step": 1101
    },
    {
      "epoch": 1.2903981264637001,
      "grad_norm": 0.8555033802986145,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 1102
    },
    {
      "epoch": 1.2915690866510539,
      "grad_norm": 0.7338226437568665,
      "learning_rate": 5e-05,
      "loss": 0.0658,
      "step": 1103
    },
    {
      "epoch": 1.2927400468384076,
      "grad_norm": 0.6445235013961792,
      "learning_rate": 5e-05,
      "loss": 0.0997,
      "step": 1104
    },
    {
      "epoch": 1.2939110070257611,
      "grad_norm": 1.132848858833313,
      "learning_rate": 5e-05,
      "loss": 0.1534,
      "step": 1105
    },
    {
      "epoch": 1.2950819672131146,
      "grad_norm": 0.9974496364593506,
      "learning_rate": 5e-05,
      "loss": 0.0563,
      "step": 1106
    },
    {
      "epoch": 1.2962529274004684,
      "grad_norm": 1.0428478717803955,
      "learning_rate": 5e-05,
      "loss": 0.1464,
      "step": 1107
    },
    {
      "epoch": 1.2974238875878221,
      "grad_norm": 1.5199350118637085,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1108
    },
    {
      "epoch": 1.2985948477751756,
      "grad_norm": 0.9734259247779846,
      "learning_rate": 5e-05,
      "loss": 0.1037,
      "step": 1109
    },
    {
      "epoch": 1.2997658079625292,
      "grad_norm": 1.4174582958221436,
      "learning_rate": 5e-05,
      "loss": 0.1657,
      "step": 1110
    },
    {
      "epoch": 1.300936768149883,
      "grad_norm": 0.6211554408073425,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 1111
    },
    {
      "epoch": 1.3021077283372366,
      "grad_norm": 1.4716646671295166,
      "learning_rate": 5e-05,
      "loss": 0.1466,
      "step": 1112
    },
    {
      "epoch": 1.3032786885245902,
      "grad_norm": 0.9836792945861816,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 1113
    },
    {
      "epoch": 1.3044496487119437,
      "grad_norm": 0.9576498866081238,
      "learning_rate": 5e-05,
      "loss": 0.0911,
      "step": 1114
    },
    {
      "epoch": 1.3056206088992974,
      "grad_norm": 1.332244634628296,
      "learning_rate": 5e-05,
      "loss": 0.1083,
      "step": 1115
    },
    {
      "epoch": 1.3067915690866512,
      "grad_norm": 0.8739365339279175,
      "learning_rate": 5e-05,
      "loss": 0.0954,
      "step": 1116
    },
    {
      "epoch": 1.3079625292740047,
      "grad_norm": 1.2825185060501099,
      "learning_rate": 5e-05,
      "loss": 0.2561,
      "step": 1117
    },
    {
      "epoch": 1.3091334894613582,
      "grad_norm": 0.9219892024993896,
      "learning_rate": 5e-05,
      "loss": 0.077,
      "step": 1118
    },
    {
      "epoch": 1.310304449648712,
      "grad_norm": 2.5709125995635986,
      "learning_rate": 5e-05,
      "loss": 0.1712,
      "step": 1119
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 1.0840524435043335,
      "learning_rate": 5e-05,
      "loss": 0.1059,
      "step": 1120
    },
    {
      "epoch": 1.3126463700234192,
      "grad_norm": 1.0770336389541626,
      "learning_rate": 5e-05,
      "loss": 0.0982,
      "step": 1121
    },
    {
      "epoch": 1.3138173302107727,
      "grad_norm": 1.4241164922714233,
      "learning_rate": 5e-05,
      "loss": 0.0748,
      "step": 1122
    },
    {
      "epoch": 1.3149882903981265,
      "grad_norm": 1.3890516757965088,
      "learning_rate": 5e-05,
      "loss": 0.1322,
      "step": 1123
    },
    {
      "epoch": 1.3161592505854802,
      "grad_norm": 1.7957403659820557,
      "learning_rate": 5e-05,
      "loss": 0.1984,
      "step": 1124
    },
    {
      "epoch": 1.3173302107728337,
      "grad_norm": 0.5850697755813599,
      "learning_rate": 5e-05,
      "loss": 0.0482,
      "step": 1125
    },
    {
      "epoch": 1.3185011709601873,
      "grad_norm": 1.1675564050674438,
      "learning_rate": 5e-05,
      "loss": 0.089,
      "step": 1126
    },
    {
      "epoch": 1.319672131147541,
      "grad_norm": 1.1394587755203247,
      "learning_rate": 5e-05,
      "loss": 0.1844,
      "step": 1127
    },
    {
      "epoch": 1.3208430913348947,
      "grad_norm": 1.0895029306411743,
      "learning_rate": 5e-05,
      "loss": 0.1857,
      "step": 1128
    },
    {
      "epoch": 1.3220140515222483,
      "grad_norm": 0.574492335319519,
      "learning_rate": 5e-05,
      "loss": 0.0525,
      "step": 1129
    },
    {
      "epoch": 1.3231850117096018,
      "grad_norm": 1.0545525550842285,
      "learning_rate": 5e-05,
      "loss": 0.0723,
      "step": 1130
    },
    {
      "epoch": 1.3243559718969555,
      "grad_norm": 0.8125278949737549,
      "learning_rate": 5e-05,
      "loss": 0.0997,
      "step": 1131
    },
    {
      "epoch": 1.325526932084309,
      "grad_norm": 0.6220159530639648,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 1132
    },
    {
      "epoch": 1.3266978922716628,
      "grad_norm": 0.7981327176094055,
      "learning_rate": 5e-05,
      "loss": 0.0785,
      "step": 1133
    },
    {
      "epoch": 1.3278688524590163,
      "grad_norm": 0.48122164607048035,
      "learning_rate": 5e-05,
      "loss": 0.0575,
      "step": 1134
    },
    {
      "epoch": 1.32903981264637,
      "grad_norm": 0.7149311900138855,
      "learning_rate": 5e-05,
      "loss": 0.1006,
      "step": 1135
    },
    {
      "epoch": 1.3302107728337236,
      "grad_norm": 0.4751627445220947,
      "learning_rate": 5e-05,
      "loss": 0.0744,
      "step": 1136
    },
    {
      "epoch": 1.3313817330210773,
      "grad_norm": 0.7336873412132263,
      "learning_rate": 5e-05,
      "loss": 0.088,
      "step": 1137
    },
    {
      "epoch": 1.3325526932084308,
      "grad_norm": 0.8469857573509216,
      "learning_rate": 5e-05,
      "loss": 0.0678,
      "step": 1138
    },
    {
      "epoch": 1.3337236533957846,
      "grad_norm": 0.8052735328674316,
      "learning_rate": 5e-05,
      "loss": 0.0728,
      "step": 1139
    },
    {
      "epoch": 1.334894613583138,
      "grad_norm": 0.9085538387298584,
      "learning_rate": 5e-05,
      "loss": 0.1431,
      "step": 1140
    },
    {
      "epoch": 1.3360655737704918,
      "grad_norm": 1.1382510662078857,
      "learning_rate": 5e-05,
      "loss": 0.1201,
      "step": 1141
    },
    {
      "epoch": 1.3372365339578454,
      "grad_norm": 0.8423131704330444,
      "learning_rate": 5e-05,
      "loss": 0.126,
      "step": 1142
    },
    {
      "epoch": 1.338407494145199,
      "grad_norm": 0.6498139500617981,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 1143
    },
    {
      "epoch": 1.3395784543325526,
      "grad_norm": 1.1830003261566162,
      "learning_rate": 5e-05,
      "loss": 0.1209,
      "step": 1144
    },
    {
      "epoch": 1.3407494145199064,
      "grad_norm": 0.5848456621170044,
      "learning_rate": 5e-05,
      "loss": 0.0873,
      "step": 1145
    },
    {
      "epoch": 1.3419203747072599,
      "grad_norm": 0.5439435243606567,
      "learning_rate": 5e-05,
      "loss": 0.0611,
      "step": 1146
    },
    {
      "epoch": 1.3430913348946136,
      "grad_norm": 0.6673890948295593,
      "learning_rate": 5e-05,
      "loss": 0.082,
      "step": 1147
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 1.0082851648330688,
      "learning_rate": 5e-05,
      "loss": 0.1423,
      "step": 1148
    },
    {
      "epoch": 1.345433255269321,
      "grad_norm": 1.1775344610214233,
      "learning_rate": 5e-05,
      "loss": 0.153,
      "step": 1149
    },
    {
      "epoch": 1.3466042154566744,
      "grad_norm": 0.6261373162269592,
      "learning_rate": 5e-05,
      "loss": 0.064,
      "step": 1150
    },
    {
      "epoch": 1.3477751756440282,
      "grad_norm": 0.9214502573013306,
      "learning_rate": 5e-05,
      "loss": 0.1169,
      "step": 1151
    },
    {
      "epoch": 1.3489461358313817,
      "grad_norm": 1.2801716327667236,
      "learning_rate": 5e-05,
      "loss": 0.1101,
      "step": 1152
    },
    {
      "epoch": 1.3501170960187354,
      "grad_norm": 1.0695992708206177,
      "learning_rate": 5e-05,
      "loss": 0.1438,
      "step": 1153
    },
    {
      "epoch": 1.351288056206089,
      "grad_norm": 0.8607674241065979,
      "learning_rate": 5e-05,
      "loss": 0.1162,
      "step": 1154
    },
    {
      "epoch": 1.3524590163934427,
      "grad_norm": 0.7719035148620605,
      "learning_rate": 5e-05,
      "loss": 0.1528,
      "step": 1155
    },
    {
      "epoch": 1.3536299765807962,
      "grad_norm": 0.8602564930915833,
      "learning_rate": 5e-05,
      "loss": 0.12,
      "step": 1156
    },
    {
      "epoch": 1.35480093676815,
      "grad_norm": 1.0224025249481201,
      "learning_rate": 5e-05,
      "loss": 0.1342,
      "step": 1157
    },
    {
      "epoch": 1.3559718969555035,
      "grad_norm": 1.1166298389434814,
      "learning_rate": 5e-05,
      "loss": 0.0916,
      "step": 1158
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 0.6662856936454773,
      "learning_rate": 5e-05,
      "loss": 0.0995,
      "step": 1159
    },
    {
      "epoch": 1.3583138173302107,
      "grad_norm": 1.2397112846374512,
      "learning_rate": 5e-05,
      "loss": 0.2097,
      "step": 1160
    },
    {
      "epoch": 1.3594847775175645,
      "grad_norm": 0.823621928691864,
      "learning_rate": 5e-05,
      "loss": 0.0754,
      "step": 1161
    },
    {
      "epoch": 1.360655737704918,
      "grad_norm": 0.9059016704559326,
      "learning_rate": 5e-05,
      "loss": 0.1076,
      "step": 1162
    },
    {
      "epoch": 1.3618266978922717,
      "grad_norm": 0.5917642116546631,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 1163
    },
    {
      "epoch": 1.3629976580796253,
      "grad_norm": 1.4253262281417847,
      "learning_rate": 5e-05,
      "loss": 0.1176,
      "step": 1164
    },
    {
      "epoch": 1.364168618266979,
      "grad_norm": 0.7156468629837036,
      "learning_rate": 5e-05,
      "loss": 0.0576,
      "step": 1165
    },
    {
      "epoch": 1.3653395784543325,
      "grad_norm": 0.7932776808738708,
      "learning_rate": 5e-05,
      "loss": 0.0825,
      "step": 1166
    },
    {
      "epoch": 1.3665105386416863,
      "grad_norm": 0.8457105755805969,
      "learning_rate": 5e-05,
      "loss": 0.1018,
      "step": 1167
    },
    {
      "epoch": 1.3676814988290398,
      "grad_norm": 1.1361275911331177,
      "learning_rate": 5e-05,
      "loss": 0.1381,
      "step": 1168
    },
    {
      "epoch": 1.3688524590163935,
      "grad_norm": 0.7689862847328186,
      "learning_rate": 5e-05,
      "loss": 0.0858,
      "step": 1169
    },
    {
      "epoch": 1.370023419203747,
      "grad_norm": 0.7836708426475525,
      "learning_rate": 5e-05,
      "loss": 0.1179,
      "step": 1170
    },
    {
      "epoch": 1.3711943793911008,
      "grad_norm": 0.8425182700157166,
      "learning_rate": 5e-05,
      "loss": 0.1369,
      "step": 1171
    },
    {
      "epoch": 1.3723653395784543,
      "grad_norm": 1.0816494226455688,
      "learning_rate": 5e-05,
      "loss": 0.1048,
      "step": 1172
    },
    {
      "epoch": 1.373536299765808,
      "grad_norm": 0.8650386333465576,
      "learning_rate": 5e-05,
      "loss": 0.112,
      "step": 1173
    },
    {
      "epoch": 1.3747072599531616,
      "grad_norm": 0.782179594039917,
      "learning_rate": 5e-05,
      "loss": 0.1465,
      "step": 1174
    },
    {
      "epoch": 1.3758782201405153,
      "grad_norm": 0.6736322641372681,
      "learning_rate": 5e-05,
      "loss": 0.0738,
      "step": 1175
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 0.5980778336524963,
      "learning_rate": 5e-05,
      "loss": 0.1205,
      "step": 1176
    },
    {
      "epoch": 1.3782201405152223,
      "grad_norm": 1.236200213432312,
      "learning_rate": 5e-05,
      "loss": 0.1265,
      "step": 1177
    },
    {
      "epoch": 1.379391100702576,
      "grad_norm": 1.5559988021850586,
      "learning_rate": 5e-05,
      "loss": 0.1662,
      "step": 1178
    },
    {
      "epoch": 1.3805620608899298,
      "grad_norm": 0.6651754379272461,
      "learning_rate": 5e-05,
      "loss": 0.1129,
      "step": 1179
    },
    {
      "epoch": 1.3817330210772834,
      "grad_norm": 1.539833426475525,
      "learning_rate": 5e-05,
      "loss": 0.1076,
      "step": 1180
    },
    {
      "epoch": 1.3829039812646369,
      "grad_norm": 1.2508230209350586,
      "learning_rate": 5e-05,
      "loss": 0.1079,
      "step": 1181
    },
    {
      "epoch": 1.3840749414519906,
      "grad_norm": 0.7217189073562622,
      "learning_rate": 5e-05,
      "loss": 0.0663,
      "step": 1182
    },
    {
      "epoch": 1.3852459016393444,
      "grad_norm": 0.7608636021614075,
      "learning_rate": 5e-05,
      "loss": 0.0802,
      "step": 1183
    },
    {
      "epoch": 1.3864168618266979,
      "grad_norm": 1.0748525857925415,
      "learning_rate": 5e-05,
      "loss": 0.1153,
      "step": 1184
    },
    {
      "epoch": 1.3875878220140514,
      "grad_norm": 0.8564883470535278,
      "learning_rate": 5e-05,
      "loss": 0.1237,
      "step": 1185
    },
    {
      "epoch": 1.3887587822014051,
      "grad_norm": 0.7180881500244141,
      "learning_rate": 5e-05,
      "loss": 0.0568,
      "step": 1186
    },
    {
      "epoch": 1.3899297423887589,
      "grad_norm": 1.0577731132507324,
      "learning_rate": 5e-05,
      "loss": 0.122,
      "step": 1187
    },
    {
      "epoch": 1.3911007025761124,
      "grad_norm": 0.5394949913024902,
      "learning_rate": 5e-05,
      "loss": 0.0577,
      "step": 1188
    },
    {
      "epoch": 1.392271662763466,
      "grad_norm": 0.5559811592102051,
      "learning_rate": 5e-05,
      "loss": 0.068,
      "step": 1189
    },
    {
      "epoch": 1.3934426229508197,
      "grad_norm": 0.9485647678375244,
      "learning_rate": 5e-05,
      "loss": 0.1006,
      "step": 1190
    },
    {
      "epoch": 1.3946135831381734,
      "grad_norm": 1.304644227027893,
      "learning_rate": 5e-05,
      "loss": 0.1147,
      "step": 1191
    },
    {
      "epoch": 1.395784543325527,
      "grad_norm": 0.6780621409416199,
      "learning_rate": 5e-05,
      "loss": 0.0474,
      "step": 1192
    },
    {
      "epoch": 1.3969555035128804,
      "grad_norm": 1.0040864944458008,
      "learning_rate": 5e-05,
      "loss": 0.1283,
      "step": 1193
    },
    {
      "epoch": 1.3981264637002342,
      "grad_norm": 0.8424453735351562,
      "learning_rate": 5e-05,
      "loss": 0.1619,
      "step": 1194
    },
    {
      "epoch": 1.399297423887588,
      "grad_norm": 0.971524178981781,
      "learning_rate": 5e-05,
      "loss": 0.0695,
      "step": 1195
    },
    {
      "epoch": 1.4004683840749415,
      "grad_norm": 0.8300011157989502,
      "learning_rate": 5e-05,
      "loss": 0.0701,
      "step": 1196
    },
    {
      "epoch": 1.401639344262295,
      "grad_norm": 0.873506486415863,
      "learning_rate": 5e-05,
      "loss": 0.0857,
      "step": 1197
    },
    {
      "epoch": 1.4028103044496487,
      "grad_norm": 0.7414113283157349,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 1198
    },
    {
      "epoch": 1.4039812646370025,
      "grad_norm": 0.9948709011077881,
      "learning_rate": 5e-05,
      "loss": 0.1129,
      "step": 1199
    },
    {
      "epoch": 1.405152224824356,
      "grad_norm": 0.8287813067436218,
      "learning_rate": 5e-05,
      "loss": 0.1483,
      "step": 1200
    },
    {
      "epoch": 1.4063231850117095,
      "grad_norm": 0.8605429530143738,
      "learning_rate": 5e-05,
      "loss": 0.068,
      "step": 1201
    },
    {
      "epoch": 1.4074941451990632,
      "grad_norm": 0.8611166477203369,
      "learning_rate": 5e-05,
      "loss": 0.1512,
      "step": 1202
    },
    {
      "epoch": 1.408665105386417,
      "grad_norm": 1.3346267938613892,
      "learning_rate": 5e-05,
      "loss": 0.5619,
      "step": 1203
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 0.9866917133331299,
      "learning_rate": 5e-05,
      "loss": 0.103,
      "step": 1204
    },
    {
      "epoch": 1.411007025761124,
      "grad_norm": 1.2379512786865234,
      "learning_rate": 5e-05,
      "loss": 0.1685,
      "step": 1205
    },
    {
      "epoch": 1.4121779859484778,
      "grad_norm": 2.2371132373809814,
      "learning_rate": 5e-05,
      "loss": 0.181,
      "step": 1206
    },
    {
      "epoch": 1.4133489461358315,
      "grad_norm": 0.9702199697494507,
      "learning_rate": 5e-05,
      "loss": 0.066,
      "step": 1207
    },
    {
      "epoch": 1.414519906323185,
      "grad_norm": 1.3991925716400146,
      "learning_rate": 5e-05,
      "loss": 0.1795,
      "step": 1208
    },
    {
      "epoch": 1.4156908665105385,
      "grad_norm": 1.184952974319458,
      "learning_rate": 5e-05,
      "loss": 0.1981,
      "step": 1209
    },
    {
      "epoch": 1.4168618266978923,
      "grad_norm": 1.4956034421920776,
      "learning_rate": 5e-05,
      "loss": 0.1087,
      "step": 1210
    },
    {
      "epoch": 1.418032786885246,
      "grad_norm": 2.0152747631073,
      "learning_rate": 5e-05,
      "loss": 0.1984,
      "step": 1211
    },
    {
      "epoch": 1.4192037470725996,
      "grad_norm": 0.8635187149047852,
      "learning_rate": 5e-05,
      "loss": 0.0903,
      "step": 1212
    },
    {
      "epoch": 1.420374707259953,
      "grad_norm": 0.8688920736312866,
      "learning_rate": 5e-05,
      "loss": 0.0964,
      "step": 1213
    },
    {
      "epoch": 1.4215456674473068,
      "grad_norm": 1.6490625143051147,
      "learning_rate": 5e-05,
      "loss": 0.21,
      "step": 1214
    },
    {
      "epoch": 1.4227166276346606,
      "grad_norm": 0.7487737536430359,
      "learning_rate": 5e-05,
      "loss": 0.1144,
      "step": 1215
    },
    {
      "epoch": 1.423887587822014,
      "grad_norm": 0.9871419668197632,
      "learning_rate": 5e-05,
      "loss": 0.1211,
      "step": 1216
    },
    {
      "epoch": 1.4250585480093676,
      "grad_norm": 0.6584017872810364,
      "learning_rate": 5e-05,
      "loss": 0.0662,
      "step": 1217
    },
    {
      "epoch": 1.4262295081967213,
      "grad_norm": 0.9929181933403015,
      "learning_rate": 5e-05,
      "loss": 0.2354,
      "step": 1218
    },
    {
      "epoch": 1.4274004683840749,
      "grad_norm": 1.2688283920288086,
      "learning_rate": 5e-05,
      "loss": 0.1853,
      "step": 1219
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.0272759199142456,
      "learning_rate": 5e-05,
      "loss": 0.189,
      "step": 1220
    },
    {
      "epoch": 1.4297423887587821,
      "grad_norm": 1.2081741094589233,
      "learning_rate": 5e-05,
      "loss": 0.2495,
      "step": 1221
    },
    {
      "epoch": 1.4309133489461359,
      "grad_norm": 0.8838430047035217,
      "learning_rate": 5e-05,
      "loss": 0.0798,
      "step": 1222
    },
    {
      "epoch": 1.4320843091334894,
      "grad_norm": 0.7159138321876526,
      "learning_rate": 5e-05,
      "loss": 0.0732,
      "step": 1223
    },
    {
      "epoch": 1.4332552693208431,
      "grad_norm": 1.1524431705474854,
      "learning_rate": 5e-05,
      "loss": 0.1469,
      "step": 1224
    },
    {
      "epoch": 1.4344262295081966,
      "grad_norm": 1.057745099067688,
      "learning_rate": 5e-05,
      "loss": 0.0995,
      "step": 1225
    },
    {
      "epoch": 1.4355971896955504,
      "grad_norm": 0.6080477833747864,
      "learning_rate": 5e-05,
      "loss": 0.0398,
      "step": 1226
    },
    {
      "epoch": 1.436768149882904,
      "grad_norm": 0.8096323609352112,
      "learning_rate": 5e-05,
      "loss": 0.1062,
      "step": 1227
    },
    {
      "epoch": 1.4379391100702577,
      "grad_norm": 0.7041084170341492,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 1228
    },
    {
      "epoch": 1.4391100702576112,
      "grad_norm": 1.1823879480361938,
      "learning_rate": 5e-05,
      "loss": 0.1232,
      "step": 1229
    },
    {
      "epoch": 1.440281030444965,
      "grad_norm": 0.5650866627693176,
      "learning_rate": 5e-05,
      "loss": 0.093,
      "step": 1230
    },
    {
      "epoch": 1.4414519906323184,
      "grad_norm": 1.5511113405227661,
      "learning_rate": 5e-05,
      "loss": 0.1032,
      "step": 1231
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 0.7123513221740723,
      "learning_rate": 5e-05,
      "loss": 0.0688,
      "step": 1232
    },
    {
      "epoch": 1.4437939110070257,
      "grad_norm": 1.1770234107971191,
      "learning_rate": 5e-05,
      "loss": 0.1323,
      "step": 1233
    },
    {
      "epoch": 1.4449648711943794,
      "grad_norm": 0.793685257434845,
      "learning_rate": 5e-05,
      "loss": 0.0535,
      "step": 1234
    },
    {
      "epoch": 1.446135831381733,
      "grad_norm": 1.7285218238830566,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 1235
    },
    {
      "epoch": 1.4473067915690867,
      "grad_norm": 1.0906496047973633,
      "learning_rate": 5e-05,
      "loss": 0.1746,
      "step": 1236
    },
    {
      "epoch": 1.4484777517564402,
      "grad_norm": 1.5084810256958008,
      "learning_rate": 5e-05,
      "loss": 0.1715,
      "step": 1237
    },
    {
      "epoch": 1.449648711943794,
      "grad_norm": 0.6121231317520142,
      "learning_rate": 5e-05,
      "loss": 0.0555,
      "step": 1238
    },
    {
      "epoch": 1.4508196721311475,
      "grad_norm": 0.8661213517189026,
      "learning_rate": 5e-05,
      "loss": 0.1294,
      "step": 1239
    },
    {
      "epoch": 1.4519906323185012,
      "grad_norm": 0.5785569548606873,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 1240
    },
    {
      "epoch": 1.4531615925058547,
      "grad_norm": 0.5432900786399841,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 1241
    },
    {
      "epoch": 1.4543325526932085,
      "grad_norm": 1.543869137763977,
      "learning_rate": 5e-05,
      "loss": 0.0774,
      "step": 1242
    },
    {
      "epoch": 1.455503512880562,
      "grad_norm": 0.7065001726150513,
      "learning_rate": 5e-05,
      "loss": 0.0613,
      "step": 1243
    },
    {
      "epoch": 1.4566744730679158,
      "grad_norm": 0.6414383053779602,
      "learning_rate": 5e-05,
      "loss": 0.056,
      "step": 1244
    },
    {
      "epoch": 1.4578454332552693,
      "grad_norm": 1.0553858280181885,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 1245
    },
    {
      "epoch": 1.459016393442623,
      "grad_norm": 0.7836628556251526,
      "learning_rate": 5e-05,
      "loss": 0.0818,
      "step": 1246
    },
    {
      "epoch": 1.4601873536299765,
      "grad_norm": 1.496229648590088,
      "learning_rate": 5e-05,
      "loss": 0.0983,
      "step": 1247
    },
    {
      "epoch": 1.4613583138173303,
      "grad_norm": 0.7068586349487305,
      "learning_rate": 5e-05,
      "loss": 0.0389,
      "step": 1248
    },
    {
      "epoch": 1.4625292740046838,
      "grad_norm": 1.0964081287384033,
      "learning_rate": 5e-05,
      "loss": 0.1317,
      "step": 1249
    },
    {
      "epoch": 1.4637002341920375,
      "grad_norm": 0.5889285802841187,
      "learning_rate": 5e-05,
      "loss": 0.1255,
      "step": 1250
    },
    {
      "epoch": 1.464871194379391,
      "grad_norm": 0.9203143119812012,
      "learning_rate": 5e-05,
      "loss": 0.1542,
      "step": 1251
    },
    {
      "epoch": 1.4660421545667448,
      "grad_norm": 1.0004104375839233,
      "learning_rate": 5e-05,
      "loss": 0.1485,
      "step": 1252
    },
    {
      "epoch": 1.4672131147540983,
      "grad_norm": 0.9930676221847534,
      "learning_rate": 5e-05,
      "loss": 0.1368,
      "step": 1253
    },
    {
      "epoch": 1.468384074941452,
      "grad_norm": 0.9647878408432007,
      "learning_rate": 5e-05,
      "loss": 0.1007,
      "step": 1254
    },
    {
      "epoch": 1.4695550351288056,
      "grad_norm": 0.964389443397522,
      "learning_rate": 5e-05,
      "loss": 0.0516,
      "step": 1255
    },
    {
      "epoch": 1.4707259953161593,
      "grad_norm": 0.7911865711212158,
      "learning_rate": 5e-05,
      "loss": 0.0782,
      "step": 1256
    },
    {
      "epoch": 1.4718969555035128,
      "grad_norm": 1.7068318128585815,
      "learning_rate": 5e-05,
      "loss": 0.1035,
      "step": 1257
    },
    {
      "epoch": 1.4730679156908666,
      "grad_norm": 1.472000241279602,
      "learning_rate": 5e-05,
      "loss": 0.0873,
      "step": 1258
    },
    {
      "epoch": 1.4742388758782201,
      "grad_norm": 1.6126856803894043,
      "learning_rate": 5e-05,
      "loss": 0.1159,
      "step": 1259
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 1.1029733419418335,
      "learning_rate": 5e-05,
      "loss": 0.1235,
      "step": 1260
    },
    {
      "epoch": 1.4765807962529274,
      "grad_norm": 0.9481120705604553,
      "learning_rate": 5e-05,
      "loss": 0.0872,
      "step": 1261
    },
    {
      "epoch": 1.4777517564402811,
      "grad_norm": 0.6685899496078491,
      "learning_rate": 5e-05,
      "loss": 0.0438,
      "step": 1262
    },
    {
      "epoch": 1.4789227166276346,
      "grad_norm": 1.2327148914337158,
      "learning_rate": 5e-05,
      "loss": 0.0772,
      "step": 1263
    },
    {
      "epoch": 1.4800936768149882,
      "grad_norm": 1.3227190971374512,
      "learning_rate": 5e-05,
      "loss": 0.1036,
      "step": 1264
    },
    {
      "epoch": 1.481264637002342,
      "grad_norm": 0.8793398141860962,
      "learning_rate": 5e-05,
      "loss": 0.1617,
      "step": 1265
    },
    {
      "epoch": 1.4824355971896956,
      "grad_norm": 1.1199380159378052,
      "learning_rate": 5e-05,
      "loss": 0.0956,
      "step": 1266
    },
    {
      "epoch": 1.4836065573770492,
      "grad_norm": 3.0713462829589844,
      "learning_rate": 5e-05,
      "loss": 0.0907,
      "step": 1267
    },
    {
      "epoch": 1.4847775175644027,
      "grad_norm": 0.9141613245010376,
      "learning_rate": 5e-05,
      "loss": 0.103,
      "step": 1268
    },
    {
      "epoch": 1.4859484777517564,
      "grad_norm": 1.279405951499939,
      "learning_rate": 5e-05,
      "loss": 0.1494,
      "step": 1269
    },
    {
      "epoch": 1.4871194379391102,
      "grad_norm": 1.4868990182876587,
      "learning_rate": 5e-05,
      "loss": 0.1942,
      "step": 1270
    },
    {
      "epoch": 1.4882903981264637,
      "grad_norm": 1.1995693445205688,
      "learning_rate": 5e-05,
      "loss": 0.1417,
      "step": 1271
    },
    {
      "epoch": 1.4894613583138172,
      "grad_norm": 1.3070558309555054,
      "learning_rate": 5e-05,
      "loss": 0.0979,
      "step": 1272
    },
    {
      "epoch": 1.490632318501171,
      "grad_norm": 1.3834102153778076,
      "learning_rate": 5e-05,
      "loss": 0.1619,
      "step": 1273
    },
    {
      "epoch": 1.4918032786885247,
      "grad_norm": 0.8962812423706055,
      "learning_rate": 5e-05,
      "loss": 0.1531,
      "step": 1274
    },
    {
      "epoch": 1.4929742388758782,
      "grad_norm": 0.848649799823761,
      "learning_rate": 5e-05,
      "loss": 0.0509,
      "step": 1275
    },
    {
      "epoch": 1.4941451990632317,
      "grad_norm": 0.9648113250732422,
      "learning_rate": 5e-05,
      "loss": 0.1215,
      "step": 1276
    },
    {
      "epoch": 1.4953161592505855,
      "grad_norm": 1.1613446474075317,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 1277
    },
    {
      "epoch": 1.4964871194379392,
      "grad_norm": 0.565399169921875,
      "learning_rate": 5e-05,
      "loss": 0.1288,
      "step": 1278
    },
    {
      "epoch": 1.4976580796252927,
      "grad_norm": 0.5499268770217896,
      "learning_rate": 5e-05,
      "loss": 0.0744,
      "step": 1279
    },
    {
      "epoch": 1.4988290398126463,
      "grad_norm": 0.5467244386672974,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 1280
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.972409725189209,
      "learning_rate": 5e-05,
      "loss": 0.0772,
      "step": 1281
    },
    {
      "epoch": 1.5011709601873537,
      "grad_norm": 0.7476098537445068,
      "learning_rate": 5e-05,
      "loss": 0.0598,
      "step": 1282
    },
    {
      "epoch": 1.5023419203747073,
      "grad_norm": 0.5531211495399475,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1283
    },
    {
      "epoch": 1.5035128805620608,
      "grad_norm": 1.4540828466415405,
      "learning_rate": 5e-05,
      "loss": 0.4823,
      "step": 1284
    },
    {
      "epoch": 1.5046838407494145,
      "grad_norm": 1.3358666896820068,
      "learning_rate": 5e-05,
      "loss": 0.119,
      "step": 1285
    },
    {
      "epoch": 1.5058548009367683,
      "grad_norm": 1.4827834367752075,
      "learning_rate": 5e-05,
      "loss": 0.3129,
      "step": 1286
    },
    {
      "epoch": 1.5070257611241218,
      "grad_norm": 0.8616025447845459,
      "learning_rate": 5e-05,
      "loss": 0.1102,
      "step": 1287
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 1.1052266359329224,
      "learning_rate": 5e-05,
      "loss": 0.4149,
      "step": 1288
    },
    {
      "epoch": 1.509367681498829,
      "grad_norm": 1.1795741319656372,
      "learning_rate": 5e-05,
      "loss": 0.1006,
      "step": 1289
    },
    {
      "epoch": 1.5105386416861828,
      "grad_norm": 0.7827264070510864,
      "learning_rate": 5e-05,
      "loss": 0.0777,
      "step": 1290
    },
    {
      "epoch": 1.5117096018735363,
      "grad_norm": 1.0981111526489258,
      "learning_rate": 5e-05,
      "loss": 0.1443,
      "step": 1291
    },
    {
      "epoch": 1.5128805620608898,
      "grad_norm": 1.6739681959152222,
      "learning_rate": 5e-05,
      "loss": 0.1955,
      "step": 1292
    },
    {
      "epoch": 1.5140515222482436,
      "grad_norm": 0.7032600045204163,
      "learning_rate": 5e-05,
      "loss": 0.0458,
      "step": 1293
    },
    {
      "epoch": 1.5152224824355973,
      "grad_norm": 0.8911421895027161,
      "learning_rate": 5e-05,
      "loss": 0.0454,
      "step": 1294
    },
    {
      "epoch": 1.5163934426229508,
      "grad_norm": 1.0656037330627441,
      "learning_rate": 5e-05,
      "loss": 0.1629,
      "step": 1295
    },
    {
      "epoch": 1.5175644028103044,
      "grad_norm": 0.7801848649978638,
      "learning_rate": 5e-05,
      "loss": 0.1636,
      "step": 1296
    },
    {
      "epoch": 1.518735362997658,
      "grad_norm": 0.9443115592002869,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 1297
    },
    {
      "epoch": 1.5199063231850118,
      "grad_norm": 0.9611455202102661,
      "learning_rate": 5e-05,
      "loss": 0.0822,
      "step": 1298
    },
    {
      "epoch": 1.5210772833723654,
      "grad_norm": 0.9233046770095825,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 1299
    },
    {
      "epoch": 1.5222482435597189,
      "grad_norm": 0.794922411441803,
      "learning_rate": 5e-05,
      "loss": 0.0646,
      "step": 1300
    },
    {
      "epoch": 1.5234192037470726,
      "grad_norm": 0.6607669591903687,
      "learning_rate": 5e-05,
      "loss": 0.0553,
      "step": 1301
    },
    {
      "epoch": 1.5245901639344264,
      "grad_norm": 0.9143939018249512,
      "learning_rate": 5e-05,
      "loss": 0.04,
      "step": 1302
    },
    {
      "epoch": 1.5257611241217799,
      "grad_norm": 0.6502789855003357,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 1303
    },
    {
      "epoch": 1.5269320843091334,
      "grad_norm": 1.3538360595703125,
      "learning_rate": 5e-05,
      "loss": 0.117,
      "step": 1304
    },
    {
      "epoch": 1.5281030444964872,
      "grad_norm": 1.0603151321411133,
      "learning_rate": 5e-05,
      "loss": 0.1418,
      "step": 1305
    },
    {
      "epoch": 1.529274004683841,
      "grad_norm": 0.5093165040016174,
      "learning_rate": 5e-05,
      "loss": 0.0422,
      "step": 1306
    },
    {
      "epoch": 1.5304449648711944,
      "grad_norm": 1.09296452999115,
      "learning_rate": 5e-05,
      "loss": 0.149,
      "step": 1307
    },
    {
      "epoch": 1.531615925058548,
      "grad_norm": 1.3567713499069214,
      "learning_rate": 5e-05,
      "loss": 0.134,
      "step": 1308
    },
    {
      "epoch": 1.5327868852459017,
      "grad_norm": 1.5836751461029053,
      "learning_rate": 5e-05,
      "loss": 0.194,
      "step": 1309
    },
    {
      "epoch": 1.5339578454332554,
      "grad_norm": 1.7888482809066772,
      "learning_rate": 5e-05,
      "loss": 0.0634,
      "step": 1310
    },
    {
      "epoch": 1.535128805620609,
      "grad_norm": 1.0097655057907104,
      "learning_rate": 5e-05,
      "loss": 0.1338,
      "step": 1311
    },
    {
      "epoch": 1.5362997658079625,
      "grad_norm": 1.4502114057540894,
      "learning_rate": 5e-05,
      "loss": 0.1462,
      "step": 1312
    },
    {
      "epoch": 1.5374707259953162,
      "grad_norm": 1.62628972530365,
      "learning_rate": 5e-05,
      "loss": 0.2026,
      "step": 1313
    },
    {
      "epoch": 1.53864168618267,
      "grad_norm": 0.7066994309425354,
      "learning_rate": 5e-05,
      "loss": 0.1364,
      "step": 1314
    },
    {
      "epoch": 1.5398126463700235,
      "grad_norm": 0.7620708346366882,
      "learning_rate": 5e-05,
      "loss": 0.0499,
      "step": 1315
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 1.1274381875991821,
      "learning_rate": 5e-05,
      "loss": 0.1034,
      "step": 1316
    },
    {
      "epoch": 1.5421545667447307,
      "grad_norm": 1.676021933555603,
      "learning_rate": 5e-05,
      "loss": 0.2932,
      "step": 1317
    },
    {
      "epoch": 1.5433255269320845,
      "grad_norm": 1.4857165813446045,
      "learning_rate": 5e-05,
      "loss": 0.0707,
      "step": 1318
    },
    {
      "epoch": 1.544496487119438,
      "grad_norm": 0.9837768077850342,
      "learning_rate": 5e-05,
      "loss": 0.1182,
      "step": 1319
    },
    {
      "epoch": 1.5456674473067915,
      "grad_norm": 0.9199222326278687,
      "learning_rate": 5e-05,
      "loss": 0.0798,
      "step": 1320
    },
    {
      "epoch": 1.5468384074941453,
      "grad_norm": 1.1292414665222168,
      "learning_rate": 5e-05,
      "loss": 0.1222,
      "step": 1321
    },
    {
      "epoch": 1.548009367681499,
      "grad_norm": 0.90902179479599,
      "learning_rate": 5e-05,
      "loss": 0.0944,
      "step": 1322
    },
    {
      "epoch": 1.5491803278688525,
      "grad_norm": 0.9819146394729614,
      "learning_rate": 5e-05,
      "loss": 0.1453,
      "step": 1323
    },
    {
      "epoch": 1.550351288056206,
      "grad_norm": 0.6617109179496765,
      "learning_rate": 5e-05,
      "loss": 0.0595,
      "step": 1324
    },
    {
      "epoch": 1.5515222482435598,
      "grad_norm": 0.7739900350570679,
      "learning_rate": 5e-05,
      "loss": 0.143,
      "step": 1325
    },
    {
      "epoch": 1.5526932084309133,
      "grad_norm": 0.6529239416122437,
      "learning_rate": 5e-05,
      "loss": 0.059,
      "step": 1326
    },
    {
      "epoch": 1.5538641686182668,
      "grad_norm": 0.9203221201896667,
      "learning_rate": 5e-05,
      "loss": 0.0795,
      "step": 1327
    },
    {
      "epoch": 1.5550351288056206,
      "grad_norm": 0.6717115640640259,
      "learning_rate": 5e-05,
      "loss": 0.062,
      "step": 1328
    },
    {
      "epoch": 1.5562060889929743,
      "grad_norm": 1.0440460443496704,
      "learning_rate": 5e-05,
      "loss": 0.1702,
      "step": 1329
    },
    {
      "epoch": 1.5573770491803278,
      "grad_norm": 0.9479598999023438,
      "learning_rate": 5e-05,
      "loss": 0.099,
      "step": 1330
    },
    {
      "epoch": 1.5585480093676813,
      "grad_norm": 1.5259559154510498,
      "learning_rate": 5e-05,
      "loss": 0.0658,
      "step": 1331
    },
    {
      "epoch": 1.559718969555035,
      "grad_norm": 1.1695046424865723,
      "learning_rate": 5e-05,
      "loss": 0.2221,
      "step": 1332
    },
    {
      "epoch": 1.5608899297423888,
      "grad_norm": 1.1967346668243408,
      "learning_rate": 5e-05,
      "loss": 0.0901,
      "step": 1333
    },
    {
      "epoch": 1.5620608899297423,
      "grad_norm": 1.0239274501800537,
      "learning_rate": 5e-05,
      "loss": 0.2168,
      "step": 1334
    },
    {
      "epoch": 1.5632318501170959,
      "grad_norm": 0.8890867829322815,
      "learning_rate": 5e-05,
      "loss": 0.1387,
      "step": 1335
    },
    {
      "epoch": 1.5644028103044496,
      "grad_norm": 1.0049127340316772,
      "learning_rate": 5e-05,
      "loss": 0.0926,
      "step": 1336
    },
    {
      "epoch": 1.5655737704918034,
      "grad_norm": 0.9547470808029175,
      "learning_rate": 5e-05,
      "loss": 0.0821,
      "step": 1337
    },
    {
      "epoch": 1.5667447306791569,
      "grad_norm": 1.0660473108291626,
      "learning_rate": 5e-05,
      "loss": 0.0788,
      "step": 1338
    },
    {
      "epoch": 1.5679156908665104,
      "grad_norm": 1.2099283933639526,
      "learning_rate": 5e-05,
      "loss": 0.1164,
      "step": 1339
    },
    {
      "epoch": 1.5690866510538641,
      "grad_norm": 1.252034068107605,
      "learning_rate": 5e-05,
      "loss": 0.099,
      "step": 1340
    },
    {
      "epoch": 1.5702576112412179,
      "grad_norm": 0.9606687426567078,
      "learning_rate": 5e-05,
      "loss": 0.0767,
      "step": 1341
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.008123517036438,
      "learning_rate": 5e-05,
      "loss": 0.059,
      "step": 1342
    },
    {
      "epoch": 1.572599531615925,
      "grad_norm": 1.5694223642349243,
      "learning_rate": 5e-05,
      "loss": 0.0662,
      "step": 1343
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 1.1141853332519531,
      "learning_rate": 5e-05,
      "loss": 0.1038,
      "step": 1344
    },
    {
      "epoch": 1.5749414519906324,
      "grad_norm": 1.073012351989746,
      "learning_rate": 5e-05,
      "loss": 0.1918,
      "step": 1345
    },
    {
      "epoch": 1.576112412177986,
      "grad_norm": 0.8668918013572693,
      "learning_rate": 5e-05,
      "loss": 0.0972,
      "step": 1346
    },
    {
      "epoch": 1.5772833723653394,
      "grad_norm": 1.5718532800674438,
      "learning_rate": 5e-05,
      "loss": 0.2427,
      "step": 1347
    },
    {
      "epoch": 1.5784543325526932,
      "grad_norm": 1.002062201499939,
      "learning_rate": 5e-05,
      "loss": 0.0553,
      "step": 1348
    },
    {
      "epoch": 1.579625292740047,
      "grad_norm": 4.052907466888428,
      "learning_rate": 5e-05,
      "loss": 0.4247,
      "step": 1349
    },
    {
      "epoch": 1.5807962529274004,
      "grad_norm": 0.7413405776023865,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 1350
    },
    {
      "epoch": 1.581967213114754,
      "grad_norm": 1.9572510719299316,
      "learning_rate": 5e-05,
      "loss": 0.3298,
      "step": 1351
    },
    {
      "epoch": 1.5831381733021077,
      "grad_norm": 1.5842580795288086,
      "learning_rate": 5e-05,
      "loss": 0.1346,
      "step": 1352
    },
    {
      "epoch": 1.5843091334894615,
      "grad_norm": 0.8773632645606995,
      "learning_rate": 5e-05,
      "loss": 0.0614,
      "step": 1353
    },
    {
      "epoch": 1.585480093676815,
      "grad_norm": 1.0071467161178589,
      "learning_rate": 5e-05,
      "loss": 0.1581,
      "step": 1354
    },
    {
      "epoch": 1.5866510538641685,
      "grad_norm": 0.5661211609840393,
      "learning_rate": 5e-05,
      "loss": 0.0443,
      "step": 1355
    },
    {
      "epoch": 1.5878220140515222,
      "grad_norm": 0.9017133712768555,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 1356
    },
    {
      "epoch": 1.588992974238876,
      "grad_norm": 1.2195191383361816,
      "learning_rate": 5e-05,
      "loss": 0.184,
      "step": 1357
    },
    {
      "epoch": 1.5901639344262295,
      "grad_norm": 0.8138027191162109,
      "learning_rate": 5e-05,
      "loss": 0.0823,
      "step": 1358
    },
    {
      "epoch": 1.591334894613583,
      "grad_norm": 0.7849574685096741,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 1359
    },
    {
      "epoch": 1.5925058548009368,
      "grad_norm": 0.6601282358169556,
      "learning_rate": 5e-05,
      "loss": 0.0704,
      "step": 1360
    },
    {
      "epoch": 1.5936768149882905,
      "grad_norm": 0.9535797238349915,
      "learning_rate": 5e-05,
      "loss": 0.134,
      "step": 1361
    },
    {
      "epoch": 1.594847775175644,
      "grad_norm": 0.5802857279777527,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 1362
    },
    {
      "epoch": 1.5960187353629975,
      "grad_norm": 1.5972309112548828,
      "learning_rate": 5e-05,
      "loss": 0.1945,
      "step": 1363
    },
    {
      "epoch": 1.5971896955503513,
      "grad_norm": 0.9973304271697998,
      "learning_rate": 5e-05,
      "loss": 0.0486,
      "step": 1364
    },
    {
      "epoch": 1.598360655737705,
      "grad_norm": 1.0934491157531738,
      "learning_rate": 5e-05,
      "loss": 0.1017,
      "step": 1365
    },
    {
      "epoch": 1.5995316159250585,
      "grad_norm": 1.4492969512939453,
      "learning_rate": 5e-05,
      "loss": 0.0571,
      "step": 1366
    },
    {
      "epoch": 1.600702576112412,
      "grad_norm": 0.5188602209091187,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 1367
    },
    {
      "epoch": 1.6018735362997658,
      "grad_norm": 1.28490149974823,
      "learning_rate": 5e-05,
      "loss": 0.1874,
      "step": 1368
    },
    {
      "epoch": 1.6030444964871196,
      "grad_norm": 1.0590182542800903,
      "learning_rate": 5e-05,
      "loss": 0.0707,
      "step": 1369
    },
    {
      "epoch": 1.604215456674473,
      "grad_norm": 1.8075156211853027,
      "learning_rate": 5e-05,
      "loss": 0.0888,
      "step": 1370
    },
    {
      "epoch": 1.6053864168618266,
      "grad_norm": 1.7817696332931519,
      "learning_rate": 5e-05,
      "loss": 0.0911,
      "step": 1371
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 1.108451247215271,
      "learning_rate": 5e-05,
      "loss": 0.1947,
      "step": 1372
    },
    {
      "epoch": 1.607728337236534,
      "grad_norm": 1.1265122890472412,
      "learning_rate": 5e-05,
      "loss": 0.1302,
      "step": 1373
    },
    {
      "epoch": 1.6088992974238876,
      "grad_norm": 0.6338521838188171,
      "learning_rate": 5e-05,
      "loss": 0.0709,
      "step": 1374
    },
    {
      "epoch": 1.6100702576112411,
      "grad_norm": 0.9577800631523132,
      "learning_rate": 5e-05,
      "loss": 0.0882,
      "step": 1375
    },
    {
      "epoch": 1.6112412177985949,
      "grad_norm": 0.7203254699707031,
      "learning_rate": 5e-05,
      "loss": 0.0596,
      "step": 1376
    },
    {
      "epoch": 1.6124121779859486,
      "grad_norm": 0.6525381207466125,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 1377
    },
    {
      "epoch": 1.6135831381733021,
      "grad_norm": 0.968823254108429,
      "learning_rate": 5e-05,
      "loss": 0.0527,
      "step": 1378
    },
    {
      "epoch": 1.6147540983606556,
      "grad_norm": 1.0519386529922485,
      "learning_rate": 5e-05,
      "loss": 0.2068,
      "step": 1379
    },
    {
      "epoch": 1.6159250585480094,
      "grad_norm": 1.0241178274154663,
      "learning_rate": 5e-05,
      "loss": 0.1861,
      "step": 1380
    },
    {
      "epoch": 1.6170960187353631,
      "grad_norm": 1.7581462860107422,
      "learning_rate": 5e-05,
      "loss": 0.1548,
      "step": 1381
    },
    {
      "epoch": 1.6182669789227166,
      "grad_norm": 0.8883626461029053,
      "learning_rate": 5e-05,
      "loss": 0.0901,
      "step": 1382
    },
    {
      "epoch": 1.6194379391100702,
      "grad_norm": 0.681879997253418,
      "learning_rate": 5e-05,
      "loss": 0.0898,
      "step": 1383
    },
    {
      "epoch": 1.620608899297424,
      "grad_norm": 1.2092809677124023,
      "learning_rate": 5e-05,
      "loss": 0.1312,
      "step": 1384
    },
    {
      "epoch": 1.6217798594847777,
      "grad_norm": 0.704224169254303,
      "learning_rate": 5e-05,
      "loss": 0.0692,
      "step": 1385
    },
    {
      "epoch": 1.6229508196721312,
      "grad_norm": 0.832720935344696,
      "learning_rate": 5e-05,
      "loss": 0.0677,
      "step": 1386
    },
    {
      "epoch": 1.6241217798594847,
      "grad_norm": 0.7465362548828125,
      "learning_rate": 5e-05,
      "loss": 0.1927,
      "step": 1387
    },
    {
      "epoch": 1.6252927400468384,
      "grad_norm": 0.74674391746521,
      "learning_rate": 5e-05,
      "loss": 0.1073,
      "step": 1388
    },
    {
      "epoch": 1.6264637002341922,
      "grad_norm": 0.7382458448410034,
      "learning_rate": 5e-05,
      "loss": 0.0779,
      "step": 1389
    },
    {
      "epoch": 1.6276346604215457,
      "grad_norm": 0.8227776885032654,
      "learning_rate": 5e-05,
      "loss": 0.1091,
      "step": 1390
    },
    {
      "epoch": 1.6288056206088992,
      "grad_norm": 0.8458611965179443,
      "learning_rate": 5e-05,
      "loss": 0.0508,
      "step": 1391
    },
    {
      "epoch": 1.629976580796253,
      "grad_norm": 0.916513979434967,
      "learning_rate": 5e-05,
      "loss": 0.0789,
      "step": 1392
    },
    {
      "epoch": 1.6311475409836067,
      "grad_norm": 0.73563152551651,
      "learning_rate": 5e-05,
      "loss": 0.0493,
      "step": 1393
    },
    {
      "epoch": 1.6323185011709602,
      "grad_norm": 0.6498777866363525,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 1394
    },
    {
      "epoch": 1.6334894613583137,
      "grad_norm": 0.8062886595726013,
      "learning_rate": 5e-05,
      "loss": 0.0728,
      "step": 1395
    },
    {
      "epoch": 1.6346604215456675,
      "grad_norm": 1.2528129816055298,
      "learning_rate": 5e-05,
      "loss": 0.1231,
      "step": 1396
    },
    {
      "epoch": 1.6358313817330212,
      "grad_norm": 1.3308837413787842,
      "learning_rate": 5e-05,
      "loss": 0.1109,
      "step": 1397
    },
    {
      "epoch": 1.6370023419203747,
      "grad_norm": 1.0237836837768555,
      "learning_rate": 5e-05,
      "loss": 0.0886,
      "step": 1398
    },
    {
      "epoch": 1.6381733021077283,
      "grad_norm": 1.208488941192627,
      "learning_rate": 5e-05,
      "loss": 0.1635,
      "step": 1399
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 1.2476131916046143,
      "learning_rate": 5e-05,
      "loss": 0.0802,
      "step": 1400
    },
    {
      "epoch": 1.6405152224824358,
      "grad_norm": 1.1299774646759033,
      "learning_rate": 5e-05,
      "loss": 0.0905,
      "step": 1401
    },
    {
      "epoch": 1.6416861826697893,
      "grad_norm": 0.5542737245559692,
      "learning_rate": 5e-05,
      "loss": 0.0657,
      "step": 1402
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 0.931282103061676,
      "learning_rate": 5e-05,
      "loss": 0.0713,
      "step": 1403
    },
    {
      "epoch": 1.6440281030444965,
      "grad_norm": 0.7374131083488464,
      "learning_rate": 5e-05,
      "loss": 0.1043,
      "step": 1404
    },
    {
      "epoch": 1.6451990632318503,
      "grad_norm": 1.0523459911346436,
      "learning_rate": 5e-05,
      "loss": 0.1069,
      "step": 1405
    },
    {
      "epoch": 1.6463700234192038,
      "grad_norm": 2.683830976486206,
      "learning_rate": 5e-05,
      "loss": 0.2509,
      "step": 1406
    },
    {
      "epoch": 1.6475409836065573,
      "grad_norm": 0.3176509439945221,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 1407
    },
    {
      "epoch": 1.648711943793911,
      "grad_norm": 0.7241633534431458,
      "learning_rate": 5e-05,
      "loss": 0.1239,
      "step": 1408
    },
    {
      "epoch": 1.6498829039812648,
      "grad_norm": 1.2519718408584595,
      "learning_rate": 5e-05,
      "loss": 0.0898,
      "step": 1409
    },
    {
      "epoch": 1.651053864168618,
      "grad_norm": 1.2762794494628906,
      "learning_rate": 5e-05,
      "loss": 0.1369,
      "step": 1410
    },
    {
      "epoch": 1.6522248243559718,
      "grad_norm": 0.5744096636772156,
      "learning_rate": 5e-05,
      "loss": 0.0425,
      "step": 1411
    },
    {
      "epoch": 1.6533957845433256,
      "grad_norm": 0.6632373332977295,
      "learning_rate": 5e-05,
      "loss": 0.0672,
      "step": 1412
    },
    {
      "epoch": 1.654566744730679,
      "grad_norm": 0.9004911780357361,
      "learning_rate": 5e-05,
      "loss": 0.1546,
      "step": 1413
    },
    {
      "epoch": 1.6557377049180326,
      "grad_norm": 0.7063676714897156,
      "learning_rate": 5e-05,
      "loss": 0.1495,
      "step": 1414
    },
    {
      "epoch": 1.6569086651053864,
      "grad_norm": 0.9578944444656372,
      "learning_rate": 5e-05,
      "loss": 0.0682,
      "step": 1415
    },
    {
      "epoch": 1.6580796252927401,
      "grad_norm": 0.677183985710144,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 1416
    },
    {
      "epoch": 1.6592505854800936,
      "grad_norm": 1.9201902151107788,
      "learning_rate": 5e-05,
      "loss": 0.2179,
      "step": 1417
    },
    {
      "epoch": 1.6604215456674472,
      "grad_norm": 0.7700191736221313,
      "learning_rate": 5e-05,
      "loss": 0.0768,
      "step": 1418
    },
    {
      "epoch": 1.661592505854801,
      "grad_norm": 1.4584251642227173,
      "learning_rate": 5e-05,
      "loss": 0.1325,
      "step": 1419
    },
    {
      "epoch": 1.6627634660421546,
      "grad_norm": 0.8770390748977661,
      "learning_rate": 5e-05,
      "loss": 0.1152,
      "step": 1420
    },
    {
      "epoch": 1.6639344262295082,
      "grad_norm": 1.0262575149536133,
      "learning_rate": 5e-05,
      "loss": 0.0443,
      "step": 1421
    },
    {
      "epoch": 1.6651053864168617,
      "grad_norm": 0.574611485004425,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 1422
    },
    {
      "epoch": 1.6662763466042154,
      "grad_norm": 1.001624345779419,
      "learning_rate": 5e-05,
      "loss": 0.0946,
      "step": 1423
    },
    {
      "epoch": 1.6674473067915692,
      "grad_norm": 1.2795003652572632,
      "learning_rate": 5e-05,
      "loss": 0.1779,
      "step": 1424
    },
    {
      "epoch": 1.6686182669789227,
      "grad_norm": 0.6643629670143127,
      "learning_rate": 5e-05,
      "loss": 0.0502,
      "step": 1425
    },
    {
      "epoch": 1.6697892271662762,
      "grad_norm": 0.3596784472465515,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 1426
    },
    {
      "epoch": 1.67096018735363,
      "grad_norm": 1.0851202011108398,
      "learning_rate": 5e-05,
      "loss": 0.1439,
      "step": 1427
    },
    {
      "epoch": 1.6721311475409837,
      "grad_norm": 1.641810655593872,
      "learning_rate": 5e-05,
      "loss": 0.145,
      "step": 1428
    },
    {
      "epoch": 1.6733021077283372,
      "grad_norm": 1.2830545902252197,
      "learning_rate": 5e-05,
      "loss": 0.0819,
      "step": 1429
    },
    {
      "epoch": 1.6744730679156907,
      "grad_norm": 0.7351986765861511,
      "learning_rate": 5e-05,
      "loss": 0.0817,
      "step": 1430
    },
    {
      "epoch": 1.6756440281030445,
      "grad_norm": 1.104832410812378,
      "learning_rate": 5e-05,
      "loss": 0.1121,
      "step": 1431
    },
    {
      "epoch": 1.6768149882903982,
      "grad_norm": 0.521543562412262,
      "learning_rate": 5e-05,
      "loss": 0.0548,
      "step": 1432
    },
    {
      "epoch": 1.6779859484777517,
      "grad_norm": 1.1207939386367798,
      "learning_rate": 5e-05,
      "loss": 0.1273,
      "step": 1433
    },
    {
      "epoch": 1.6791569086651053,
      "grad_norm": 0.8240570425987244,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 1434
    },
    {
      "epoch": 1.680327868852459,
      "grad_norm": 0.9607218503952026,
      "learning_rate": 5e-05,
      "loss": 0.1917,
      "step": 1435
    },
    {
      "epoch": 1.6814988290398127,
      "grad_norm": 0.997840404510498,
      "learning_rate": 5e-05,
      "loss": 0.0833,
      "step": 1436
    },
    {
      "epoch": 1.6826697892271663,
      "grad_norm": 1.1759110689163208,
      "learning_rate": 5e-05,
      "loss": 0.0824,
      "step": 1437
    },
    {
      "epoch": 1.6838407494145198,
      "grad_norm": 1.3640998601913452,
      "learning_rate": 5e-05,
      "loss": 0.1801,
      "step": 1438
    },
    {
      "epoch": 1.6850117096018735,
      "grad_norm": 0.7978399991989136,
      "learning_rate": 5e-05,
      "loss": 0.0446,
      "step": 1439
    },
    {
      "epoch": 1.6861826697892273,
      "grad_norm": 1.0379273891448975,
      "learning_rate": 5e-05,
      "loss": 0.0499,
      "step": 1440
    },
    {
      "epoch": 1.6873536299765808,
      "grad_norm": 0.6621832847595215,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 1441
    },
    {
      "epoch": 1.6885245901639343,
      "grad_norm": 0.7825035452842712,
      "learning_rate": 5e-05,
      "loss": 0.1008,
      "step": 1442
    },
    {
      "epoch": 1.689695550351288,
      "grad_norm": 0.8697498440742493,
      "learning_rate": 5e-05,
      "loss": 0.1374,
      "step": 1443
    },
    {
      "epoch": 1.6908665105386418,
      "grad_norm": 0.8459056615829468,
      "learning_rate": 5e-05,
      "loss": 0.1014,
      "step": 1444
    },
    {
      "epoch": 1.6920374707259953,
      "grad_norm": 0.7929999232292175,
      "learning_rate": 5e-05,
      "loss": 0.1163,
      "step": 1445
    },
    {
      "epoch": 1.6932084309133488,
      "grad_norm": 1.0241729021072388,
      "learning_rate": 5e-05,
      "loss": 0.1283,
      "step": 1446
    },
    {
      "epoch": 1.6943793911007026,
      "grad_norm": 1.0765933990478516,
      "learning_rate": 5e-05,
      "loss": 0.1194,
      "step": 1447
    },
    {
      "epoch": 1.6955503512880563,
      "grad_norm": 0.7059209942817688,
      "learning_rate": 5e-05,
      "loss": 0.0683,
      "step": 1448
    },
    {
      "epoch": 1.6967213114754098,
      "grad_norm": 1.040554165840149,
      "learning_rate": 5e-05,
      "loss": 0.0369,
      "step": 1449
    },
    {
      "epoch": 1.6978922716627634,
      "grad_norm": 0.745676577091217,
      "learning_rate": 5e-05,
      "loss": 0.0571,
      "step": 1450
    },
    {
      "epoch": 1.699063231850117,
      "grad_norm": 0.3946520686149597,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 1451
    },
    {
      "epoch": 1.7002341920374708,
      "grad_norm": 1.5022807121276855,
      "learning_rate": 5e-05,
      "loss": 0.122,
      "step": 1452
    },
    {
      "epoch": 1.7014051522248244,
      "grad_norm": 0.9991323947906494,
      "learning_rate": 5e-05,
      "loss": 0.0603,
      "step": 1453
    },
    {
      "epoch": 1.7025761124121779,
      "grad_norm": 1.213980793952942,
      "learning_rate": 5e-05,
      "loss": 0.122,
      "step": 1454
    },
    {
      "epoch": 1.7037470725995316,
      "grad_norm": 0.9178876876831055,
      "learning_rate": 5e-05,
      "loss": 0.1298,
      "step": 1455
    },
    {
      "epoch": 1.7049180327868854,
      "grad_norm": 0.62298983335495,
      "learning_rate": 5e-05,
      "loss": 0.0546,
      "step": 1456
    },
    {
      "epoch": 1.7060889929742389,
      "grad_norm": 1.9637142419815063,
      "learning_rate": 5e-05,
      "loss": 0.1068,
      "step": 1457
    },
    {
      "epoch": 1.7072599531615924,
      "grad_norm": 1.6059027910232544,
      "learning_rate": 5e-05,
      "loss": 0.0873,
      "step": 1458
    },
    {
      "epoch": 1.7084309133489461,
      "grad_norm": 0.7113065123558044,
      "learning_rate": 5e-05,
      "loss": 0.1324,
      "step": 1459
    },
    {
      "epoch": 1.7096018735362999,
      "grad_norm": 0.7299492955207825,
      "learning_rate": 5e-05,
      "loss": 0.0672,
      "step": 1460
    },
    {
      "epoch": 1.7107728337236534,
      "grad_norm": 1.2371225357055664,
      "learning_rate": 5e-05,
      "loss": 0.129,
      "step": 1461
    },
    {
      "epoch": 1.711943793911007,
      "grad_norm": 1.9775642156600952,
      "learning_rate": 5e-05,
      "loss": 0.3241,
      "step": 1462
    },
    {
      "epoch": 1.7131147540983607,
      "grad_norm": 1.3510595560073853,
      "learning_rate": 5e-05,
      "loss": 0.0845,
      "step": 1463
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.7083068490028381,
      "learning_rate": 5e-05,
      "loss": 0.0549,
      "step": 1464
    },
    {
      "epoch": 1.715456674473068,
      "grad_norm": 0.5587238669395447,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 1465
    },
    {
      "epoch": 1.7166276346604215,
      "grad_norm": 1.2534489631652832,
      "learning_rate": 5e-05,
      "loss": 0.1461,
      "step": 1466
    },
    {
      "epoch": 1.7177985948477752,
      "grad_norm": 0.7907682657241821,
      "learning_rate": 5e-05,
      "loss": 0.0728,
      "step": 1467
    },
    {
      "epoch": 1.718969555035129,
      "grad_norm": 1.4171998500823975,
      "learning_rate": 5e-05,
      "loss": 0.2364,
      "step": 1468
    },
    {
      "epoch": 1.7201405152224825,
      "grad_norm": 0.7097839713096619,
      "learning_rate": 5e-05,
      "loss": 0.0478,
      "step": 1469
    },
    {
      "epoch": 1.721311475409836,
      "grad_norm": 0.7293909788131714,
      "learning_rate": 5e-05,
      "loss": 0.1536,
      "step": 1470
    },
    {
      "epoch": 1.7224824355971897,
      "grad_norm": 1.002242922782898,
      "learning_rate": 5e-05,
      "loss": 0.0904,
      "step": 1471
    },
    {
      "epoch": 1.7236533957845435,
      "grad_norm": 0.6391021609306335,
      "learning_rate": 5e-05,
      "loss": 0.0426,
      "step": 1472
    },
    {
      "epoch": 1.724824355971897,
      "grad_norm": 1.0695136785507202,
      "learning_rate": 5e-05,
      "loss": 0.0468,
      "step": 1473
    },
    {
      "epoch": 1.7259953161592505,
      "grad_norm": 0.7871338129043579,
      "learning_rate": 5e-05,
      "loss": 0.1044,
      "step": 1474
    },
    {
      "epoch": 1.7271662763466042,
      "grad_norm": 1.2329667806625366,
      "learning_rate": 5e-05,
      "loss": 0.0975,
      "step": 1475
    },
    {
      "epoch": 1.728337236533958,
      "grad_norm": 1.2718297243118286,
      "learning_rate": 5e-05,
      "loss": 0.1092,
      "step": 1476
    },
    {
      "epoch": 1.7295081967213115,
      "grad_norm": 1.0329811573028564,
      "learning_rate": 5e-05,
      "loss": 0.1208,
      "step": 1477
    },
    {
      "epoch": 1.730679156908665,
      "grad_norm": 1.000523567199707,
      "learning_rate": 5e-05,
      "loss": 0.0717,
      "step": 1478
    },
    {
      "epoch": 1.7318501170960188,
      "grad_norm": 0.955432116985321,
      "learning_rate": 5e-05,
      "loss": 0.0867,
      "step": 1479
    },
    {
      "epoch": 1.7330210772833725,
      "grad_norm": 0.6265769004821777,
      "learning_rate": 5e-05,
      "loss": 0.0938,
      "step": 1480
    },
    {
      "epoch": 1.734192037470726,
      "grad_norm": 0.9963459968566895,
      "learning_rate": 5e-05,
      "loss": 0.1181,
      "step": 1481
    },
    {
      "epoch": 1.7353629976580796,
      "grad_norm": 1.5047811269760132,
      "learning_rate": 5e-05,
      "loss": 0.0979,
      "step": 1482
    },
    {
      "epoch": 1.7365339578454333,
      "grad_norm": 0.9288880228996277,
      "learning_rate": 5e-05,
      "loss": 0.0661,
      "step": 1483
    },
    {
      "epoch": 1.737704918032787,
      "grad_norm": 1.0267879962921143,
      "learning_rate": 5e-05,
      "loss": 0.2009,
      "step": 1484
    },
    {
      "epoch": 1.7388758782201406,
      "grad_norm": 1.4490476846694946,
      "learning_rate": 5e-05,
      "loss": 0.0647,
      "step": 1485
    },
    {
      "epoch": 1.740046838407494,
      "grad_norm": 1.2611651420593262,
      "learning_rate": 5e-05,
      "loss": 0.1396,
      "step": 1486
    },
    {
      "epoch": 1.7412177985948478,
      "grad_norm": 1.2104945182800293,
      "learning_rate": 5e-05,
      "loss": 0.0739,
      "step": 1487
    },
    {
      "epoch": 1.7423887587822016,
      "grad_norm": 2.1345362663269043,
      "learning_rate": 5e-05,
      "loss": 0.0777,
      "step": 1488
    },
    {
      "epoch": 1.743559718969555,
      "grad_norm": 0.850832462310791,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 1489
    },
    {
      "epoch": 1.7447306791569086,
      "grad_norm": 0.7515583038330078,
      "learning_rate": 5e-05,
      "loss": 0.0502,
      "step": 1490
    },
    {
      "epoch": 1.7459016393442623,
      "grad_norm": 1.0584203004837036,
      "learning_rate": 5e-05,
      "loss": 0.2276,
      "step": 1491
    },
    {
      "epoch": 1.747072599531616,
      "grad_norm": 1.3468197584152222,
      "learning_rate": 5e-05,
      "loss": 0.0529,
      "step": 1492
    },
    {
      "epoch": 1.7482435597189696,
      "grad_norm": 0.9475806355476379,
      "learning_rate": 5e-05,
      "loss": 0.0811,
      "step": 1493
    },
    {
      "epoch": 1.7494145199063231,
      "grad_norm": 0.9459162354469299,
      "learning_rate": 5e-05,
      "loss": 0.0813,
      "step": 1494
    },
    {
      "epoch": 1.7505854800936769,
      "grad_norm": 1.0849144458770752,
      "learning_rate": 5e-05,
      "loss": 0.1023,
      "step": 1495
    },
    {
      "epoch": 1.7517564402810304,
      "grad_norm": 2.873790979385376,
      "learning_rate": 5e-05,
      "loss": 0.1154,
      "step": 1496
    },
    {
      "epoch": 1.752927400468384,
      "grad_norm": 0.8405879139900208,
      "learning_rate": 5e-05,
      "loss": 0.061,
      "step": 1497
    },
    {
      "epoch": 1.7540983606557377,
      "grad_norm": 0.705544650554657,
      "learning_rate": 5e-05,
      "loss": 0.1415,
      "step": 1498
    },
    {
      "epoch": 1.7552693208430914,
      "grad_norm": 0.5565266013145447,
      "learning_rate": 5e-05,
      "loss": 0.0617,
      "step": 1499
    },
    {
      "epoch": 1.756440281030445,
      "grad_norm": 0.7156499624252319,
      "learning_rate": 5e-05,
      "loss": 0.0935,
      "step": 1500
    },
    {
      "epoch": 1.7576112412177984,
      "grad_norm": 1.4952008724212646,
      "learning_rate": 5e-05,
      "loss": 0.0995,
      "step": 1501
    },
    {
      "epoch": 1.7587822014051522,
      "grad_norm": 1.0493037700653076,
      "learning_rate": 5e-05,
      "loss": 0.07,
      "step": 1502
    },
    {
      "epoch": 1.759953161592506,
      "grad_norm": 1.0755949020385742,
      "learning_rate": 5e-05,
      "loss": 0.064,
      "step": 1503
    },
    {
      "epoch": 1.7611241217798594,
      "grad_norm": 0.7355666756629944,
      "learning_rate": 5e-05,
      "loss": 0.0841,
      "step": 1504
    },
    {
      "epoch": 1.762295081967213,
      "grad_norm": 1.2887810468673706,
      "learning_rate": 5e-05,
      "loss": 0.1187,
      "step": 1505
    },
    {
      "epoch": 1.7634660421545667,
      "grad_norm": 0.7584190964698792,
      "learning_rate": 5e-05,
      "loss": 0.0541,
      "step": 1506
    },
    {
      "epoch": 1.7646370023419204,
      "grad_norm": 0.8186696767807007,
      "learning_rate": 5e-05,
      "loss": 0.1063,
      "step": 1507
    },
    {
      "epoch": 1.765807962529274,
      "grad_norm": 0.7790956497192383,
      "learning_rate": 5e-05,
      "loss": 0.0902,
      "step": 1508
    },
    {
      "epoch": 1.7669789227166275,
      "grad_norm": 1.356122612953186,
      "learning_rate": 5e-05,
      "loss": 0.0991,
      "step": 1509
    },
    {
      "epoch": 1.7681498829039812,
      "grad_norm": 0.9370011687278748,
      "learning_rate": 5e-05,
      "loss": 0.0874,
      "step": 1510
    },
    {
      "epoch": 1.769320843091335,
      "grad_norm": 1.098102331161499,
      "learning_rate": 5e-05,
      "loss": 0.1988,
      "step": 1511
    },
    {
      "epoch": 1.7704918032786885,
      "grad_norm": 0.5388156175613403,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 1512
    },
    {
      "epoch": 1.771662763466042,
      "grad_norm": 1.8555870056152344,
      "learning_rate": 5e-05,
      "loss": 0.0795,
      "step": 1513
    },
    {
      "epoch": 1.7728337236533958,
      "grad_norm": 1.321489691734314,
      "learning_rate": 5e-05,
      "loss": 0.1396,
      "step": 1514
    },
    {
      "epoch": 1.7740046838407495,
      "grad_norm": 1.2173874378204346,
      "learning_rate": 5e-05,
      "loss": 0.0886,
      "step": 1515
    },
    {
      "epoch": 1.775175644028103,
      "grad_norm": 0.7791826725006104,
      "learning_rate": 5e-05,
      "loss": 0.0509,
      "step": 1516
    },
    {
      "epoch": 1.7763466042154565,
      "grad_norm": 1.0622183084487915,
      "learning_rate": 5e-05,
      "loss": 0.0895,
      "step": 1517
    },
    {
      "epoch": 1.7775175644028103,
      "grad_norm": 1.326160192489624,
      "learning_rate": 5e-05,
      "loss": 0.1538,
      "step": 1518
    },
    {
      "epoch": 1.778688524590164,
      "grad_norm": 2.3141708374023438,
      "learning_rate": 5e-05,
      "loss": 0.3315,
      "step": 1519
    },
    {
      "epoch": 1.7798594847775175,
      "grad_norm": 0.8225988149642944,
      "learning_rate": 5e-05,
      "loss": 0.0186,
      "step": 1520
    },
    {
      "epoch": 1.781030444964871,
      "grad_norm": 0.792951226234436,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1521
    },
    {
      "epoch": 1.7822014051522248,
      "grad_norm": 1.0799378156661987,
      "learning_rate": 5e-05,
      "loss": 0.1224,
      "step": 1522
    },
    {
      "epoch": 1.7833723653395785,
      "grad_norm": 0.7447335720062256,
      "learning_rate": 5e-05,
      "loss": 0.137,
      "step": 1523
    },
    {
      "epoch": 1.784543325526932,
      "grad_norm": 0.8255112171173096,
      "learning_rate": 5e-05,
      "loss": 0.0533,
      "step": 1524
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 1.1674975156784058,
      "learning_rate": 5e-05,
      "loss": 0.0946,
      "step": 1525
    },
    {
      "epoch": 1.7868852459016393,
      "grad_norm": 1.0786242485046387,
      "learning_rate": 5e-05,
      "loss": 0.1231,
      "step": 1526
    },
    {
      "epoch": 1.788056206088993,
      "grad_norm": 1.1261839866638184,
      "learning_rate": 5e-05,
      "loss": 0.1861,
      "step": 1527
    },
    {
      "epoch": 1.7892271662763466,
      "grad_norm": 1.5367673635482788,
      "learning_rate": 5e-05,
      "loss": 0.145,
      "step": 1528
    },
    {
      "epoch": 1.7903981264637001,
      "grad_norm": 1.2363697290420532,
      "learning_rate": 5e-05,
      "loss": 0.1253,
      "step": 1529
    },
    {
      "epoch": 1.7915690866510539,
      "grad_norm": 1.2991268634796143,
      "learning_rate": 5e-05,
      "loss": 0.1078,
      "step": 1530
    },
    {
      "epoch": 1.7927400468384076,
      "grad_norm": 0.8203684687614441,
      "learning_rate": 5e-05,
      "loss": 0.1037,
      "step": 1531
    },
    {
      "epoch": 1.7939110070257611,
      "grad_norm": 0.7587723731994629,
      "learning_rate": 5e-05,
      "loss": 0.0722,
      "step": 1532
    },
    {
      "epoch": 1.7950819672131146,
      "grad_norm": 0.9494809508323669,
      "learning_rate": 5e-05,
      "loss": 0.0551,
      "step": 1533
    },
    {
      "epoch": 1.7962529274004684,
      "grad_norm": 1.467406988143921,
      "learning_rate": 5e-05,
      "loss": 0.1621,
      "step": 1534
    },
    {
      "epoch": 1.7974238875878221,
      "grad_norm": 0.8215840458869934,
      "learning_rate": 5e-05,
      "loss": 0.0801,
      "step": 1535
    },
    {
      "epoch": 1.7985948477751756,
      "grad_norm": 0.8705247044563293,
      "learning_rate": 5e-05,
      "loss": 0.0726,
      "step": 1536
    },
    {
      "epoch": 1.7997658079625292,
      "grad_norm": 1.467301845550537,
      "learning_rate": 5e-05,
      "loss": 0.0635,
      "step": 1537
    },
    {
      "epoch": 1.800936768149883,
      "grad_norm": 1.001350998878479,
      "learning_rate": 5e-05,
      "loss": 0.0758,
      "step": 1538
    },
    {
      "epoch": 1.8021077283372366,
      "grad_norm": 0.7926905155181885,
      "learning_rate": 5e-05,
      "loss": 0.0826,
      "step": 1539
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 1.6472867727279663,
      "learning_rate": 5e-05,
      "loss": 0.0967,
      "step": 1540
    },
    {
      "epoch": 1.8044496487119437,
      "grad_norm": 0.9444388151168823,
      "learning_rate": 5e-05,
      "loss": 0.0989,
      "step": 1541
    },
    {
      "epoch": 1.8056206088992974,
      "grad_norm": 1.0026295185089111,
      "learning_rate": 5e-05,
      "loss": 0.078,
      "step": 1542
    },
    {
      "epoch": 1.8067915690866512,
      "grad_norm": 2.0969934463500977,
      "learning_rate": 5e-05,
      "loss": 0.1766,
      "step": 1543
    },
    {
      "epoch": 1.8079625292740047,
      "grad_norm": 0.7379223108291626,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 1544
    },
    {
      "epoch": 1.8091334894613582,
      "grad_norm": 0.9383180737495422,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 1545
    },
    {
      "epoch": 1.810304449648712,
      "grad_norm": 0.6996899247169495,
      "learning_rate": 5e-05,
      "loss": 0.0658,
      "step": 1546
    },
    {
      "epoch": 1.8114754098360657,
      "grad_norm": 1.9158439636230469,
      "learning_rate": 5e-05,
      "loss": 0.1346,
      "step": 1547
    },
    {
      "epoch": 1.8126463700234192,
      "grad_norm": 1.8479979038238525,
      "learning_rate": 5e-05,
      "loss": 0.0585,
      "step": 1548
    },
    {
      "epoch": 1.8138173302107727,
      "grad_norm": 0.8142930865287781,
      "learning_rate": 5e-05,
      "loss": 0.0535,
      "step": 1549
    },
    {
      "epoch": 1.8149882903981265,
      "grad_norm": 1.7884267568588257,
      "learning_rate": 5e-05,
      "loss": 0.1044,
      "step": 1550
    },
    {
      "epoch": 1.8161592505854802,
      "grad_norm": 0.7649821043014526,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 1551
    },
    {
      "epoch": 1.8173302107728337,
      "grad_norm": 1.4689832925796509,
      "learning_rate": 5e-05,
      "loss": 0.0872,
      "step": 1552
    },
    {
      "epoch": 1.8185011709601873,
      "grad_norm": 1.22202467918396,
      "learning_rate": 5e-05,
      "loss": 0.0925,
      "step": 1553
    },
    {
      "epoch": 1.819672131147541,
      "grad_norm": 1.5802046060562134,
      "learning_rate": 5e-05,
      "loss": 0.0985,
      "step": 1554
    },
    {
      "epoch": 1.8208430913348947,
      "grad_norm": 0.9450597167015076,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1555
    },
    {
      "epoch": 1.8220140515222483,
      "grad_norm": 1.5558634996414185,
      "learning_rate": 5e-05,
      "loss": 0.077,
      "step": 1556
    },
    {
      "epoch": 1.8231850117096018,
      "grad_norm": 1.0457062721252441,
      "learning_rate": 5e-05,
      "loss": 0.0528,
      "step": 1557
    },
    {
      "epoch": 1.8243559718969555,
      "grad_norm": 1.0725069046020508,
      "learning_rate": 5e-05,
      "loss": 0.1005,
      "step": 1558
    },
    {
      "epoch": 1.8255269320843093,
      "grad_norm": 1.1631827354431152,
      "learning_rate": 5e-05,
      "loss": 0.0913,
      "step": 1559
    },
    {
      "epoch": 1.8266978922716628,
      "grad_norm": 1.175981879234314,
      "learning_rate": 5e-05,
      "loss": 0.0834,
      "step": 1560
    },
    {
      "epoch": 1.8278688524590163,
      "grad_norm": 0.47546887397766113,
      "learning_rate": 5e-05,
      "loss": 0.0393,
      "step": 1561
    },
    {
      "epoch": 1.82903981264637,
      "grad_norm": 1.4211231470108032,
      "learning_rate": 5e-05,
      "loss": 0.0902,
      "step": 1562
    },
    {
      "epoch": 1.8302107728337238,
      "grad_norm": 0.9848178625106812,
      "learning_rate": 5e-05,
      "loss": 0.1235,
      "step": 1563
    },
    {
      "epoch": 1.8313817330210773,
      "grad_norm": 0.7885630130767822,
      "learning_rate": 5e-05,
      "loss": 0.0821,
      "step": 1564
    },
    {
      "epoch": 1.8325526932084308,
      "grad_norm": 1.0201257467269897,
      "learning_rate": 5e-05,
      "loss": 0.0795,
      "step": 1565
    },
    {
      "epoch": 1.8337236533957846,
      "grad_norm": 0.8267556428909302,
      "learning_rate": 5e-05,
      "loss": 0.1087,
      "step": 1566
    },
    {
      "epoch": 1.8348946135831383,
      "grad_norm": 0.9812317490577698,
      "learning_rate": 5e-05,
      "loss": 0.0731,
      "step": 1567
    },
    {
      "epoch": 1.8360655737704918,
      "grad_norm": 1.4384104013442993,
      "learning_rate": 5e-05,
      "loss": 0.1285,
      "step": 1568
    },
    {
      "epoch": 1.8372365339578454,
      "grad_norm": 0.7449677586555481,
      "learning_rate": 5e-05,
      "loss": 0.0531,
      "step": 1569
    },
    {
      "epoch": 1.838407494145199,
      "grad_norm": 0.7358453869819641,
      "learning_rate": 5e-05,
      "loss": 0.0492,
      "step": 1570
    },
    {
      "epoch": 1.8395784543325528,
      "grad_norm": 0.6556248664855957,
      "learning_rate": 5e-05,
      "loss": 0.0645,
      "step": 1571
    },
    {
      "epoch": 1.8407494145199064,
      "grad_norm": 1.1244159936904907,
      "learning_rate": 5e-05,
      "loss": 0.135,
      "step": 1572
    },
    {
      "epoch": 1.8419203747072599,
      "grad_norm": 0.7537005543708801,
      "learning_rate": 5e-05,
      "loss": 0.0481,
      "step": 1573
    },
    {
      "epoch": 1.8430913348946136,
      "grad_norm": 0.8719738721847534,
      "learning_rate": 5e-05,
      "loss": 0.068,
      "step": 1574
    },
    {
      "epoch": 1.8442622950819674,
      "grad_norm": 1.1585708856582642,
      "learning_rate": 5e-05,
      "loss": 0.1056,
      "step": 1575
    },
    {
      "epoch": 1.845433255269321,
      "grad_norm": 1.2863729000091553,
      "learning_rate": 5e-05,
      "loss": 0.0773,
      "step": 1576
    },
    {
      "epoch": 1.8466042154566744,
      "grad_norm": 0.7193450331687927,
      "learning_rate": 5e-05,
      "loss": 0.1187,
      "step": 1577
    },
    {
      "epoch": 1.8477751756440282,
      "grad_norm": 0.6968299150466919,
      "learning_rate": 5e-05,
      "loss": 0.1112,
      "step": 1578
    },
    {
      "epoch": 1.848946135831382,
      "grad_norm": 0.8812882900238037,
      "learning_rate": 5e-05,
      "loss": 0.0655,
      "step": 1579
    },
    {
      "epoch": 1.8501170960187352,
      "grad_norm": 0.8713743090629578,
      "learning_rate": 5e-05,
      "loss": 0.0969,
      "step": 1580
    },
    {
      "epoch": 1.851288056206089,
      "grad_norm": 0.7139488458633423,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 1581
    },
    {
      "epoch": 1.8524590163934427,
      "grad_norm": 0.900088369846344,
      "learning_rate": 5e-05,
      "loss": 0.1093,
      "step": 1582
    },
    {
      "epoch": 1.8536299765807962,
      "grad_norm": 0.7342048287391663,
      "learning_rate": 5e-05,
      "loss": 0.0908,
      "step": 1583
    },
    {
      "epoch": 1.8548009367681497,
      "grad_norm": 0.7002763152122498,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 1584
    },
    {
      "epoch": 1.8559718969555035,
      "grad_norm": 0.7300931811332703,
      "learning_rate": 5e-05,
      "loss": 0.1045,
      "step": 1585
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.9717668294906616,
      "learning_rate": 5e-05,
      "loss": 0.0791,
      "step": 1586
    },
    {
      "epoch": 1.8583138173302107,
      "grad_norm": 1.385552167892456,
      "learning_rate": 5e-05,
      "loss": 0.0503,
      "step": 1587
    },
    {
      "epoch": 1.8594847775175642,
      "grad_norm": 0.5881173610687256,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 1588
    },
    {
      "epoch": 1.860655737704918,
      "grad_norm": 1.235424518585205,
      "learning_rate": 5e-05,
      "loss": 0.1016,
      "step": 1589
    },
    {
      "epoch": 1.8618266978922717,
      "grad_norm": 0.9583006501197815,
      "learning_rate": 5e-05,
      "loss": 0.0549,
      "step": 1590
    },
    {
      "epoch": 1.8629976580796253,
      "grad_norm": 1.9292140007019043,
      "learning_rate": 5e-05,
      "loss": 0.2312,
      "step": 1591
    },
    {
      "epoch": 1.8641686182669788,
      "grad_norm": 0.46250614523887634,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 1592
    },
    {
      "epoch": 1.8653395784543325,
      "grad_norm": 0.5766193270683289,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 1593
    },
    {
      "epoch": 1.8665105386416863,
      "grad_norm": 0.3638472259044647,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 1594
    },
    {
      "epoch": 1.8676814988290398,
      "grad_norm": 0.620448648929596,
      "learning_rate": 5e-05,
      "loss": 0.0727,
      "step": 1595
    },
    {
      "epoch": 1.8688524590163933,
      "grad_norm": 0.8109788298606873,
      "learning_rate": 5e-05,
      "loss": 0.1268,
      "step": 1596
    },
    {
      "epoch": 1.870023419203747,
      "grad_norm": 1.0068271160125732,
      "learning_rate": 5e-05,
      "loss": 0.1022,
      "step": 1597
    },
    {
      "epoch": 1.8711943793911008,
      "grad_norm": 0.6590712070465088,
      "learning_rate": 5e-05,
      "loss": 0.0464,
      "step": 1598
    },
    {
      "epoch": 1.8723653395784543,
      "grad_norm": 1.058575987815857,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 1599
    },
    {
      "epoch": 1.8735362997658078,
      "grad_norm": 1.330453634262085,
      "learning_rate": 5e-05,
      "loss": 0.1656,
      "step": 1600
    },
    {
      "epoch": 1.8747072599531616,
      "grad_norm": 1.2476398944854736,
      "learning_rate": 5e-05,
      "loss": 0.081,
      "step": 1601
    },
    {
      "epoch": 1.8758782201405153,
      "grad_norm": 1.7538371086120605,
      "learning_rate": 5e-05,
      "loss": 0.1603,
      "step": 1602
    },
    {
      "epoch": 1.8770491803278688,
      "grad_norm": 1.0702329874038696,
      "learning_rate": 5e-05,
      "loss": 0.0829,
      "step": 1603
    },
    {
      "epoch": 1.8782201405152223,
      "grad_norm": 1.0048754215240479,
      "learning_rate": 5e-05,
      "loss": 0.0405,
      "step": 1604
    },
    {
      "epoch": 1.879391100702576,
      "grad_norm": 1.0004782676696777,
      "learning_rate": 5e-05,
      "loss": 0.0627,
      "step": 1605
    },
    {
      "epoch": 1.8805620608899298,
      "grad_norm": 0.9928520917892456,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 1606
    },
    {
      "epoch": 1.8817330210772834,
      "grad_norm": 0.9207344055175781,
      "learning_rate": 5e-05,
      "loss": 0.0872,
      "step": 1607
    },
    {
      "epoch": 1.8829039812646369,
      "grad_norm": 0.9982552528381348,
      "learning_rate": 5e-05,
      "loss": 0.0954,
      "step": 1608
    },
    {
      "epoch": 1.8840749414519906,
      "grad_norm": 0.726678192615509,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 1609
    },
    {
      "epoch": 1.8852459016393444,
      "grad_norm": 1.190495491027832,
      "learning_rate": 5e-05,
      "loss": 0.0908,
      "step": 1610
    },
    {
      "epoch": 1.8864168618266979,
      "grad_norm": 0.8476972579956055,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1611
    },
    {
      "epoch": 1.8875878220140514,
      "grad_norm": 0.8386058807373047,
      "learning_rate": 5e-05,
      "loss": 0.0984,
      "step": 1612
    },
    {
      "epoch": 1.8887587822014051,
      "grad_norm": 1.2727073431015015,
      "learning_rate": 5e-05,
      "loss": 0.0891,
      "step": 1613
    },
    {
      "epoch": 1.8899297423887589,
      "grad_norm": 1.1251022815704346,
      "learning_rate": 5e-05,
      "loss": 0.0965,
      "step": 1614
    },
    {
      "epoch": 1.8911007025761124,
      "grad_norm": 1.2287824153900146,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 1615
    },
    {
      "epoch": 1.892271662763466,
      "grad_norm": 0.5773045420646667,
      "learning_rate": 5e-05,
      "loss": 0.0413,
      "step": 1616
    },
    {
      "epoch": 1.8934426229508197,
      "grad_norm": 1.312035083770752,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 1617
    },
    {
      "epoch": 1.8946135831381734,
      "grad_norm": 2.2923026084899902,
      "learning_rate": 5e-05,
      "loss": 0.1393,
      "step": 1618
    },
    {
      "epoch": 1.895784543325527,
      "grad_norm": 0.8977451324462891,
      "learning_rate": 5e-05,
      "loss": 0.1078,
      "step": 1619
    },
    {
      "epoch": 1.8969555035128804,
      "grad_norm": 0.3171037435531616,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 1620
    },
    {
      "epoch": 1.8981264637002342,
      "grad_norm": 1.5618785619735718,
      "learning_rate": 5e-05,
      "loss": 0.1372,
      "step": 1621
    },
    {
      "epoch": 1.899297423887588,
      "grad_norm": 0.9190545678138733,
      "learning_rate": 5e-05,
      "loss": 0.0718,
      "step": 1622
    },
    {
      "epoch": 1.9004683840749415,
      "grad_norm": 1.8099912405014038,
      "learning_rate": 5e-05,
      "loss": 0.0745,
      "step": 1623
    },
    {
      "epoch": 1.901639344262295,
      "grad_norm": 0.540134847164154,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 1624
    },
    {
      "epoch": 1.9028103044496487,
      "grad_norm": 1.1598238945007324,
      "learning_rate": 5e-05,
      "loss": 0.1009,
      "step": 1625
    },
    {
      "epoch": 1.9039812646370025,
      "grad_norm": 0.5582563877105713,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 1626
    },
    {
      "epoch": 1.905152224824356,
      "grad_norm": 0.8023038506507874,
      "learning_rate": 5e-05,
      "loss": 0.0916,
      "step": 1627
    },
    {
      "epoch": 1.9063231850117095,
      "grad_norm": 0.7783001065254211,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 1628
    },
    {
      "epoch": 1.9074941451990632,
      "grad_norm": 0.9510451555252075,
      "learning_rate": 5e-05,
      "loss": 0.1045,
      "step": 1629
    },
    {
      "epoch": 1.908665105386417,
      "grad_norm": 1.3566621541976929,
      "learning_rate": 5e-05,
      "loss": 0.115,
      "step": 1630
    },
    {
      "epoch": 1.9098360655737705,
      "grad_norm": 0.9767225384712219,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 1631
    },
    {
      "epoch": 1.911007025761124,
      "grad_norm": 1.3826254606246948,
      "learning_rate": 5e-05,
      "loss": 0.0996,
      "step": 1632
    },
    {
      "epoch": 1.9121779859484778,
      "grad_norm": 1.0482324361801147,
      "learning_rate": 5e-05,
      "loss": 0.1493,
      "step": 1633
    },
    {
      "epoch": 1.9133489461358315,
      "grad_norm": 1.1838548183441162,
      "learning_rate": 5e-05,
      "loss": 0.1477,
      "step": 1634
    },
    {
      "epoch": 1.914519906323185,
      "grad_norm": 0.6816602349281311,
      "learning_rate": 5e-05,
      "loss": 0.0566,
      "step": 1635
    },
    {
      "epoch": 1.9156908665105385,
      "grad_norm": 1.2569228410720825,
      "learning_rate": 5e-05,
      "loss": 0.1487,
      "step": 1636
    },
    {
      "epoch": 1.9168618266978923,
      "grad_norm": 0.6960170269012451,
      "learning_rate": 5e-05,
      "loss": 0.072,
      "step": 1637
    },
    {
      "epoch": 1.918032786885246,
      "grad_norm": 0.6299132108688354,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 1638
    },
    {
      "epoch": 1.9192037470725996,
      "grad_norm": 0.899884819984436,
      "learning_rate": 5e-05,
      "loss": 0.1524,
      "step": 1639
    },
    {
      "epoch": 1.920374707259953,
      "grad_norm": 1.049007534980774,
      "learning_rate": 5e-05,
      "loss": 0.1639,
      "step": 1640
    },
    {
      "epoch": 1.9215456674473068,
      "grad_norm": 0.8223922252655029,
      "learning_rate": 5e-05,
      "loss": 0.1015,
      "step": 1641
    },
    {
      "epoch": 1.9227166276346606,
      "grad_norm": 0.5307344198226929,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 1642
    },
    {
      "epoch": 1.923887587822014,
      "grad_norm": 0.8457759022712708,
      "learning_rate": 5e-05,
      "loss": 0.092,
      "step": 1643
    },
    {
      "epoch": 1.9250585480093676,
      "grad_norm": 0.5975802540779114,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 1644
    },
    {
      "epoch": 1.9262295081967213,
      "grad_norm": 1.5248777866363525,
      "learning_rate": 5e-05,
      "loss": 0.0951,
      "step": 1645
    },
    {
      "epoch": 1.927400468384075,
      "grad_norm": 0.8006476759910583,
      "learning_rate": 5e-05,
      "loss": 0.1367,
      "step": 1646
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 0.5124492645263672,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 1647
    },
    {
      "epoch": 1.9297423887587821,
      "grad_norm": 0.9748665690422058,
      "learning_rate": 5e-05,
      "loss": 0.1736,
      "step": 1648
    },
    {
      "epoch": 1.9309133489461359,
      "grad_norm": 1.2659395933151245,
      "learning_rate": 5e-05,
      "loss": 0.3345,
      "step": 1649
    },
    {
      "epoch": 1.9320843091334896,
      "grad_norm": 1.0270819664001465,
      "learning_rate": 5e-05,
      "loss": 0.1303,
      "step": 1650
    },
    {
      "epoch": 1.9332552693208431,
      "grad_norm": 0.8635085225105286,
      "learning_rate": 5e-05,
      "loss": 0.1043,
      "step": 1651
    },
    {
      "epoch": 1.9344262295081966,
      "grad_norm": 0.8089017271995544,
      "learning_rate": 5e-05,
      "loss": 0.1686,
      "step": 1652
    },
    {
      "epoch": 1.9355971896955504,
      "grad_norm": 1.8535935878753662,
      "learning_rate": 5e-05,
      "loss": 0.1606,
      "step": 1653
    },
    {
      "epoch": 1.9367681498829041,
      "grad_norm": 0.7900869250297546,
      "learning_rate": 5e-05,
      "loss": 0.0512,
      "step": 1654
    },
    {
      "epoch": 1.9379391100702577,
      "grad_norm": 1.0267118215560913,
      "learning_rate": 5e-05,
      "loss": 0.1117,
      "step": 1655
    },
    {
      "epoch": 1.9391100702576112,
      "grad_norm": 1.4030849933624268,
      "learning_rate": 5e-05,
      "loss": 0.0738,
      "step": 1656
    },
    {
      "epoch": 1.940281030444965,
      "grad_norm": 0.6894080638885498,
      "learning_rate": 5e-05,
      "loss": 0.0911,
      "step": 1657
    },
    {
      "epoch": 1.9414519906323187,
      "grad_norm": 0.974815845489502,
      "learning_rate": 5e-05,
      "loss": 0.07,
      "step": 1658
    },
    {
      "epoch": 1.9426229508196722,
      "grad_norm": 1.8968579769134521,
      "learning_rate": 5e-05,
      "loss": 0.1109,
      "step": 1659
    },
    {
      "epoch": 1.9437939110070257,
      "grad_norm": 0.7414000630378723,
      "learning_rate": 5e-05,
      "loss": 0.0944,
      "step": 1660
    },
    {
      "epoch": 1.9449648711943794,
      "grad_norm": 0.6924798488616943,
      "learning_rate": 5e-05,
      "loss": 0.0716,
      "step": 1661
    },
    {
      "epoch": 1.9461358313817332,
      "grad_norm": 0.6854343414306641,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1662
    },
    {
      "epoch": 1.9473067915690867,
      "grad_norm": 0.8833489418029785,
      "learning_rate": 5e-05,
      "loss": 0.0688,
      "step": 1663
    },
    {
      "epoch": 1.9484777517564402,
      "grad_norm": 1.6120033264160156,
      "learning_rate": 5e-05,
      "loss": 0.0679,
      "step": 1664
    },
    {
      "epoch": 1.949648711943794,
      "grad_norm": 1.2714191675186157,
      "learning_rate": 5e-05,
      "loss": 0.0923,
      "step": 1665
    },
    {
      "epoch": 1.9508196721311475,
      "grad_norm": 0.6840183734893799,
      "learning_rate": 5e-05,
      "loss": 0.0812,
      "step": 1666
    },
    {
      "epoch": 1.951990632318501,
      "grad_norm": 0.804551362991333,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 1667
    },
    {
      "epoch": 1.9531615925058547,
      "grad_norm": 0.8239588737487793,
      "learning_rate": 5e-05,
      "loss": 0.0775,
      "step": 1668
    },
    {
      "epoch": 1.9543325526932085,
      "grad_norm": 1.0779919624328613,
      "learning_rate": 5e-05,
      "loss": 0.1029,
      "step": 1669
    },
    {
      "epoch": 1.955503512880562,
      "grad_norm": 1.1751142740249634,
      "learning_rate": 5e-05,
      "loss": 0.0659,
      "step": 1670
    },
    {
      "epoch": 1.9566744730679155,
      "grad_norm": 1.7591564655303955,
      "learning_rate": 5e-05,
      "loss": 0.1353,
      "step": 1671
    },
    {
      "epoch": 1.9578454332552693,
      "grad_norm": 1.1659352779388428,
      "learning_rate": 5e-05,
      "loss": 0.0768,
      "step": 1672
    },
    {
      "epoch": 1.959016393442623,
      "grad_norm": 0.7852348685264587,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 1673
    },
    {
      "epoch": 1.9601873536299765,
      "grad_norm": 1.2038459777832031,
      "learning_rate": 5e-05,
      "loss": 0.1435,
      "step": 1674
    },
    {
      "epoch": 1.96135831381733,
      "grad_norm": 1.2188605070114136,
      "learning_rate": 5e-05,
      "loss": 0.106,
      "step": 1675
    },
    {
      "epoch": 1.9625292740046838,
      "grad_norm": 0.7801944017410278,
      "learning_rate": 5e-05,
      "loss": 0.0729,
      "step": 1676
    },
    {
      "epoch": 1.9637002341920375,
      "grad_norm": 0.7804877758026123,
      "learning_rate": 5e-05,
      "loss": 0.1206,
      "step": 1677
    },
    {
      "epoch": 1.964871194379391,
      "grad_norm": 0.8205691576004028,
      "learning_rate": 5e-05,
      "loss": 0.0876,
      "step": 1678
    },
    {
      "epoch": 1.9660421545667446,
      "grad_norm": 0.8953105211257935,
      "learning_rate": 5e-05,
      "loss": 0.0676,
      "step": 1679
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 1.13882577419281,
      "learning_rate": 5e-05,
      "loss": 0.0898,
      "step": 1680
    },
    {
      "epoch": 1.968384074941452,
      "grad_norm": 1.66371750831604,
      "learning_rate": 5e-05,
      "loss": 0.0972,
      "step": 1681
    },
    {
      "epoch": 1.9695550351288056,
      "grad_norm": 0.74961918592453,
      "learning_rate": 5e-05,
      "loss": 0.0628,
      "step": 1682
    },
    {
      "epoch": 1.970725995316159,
      "grad_norm": 1.2872143983840942,
      "learning_rate": 5e-05,
      "loss": 0.1219,
      "step": 1683
    },
    {
      "epoch": 1.9718969555035128,
      "grad_norm": 1.8074911832809448,
      "learning_rate": 5e-05,
      "loss": 0.1097,
      "step": 1684
    },
    {
      "epoch": 1.9730679156908666,
      "grad_norm": 0.9238805174827576,
      "learning_rate": 5e-05,
      "loss": 0.1416,
      "step": 1685
    },
    {
      "epoch": 1.9742388758782201,
      "grad_norm": 0.8618279099464417,
      "learning_rate": 5e-05,
      "loss": 0.0456,
      "step": 1686
    },
    {
      "epoch": 1.9754098360655736,
      "grad_norm": 1.0925145149230957,
      "learning_rate": 5e-05,
      "loss": 0.0864,
      "step": 1687
    },
    {
      "epoch": 1.9765807962529274,
      "grad_norm": 1.0094566345214844,
      "learning_rate": 5e-05,
      "loss": 0.0636,
      "step": 1688
    },
    {
      "epoch": 1.9777517564402811,
      "grad_norm": 0.9952378869056702,
      "learning_rate": 5e-05,
      "loss": 0.1839,
      "step": 1689
    },
    {
      "epoch": 1.9789227166276346,
      "grad_norm": 0.41849759221076965,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 1690
    },
    {
      "epoch": 1.9800936768149882,
      "grad_norm": 0.45672518014907837,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 1691
    },
    {
      "epoch": 1.981264637002342,
      "grad_norm": 0.49522408843040466,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 1692
    },
    {
      "epoch": 1.9824355971896956,
      "grad_norm": 1.124793291091919,
      "learning_rate": 5e-05,
      "loss": 0.0956,
      "step": 1693
    },
    {
      "epoch": 1.9836065573770492,
      "grad_norm": 0.8876978158950806,
      "learning_rate": 5e-05,
      "loss": 0.0905,
      "step": 1694
    },
    {
      "epoch": 1.9847775175644027,
      "grad_norm": 1.692873239517212,
      "learning_rate": 5e-05,
      "loss": 0.2061,
      "step": 1695
    },
    {
      "epoch": 1.9859484777517564,
      "grad_norm": 1.119110345840454,
      "learning_rate": 5e-05,
      "loss": 0.1047,
      "step": 1696
    },
    {
      "epoch": 1.9871194379391102,
      "grad_norm": 0.9000266194343567,
      "learning_rate": 5e-05,
      "loss": 0.0952,
      "step": 1697
    },
    {
      "epoch": 1.9882903981264637,
      "grad_norm": 0.8287221193313599,
      "learning_rate": 5e-05,
      "loss": 0.0716,
      "step": 1698
    },
    {
      "epoch": 1.9894613583138172,
      "grad_norm": 1.7677030563354492,
      "learning_rate": 5e-05,
      "loss": 0.1103,
      "step": 1699
    },
    {
      "epoch": 1.990632318501171,
      "grad_norm": 1.1241317987442017,
      "learning_rate": 5e-05,
      "loss": 0.0732,
      "step": 1700
    },
    {
      "epoch": 1.9918032786885247,
      "grad_norm": 0.8011396527290344,
      "learning_rate": 5e-05,
      "loss": 0.0583,
      "step": 1701
    },
    {
      "epoch": 1.9929742388758782,
      "grad_norm": 0.9509716033935547,
      "learning_rate": 5e-05,
      "loss": 0.1116,
      "step": 1702
    },
    {
      "epoch": 1.9941451990632317,
      "grad_norm": 0.7858178615570068,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 1703
    },
    {
      "epoch": 1.9953161592505855,
      "grad_norm": 1.1470426321029663,
      "learning_rate": 5e-05,
      "loss": 0.0836,
      "step": 1704
    },
    {
      "epoch": 1.9964871194379392,
      "grad_norm": 0.7118511199951172,
      "learning_rate": 5e-05,
      "loss": 0.0951,
      "step": 1705
    },
    {
      "epoch": 1.9976580796252927,
      "grad_norm": 0.7066919803619385,
      "learning_rate": 5e-05,
      "loss": 0.0754,
      "step": 1706
    },
    {
      "epoch": 1.9988290398126463,
      "grad_norm": 0.5668503642082214,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 1707
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3669246435165405,
      "learning_rate": 5e-05,
      "loss": 0.091,
      "step": 1708
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.20188027620315552,
      "eval_runtime": 46.3023,
      "eval_samples_per_second": 18.055,
      "eval_steps_per_second": 2.268,
      "step": 1708
    },
    {
      "epoch": 2.0,
      "step": 1708,
      "total_flos": 5.312917609245573e+17,
      "train_loss": 0.16796565636451546,
      "train_runtime": 2360.2127,
      "train_samples_per_second": 5.789,
      "train_steps_per_second": 0.724
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1708,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "total_flos": 5.312917609245573e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
